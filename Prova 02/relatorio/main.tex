\documentclass[a4paper]{article}
\usepackage{student}
\usepackage{graphicx}
\usepackage{caption}
\usepackage[version=4]{mhchem}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows.meta, positioning, decorations.pathreplacing}
\usepackage{enumitem}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{float}
\usepackage[portuguese]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[numbers,sort&compress]{natbib} % ou [authoryear]
\usepackage[alf]{abntex2cite} % citação ABNT autor-data
\usepackage{tcolorbox}
\usepackage{listings}
\usepackage{xcolor}

% --- Suporte a UTF-8 ---
\usepackage{lmodern}           % fonte moderna compatível com T1

% --- Pacotes para exibir código ---


% --- Define o idioma R com palavras-chave e cores ---
\lstdefinelanguage{R}{
  keywords={function, if, else, for, in, while, repeat, return, library, require, next, break},
  keywordstyle=\color{blue}\bfseries,
  ndkeywords={TRUE, FALSE, NULL, NA, Inf, NaN},
  ndkeywordstyle=\color{magenta}\bfseries,
  comment=[l]{\#},
  commentstyle=\color{gray}\ttfamily,
  stringstyle=\color{orange}\ttfamily,
  morestring=[b]",
  morestring=[b]',
  sensitive=true
}

% --- Mapeamento para UTF-8 (acentos, ç, etc.) ---
% ---- estilo do listing ----
\lstset{
  literate=
    {á}{{\'a}}1 {ã}{{\~a}}1 {â}{{\^a}}1 {à}{{\`a}}1
    {é}{{\'e}}1 {ê}{{\^e}}1
    {í}{{\'i}}1
    {ó}{{\'o}}1 {õ}{{\~o}}1 {ô}{{\^o}}1
    {ú}{{\'u}}1 {ü}{{\"u}}1
    {ç}{{\c{c}}}1
    {Á}{{\'A}}1 {Ã}{{\~A}}1 {Â}{{\^A}}1
    {É}{{\'E}}1 {Ê}{{\^E}}1
    {Í}{{\'I}}1
    {Ó}{{\'O}}1 {Õ}{{\~O}}1 {Ô}{{\^O}}1
    {Ú}{{\'U}}1 {Ü}{{\"U}}1
    {Ç}{{\c{C}}}1,
  basicstyle=\ttfamily\small,
  backgroundcolor=\color{gray!5},
  frame=single,
  breaklines=true,
  captionpos=b,
  columns=flexible,
  showstringspaces=false,
  keepspaces=true,
  numbers=left,
  numberstyle=\tiny\color{gray},
  stepnumber=1,
  numbersep=6pt
}
\DeclareMathOperator{\softmax}{softmax}
\DeclareMathOperator{\Concat}{Concat}
\pagestyle{plain}

\tikzstyle{arrow} = [thick,->,>=stealth]

% Definindo o estilo de destaque com linhas pontilhadas
\tikzstyle{highlight} = [draw, dashed, thick, rectangle, rounded corners, inner sep=0.2cm, orange]


\tikzstyle{startstop} = [
    rectangle, rounded corners, minimum width=0.5cm,
    text centered, draw=black, fill=blue!10, font=\small
]
\tikzstyle{startstop_S} = [
    rectangle, rounded corners, minimum width=0.5cm, minimum height=0.8cm,
    text centered, draw=black, fill=green!30, font=\small
]
\tikzstyle{decision} = [
    diamond, aspect=2, draw=black, fill=orange!15, align=center,
    text centered, inner sep=0pt, font=\small
]
\tikzstyle{decision_S} = [
    diamond, aspect=2, draw=black, fill=orange!30, align=center,
    text centered, inner sep=0pt, font=\small
]
\tikzstyle{arrow} = [thick,->,>=stealth]



% Metadata
\date{\today}
\setmodule{MAE5911/IME: Fundamentos de Estatística e Machine Learning. \\ Prof.: Alexandre Galvão Patriota} 
\setterm{2o. semestre, 2025}

%-------------------------------%
% Other details
% TODO: Fill these
%-------------------------------%
\title{Prova 02 - 05/12}
\setmembername{Nara Avila Moraes}  % Fill group member names
\setmemberuid{5716734}  % Fill group member uids (same order)

%-------------------------------%
% Add / Delete commands and packages
% TODO: Add / Delete here as you need
%-------------------------------%
\usepackage{amsmath,amssymb,bm}

\newcommand{\KL}{\mathrm{KL}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\T}{\top}

\newcommand{\expdist}[2]{%
        \normalfont{\textsc{Exp}}(#1, #2)%
    }
\newcommand{\expparam}{\bm \lambda}
\newcommand{\Expparam}{\bm \Lambda}
\newcommand{\natparam}{\bm \eta}
\newcommand{\Natparam}{\bm H}
\newcommand{\sufstat}{\bm u}

% Main document
\begin{document}
    % Add header
    \header{}

\textbf{Questão 01:} Descreva o mecanismo de atenção com múltiplas cabeças (Multi-head Attention). 
Apresente o desenvolvimento como feito em sala, considerando a versão ``mascarada'', para identificar 
médias ponderadas dinamicamente.

    \begin{answer}[]
O mecanismo de atenção é o que transforma representação semântica de tokens em representação contextualizada, 
garantindo que os tokens sejam reinterpretados de acordo com o contexto em que é referido. O mecanismo consiste em durante o treino aprender as 
matrizes $W_Q$ e $W_K$ que representam parâmetros de \emph{query}, o que este token procura, e \emph{key}, o que este token oferece, tais que $$ Q_i=x_i*W_Q $$ $$ K_i = x_i*W_K,$$ no qual, durante o teste, o embedding $x$ de cada token é redefinido para um novo valor utilizando a fórmula
\[
\mathrm{Attention}(Q, K, V)
=
\mathrm{softmax}\!\left(
\frac{Q K^{\top}}{\sqrt{d_k}} + M
\right) V
\;=\;
\sum_{j} A_{ij} \, V_j
\]
na qual M é a máscara que apenas atribui contribuição não nula para tokens em relação a si mesmo e aos tokens predecessores, 
garantindo uma análise causal de contexto e V é a estatística suficiente sobre a qual tomamos a expectativa, também calculado via matriz de parâmetros $W_V$, tal que $V_i = x_i * W_V$.
O resultado final é a matriz Attention, que tem dimensão $n \times d_{model}$, para $n$ tokens e dimensão do embedding $d_{model}$, no modelo com uma cabeça.

Para o modelo com multiplas cabeças, o \emph{Multi-head Attention}, a dimensão do embedding é dividida em $h$ cabeças, cada uma com dimensão $d_k = d_{model}/h$. Cada cabeça aprende suas próprias matrizes de parâmetros $W_Q^{(i)}$, $W_K^{(i)}$ e $W_V^{(i)}$, para $i = 1, \ldots, h$. 
O mecanismo de atenção é aplicado separadamente para cada cabeça, resultando em $h$ matrizes de atenção distintas. 

Este modelo tem a vantagem de reduzir a dimensionalidade do embedding e permitir a paralelização do cálculo da matriz de atenção, além de permitir que cada cabeça foque em diferentes partes da sequência de entrada $x$,
analisando separadamente diferentes aspectos de relacionamento entre tokens, como por exemplo, semântica, sintaxe, ou interdependências. 
Essas matrizes são então concatenadas e projetadas de volta para a dimensão original do embedding usando uma matriz de projeção de parâmetros adicional $W_O$, 
que combina as matrizes de atenção com pesos diferentes, adicionando uma camada adicional de refinamento do contexto, podendo adicionalmente ser utilizada para reduzir a dimensionalidade do embedding. 
A fórmula do modelo com multi-head attention é:
$$
\mathrm{MultiHead}(Q,K,V)
=
\mathrm{Concat}\!\left(
\mathrm{Attention}(Q W_1^{Q},\, K W_1^{K},\, V W_1^{V}),
\;\ldots,\;
\mathrm{Attention}(Q W_h^{Q},\, K W_h^{K},\, V W_h^{V})
\right)
W^{O}.
$$


\subsubsection*{Exemplo do cálculo de \emph{multi-head attention}, partindo dos seguintes blocos $W_Q$, $W_K$ e $W_V$ de parâmetros de atenção treinados, e $W_O = I_4$ neste exemplo por simplicidade, para $h=2$:}

\begin{center}
\begin{minipage}{0.47\linewidth}
\begin{tcolorbox}[title={Head 1}, colback=white, boxsep=0pt]
\[
W_Q^{(1)} =
\begin{bmatrix}
1 & 0\\
0 & 1\\
1 & 0\\
0 & 1
\end{bmatrix},
W_K^{(1)} =
\begin{bmatrix}
1 & 1\\
0 & 1\\
1 & 0\\
0 & 1
\end{bmatrix},
W_V^{(1)} =
\begin{bmatrix}
1 & 0\\
0 & 1\\
1 & 1\\
0 & 1
\end{bmatrix}
\]
\end{tcolorbox}
\end{minipage}
\hfill
\begin{minipage}{0.47\linewidth}
\begin{tcolorbox}[title={Head 2}, colback=white, boxsep=0pt]
\[
W_Q^{(2)} =
\begin{bmatrix}
0 & 1\\
1 & 0\\
0 & 1\\
1 & 0
\end{bmatrix},
W_K^{(2)} =
\begin{bmatrix}
1 & 0\\
1 & 1\\
0 & 1\\
1 & 0
\end{bmatrix},
W_V^{(2)} =
\begin{bmatrix}
0 & 1\\
1 & 0\\
1 & 0\\
0 & 1
\end{bmatrix}
\]
\end{tcolorbox}
\end{minipage}
\end{center}
Considere um prompt com $n=3$ tokens:
\[
(\text{Life},\ \text{is},\ \text{awesome})
\]

Cada token é representado por um embedding de dimensão $d_{\text{model}} = 4$.
Para fins ilustrativos consideramos a seguinte matriz de embeddings:,
\[
X =
\begin{bmatrix}
\text{Life} \\
\text{is} \\
\text{awesome}
\end{bmatrix}
=
\begin{bmatrix}
1 & 0 & 1 & 0 \\
0 & 1 & 1 & 0 \\
1 & 1 & 0 & 1
\end{bmatrix}
\in \mathbb{R}^{3 \times 4}.
\]

Cálculo das projecoes Q, K, V para cada token:
\[
Q^{(i)} = X W_Q^{(i)},\quad
K^{(i)} = X W_K^{(i)},\quad
V^{(i)} = X W_V^{(i)},
\]

\begin{center}
\begin{minipage}{0.47\linewidth}
\begin{tcolorbox}[title={Head 1}, colback=white, boxsep=0pt]
\[
Q^{(1)} =
\begin{bmatrix}
2 & 0 \\
1 & 1 \\
1 & 2
\end{bmatrix},
K^{(1)} =
\begin{bmatrix}
2 & 1 \\
1 & 1 \\
1 & 3
\end{bmatrix},
V^{(1)} =
\begin{bmatrix}
2 & 1 \\
1 & 2 \\
1 & 2
\end{bmatrix}
\]
\end{tcolorbox}
\end{minipage}
\hfill
\begin{minipage}{0.47\linewidth}
\begin{tcolorbox}[title={Head 2}, colback=white, boxsep=0pt]
\[
Q^{(2)} =
\begin{bmatrix}
0 & 2 \\
1 & 1 \\
2 & 1
\end{bmatrix},
K^{(2)} =
\begin{bmatrix}
1 & 1 \\
1 & 2 \\
3 & 1
\end{bmatrix},
V^{(2)} =
\begin{bmatrix}
1 & 1 \\
2 & 0 \\
1 & 2
\end{bmatrix}
\]
\end{tcolorbox}
\end{minipage}
\end{center}

Calculamos agora, para $d_k = d_{\text{model}} / h = 2$:

\[
S^{(i)} = \frac{Q^{(i)} {K^{(i)}}^{\top}}{\sqrt{d_k}}
\]
\begin{center}
\begin{minipage}{0.47\linewidth}
\begin{tcolorbox}[title={Head 1}, colback=white, boxsep=0pt]
\[
S^{(1)} =
\frac{1}{\sqrt{2}}
\begin{bmatrix}
4 & 2 & 2 \\
3 & 2 & 4 \\
4 & 3 & 7
\end{bmatrix}
\]
\end{tcolorbox}
\end{minipage}
\hfill
\begin{minipage}{0.47\linewidth}
\begin{tcolorbox}[title={Head 2}, colback=white, boxsep=0pt]
\[
S^{(2)} = \frac{1}{\sqrt{2}}
\begin{bmatrix}
2 & 4 & 2 \\
2 & 3 & 4 \\
3 & 4 & 7
\end{bmatrix}
\]
\end{tcolorbox}
\end{minipage}
\end{center}

Aplicamos em seguida a máscara causal, o que impede que o token $t$ seja co-relacionado com tokens em posições subsequêntes (futuras) $j>t$, exatamente igual ocorre no treinamento.
Os scores mascarados são obtidos por:
\[
\widetilde{S}^{(i)} = S^{(i)} + M, 
\qquad
M =
\begin{bmatrix}
0 & -\infty & -\infty \\
0 & 0        & -\infty \\
0 & 0        & 0
\end{bmatrix},
\]

Note que a máscara zera as probabilidades das posições não causais:
\[
\widetilde{S}^{(1)}_{1,:}
= \bigl(s_{11}, -\infty, -\infty\bigr)
\quad
\implies
\quad
\softmax\bigl(\widetilde{S}^{(1)}_{1,:}\bigr)
=
\frac{\bigl(e^{s_{11}}, e^{-\infty}, e^{-\infty}\bigr)}
     {e^{s_{11}} + e^{-\infty} + e^{-\infty}}
= (1,0,0),
\]


Aplicando a função $\softmax$ linha a linha aos scores mascarados,
obtemos as matrizes $A^{(1)}$ e $A^{(2)}$.


\begin{center}
\begin{minipage}{0.47\linewidth}
\begin{tcolorbox}[title={Head 1}, colback=white, boxsep=0pt]
\[
A^{(1)} =
\begin{bmatrix}
1.000 & 0.000 & 0.000 \\
0.670 & 0.330 & 0.000 \\
0.102 & 0.050 & 0.848
\end{bmatrix}
\]
\end{tcolorbox}
\end{minipage}
\hfill
\begin{minipage}{0.47\linewidth}
\begin{tcolorbox}[title={Head 2}, colback=white, boxsep=0pt]
\[
A^{(2)} = 
\begin{bmatrix}
1.000 & 0.000 & 0.000 \\
0.330 & 0.670 & 0.000 \\
0.050 & 0.102 & 0.848
\end{bmatrix}
\]
\end{tcolorbox}
\end{minipage}
\end{center}


A saída da cabeça $i$ é
\[
\text{head}_i = A^{(i)} V^{(i)} \in \mathbb{R}^{3 \times 2}.
\]

\begin{center}
\begin{minipage}{0.47\linewidth}
\begin{tcolorbox}[title={Head 1}, colback=white, boxsep=0.6mm, left=0mm]
\[
A^{(1)} V^{(1)}=
\simeq
\begin{bmatrix}
2.000 & 1.000 \\
1.670 & 1.330 \\
1.102 & 1.898
\end{bmatrix}
\]
\end{tcolorbox}
\end{minipage}
\hfill
\begin{minipage}{0.47\linewidth}
\begin{tcolorbox}[title={Head 2}, colback=white, boxsep=0.6mm, left=0mm]
\[
A^{(2)} V^{(2)}=
\simeq
\begin{bmatrix}
1.000 & 1.000 \\
1.670 & 0.330 \\
1.102 & 1.747
\end{bmatrix}
\]
\end{tcolorbox}
\end{minipage}
\end{center}

Considerando, por simplicidade, a projeção de saída como a identidade,
\[
W^{O} = I_{4},
\]
a saída final do bloco de \emph{multi-head attention} é
\[
\mathrm{MultiHead}(Q,K,V)
=
\Concat(H^{(1)}|H^{(2)}) W_{O}
\in \mathbb{R}^{3 \times 4}.
\]
\begin{center}
\begin{minipage}{0.85\linewidth}
\begin{tcolorbox}[title={Multi-Head Attention}, colback=white, boxsep=0.6mm, left=0mm]
\[
MultiHead(Q,K,V)=
\begin{bmatrix}
2.000 & 1.000 & 1.000 & 1.000 \\
1.670 & 1.330 & 1.670 & 0.330 \\
1.102 & 1.898 & 1.102 & 1.747
\end{bmatrix}
\begin{bmatrix}
1.000 & 0.000 & 0.000 & 0.000 \\
0.000 & 1.000 & 0.000 & 0.000 \\
0.000 & 0.000 & 1.000 & 0.000 \\
0.000 & 0.000 & 0.000 & 1.000
\end{bmatrix}
\]
\end{tcolorbox}
\end{minipage}
\hfill
\end{center}

\paragraph{Resultado final:}

\begin{center}
\begin{minipage}{0.70\linewidth}
\begin{tcolorbox}[title={Multi-Head Attention (novos \emph{embeddings} contextualizados)}, colback=white, boxsep=0.6mm, left=0mm]
\[
Attention(Q,K,V)=
\begin{bmatrix}
\text{Life} \\
\text{is} \\
\text{awesome}
\end{bmatrix}
=
\begin{bmatrix}
2.000 & 1.000 & 1.000 & 1.000 \\
1.670 & 1.330 & 1.670 & 0.330 \\
1.102 & 1.898 & 1.102 & 1.747
\end{bmatrix}
\]
\end{tcolorbox}
\end{minipage}
\hfill
\end{center}
\end{answer}

\begin{lstlisting}[language=R, caption={\textbf{Definição} dos blocos de multi-head attention treináveis}]
# -------------------------------------------------------------------------------------------------------
# Cada camada N da rede neural receberá um módulo de atenção multi-cabeças definido da seguinte forma:
# -------------------------------------------------------------------------------------------------------
self$MM <- torch::nn_module_list(lapply(1:N_Layers,
  function(x) torch::nn_multihead_attention(
    n_embd,               # d_model: dimensão do vetores de embedding
    N_Head,               # h: número de cabeças
    dropout = p0,         # tecnica para reduzir o overfitting
    batch_first = TRUE))) # informa a disposição dos dados à biblioteca torch
\end{lstlisting}

\begin{lstlisting}[language=R,caption={\textbf{Definição} da Máscara causal}]
# ---------------------------------------------------------------------------------------------------------
# Cria uma matriz TxT, sendo T = número de tokens na sequência de entrada
# Garante que será tratado no mesmo device que X (CPU ou GPU) para que eles possam ser operados juntos
# Aplica 1 à diagonal superior, transforma 1 em booleano TRUE para que a biblioteca torch saiba quais 
# posições precisam ser mascaradas
# ---------------------------------------------------------------------------------------------------------
wei <- torch::torch_triu(
  torch::torch_ones(T, T, device = x$device),
  diagonal = 1)$to(dtype = torch::torch_bool())
\end{lstlisting}

\begin{lstlisting}[language=R,caption={\textbf{Cálculo} da atenção multi-cabeças mascarada em cada camada}]
for (j in 1:self$N) {

  # -------------------------------------------------------------------------------------------------------
  # 1) Pré-normalização (LayerNorm)
  #    Normalização dos embeddings, já que todos embeddings interagirão com todos os outros.
  #    A normalização não altera a informação e estabiliza o treinamento.
  # -------------------------------------------------------------------------------------------------------
  QKV <- self$scale1[[j]](output)

  # -------------------------------------------------------------------------------------------------------
  # 2) Chamada da função Multi-Head Self-Attention mascarada do torch que calcula Q, K e V internamente a partir de QKV.
  # -------------------------------------------------------------------------------------------------------
  attn_out <- self$MM[[j]](
    query      = QKV,     # Input para o cálculo de Q
    key        = QKV,     # Input para o cálculo de K
    value      = QKV,     # Input para o cálculo de V
    attn_mask  = wei,     # Máscara causal
    need_weights = FALSE  # Dispensa o retorno dos pesos utilizados no cáculo da média ponderada
  )[[1]]                  # Guarda apenas o resultado da média ponderada, mas não os pesos

  # -------------------------------------------------------------------------------------------------------
  # 3) Conexão residual após atenção
  # Mantém a informação original dos embeddings e evita que a transformação da atenção distorça excessivamente a representação.
  # Estabiliza o fluxo do gradiente.
  # -------------------------------------------------------------------------------------------------------
  output <- output + attn_out

  # -------------------------------------------------------------------------------------------------------
  # 4) Feed-Forward Network + residual
  #  Aplicação de uma MLP ponto-a-ponto em cada token individualmente, capturando relações não lineares permitindo que a rede aprenda padrões complexos.
  #  Mantendo o resíduo e aplicando a normalização para estabilizar a operação.
  # -------------------------------------------------------------------------------------------------------
  output <- output + self$FFN[[j]]( self$scale2[[j]](output) )
}
\end{lstlisting}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\textbf{Questão 02:} Treine um modelo de linguagem com dimensão embedding de 128, duas camadas e duas 
cabeças para os dados de Shakespeare.

    \begin{answer}[]
\subsection*{Descrição do experimento e configuração do modelo}

\subsubsection*{Corpus utilizado}

O modelo foi treinado sobre um corpus textual extraído da edição completa 
das obras de William Shakespeare disponibilizada pelo \emph{Project Gutenberg}, 
um repositório digital que distribui gratuitamente obras que já se encontram 
em domínio público, contendo todo o conteúdo produzido pelo autor:
peças teatrais, poemas narrativos, sonetos, trechos introdutórios e notas 
editoriais presentes na edição digital, com aproximadamente 5{,}4 milhões de caracteres.

\begin{figure}[H]
\centering

\begin{minipage}{0.47\linewidth}
    \centering
    \includegraphics[width=\linewidth]{imagens/corpus1.png}
\end{minipage}
\hfill
\begin{minipage}{0.47\linewidth}
    \centering
    \includegraphics[width=\linewidth]{imagens/corpus2.png}
\end{minipage}

\caption{Trechos do corpus utilizado no experimento, provenientes da obra completa de Shakespeare disponibilizado pelo Projeto Gutenberg.}
\label{fig:corpus_shakespeare}
\end{figure}

\subsubsection*{Vocabulário extraído do corpus}

O modelo foi treinado no regime \textit{character-level}, o primeiro passo 
consistiu em extrair todos os caracteres distintos presentes no arquivo 
\texttt{Shakespeare.txt}. Inclui letras maiúsculas e minúsculas, dígitos, 
pontuação, símbolos especiais, acentos, quebras de linha e espaços.  

O vocabulário final contém \textbf{109 tokens}, sendo o primeiro reservado para o 
símbolo especial \texttt{<PAD>} utilizado em operações internas.  
Segue o conjunto de caracteres identificados:

\begin{figure}[H]
\centering
\includegraphics[width=0.95\linewidth]{imagens/voc.png}%
\caption{Vocabulário extraido no corpus utilizado (obra completa de Shakespeare).}
\label{fig:vocabulario}
\end{figure}


\subsubsection*{Hiperparâmetros do modelo}
Os hiperparâmetros utilizados no modelo são:
\begin{table}[H]
\centering
\caption{Hiperparâmetros utilizados no modelo GPT treinado.}
\begin{tabular}{lll}
\toprule
\textbf{Hiperparâmetro} & \textbf{Valor} & \textbf{Descrição}\\
\midrule
\texttt{block\_size}     & 50 & contexto máximo (janela do attention). \\
\texttt{n\_embd}         & 128 & dimensão dos embeddings. \\
\texttt{N\_Layers}       & 2 & número de blocos Transformer. \\
\texttt{N\_Head}         & 2 & número de cabeças de atenção. \\
\texttt{dropout}         & 0.2 & regularização. \\
\texttt{learning\_rate}  & 0.003 & taxa de aprendizado. \\
\texttt{batch\_size}     & 64 & tamanho do mini‐batch. \\
\texttt{epochs}          & 1200 & número total de épocas. \\
\bottomrule
\end{tabular}
\label{tab:hiperparametros}
\end{table}

O tamanho de contexto (\texttt{block\_size = 50}) limita o alcance da atenção foi ajustado empiricamente para conseguir captar o estilo do texto e ao mesmo tempo ser viável para ser treinado em hardware pessoal. 
A dimensão dos vetores de embedding (\texttt{n\_embd = 128}) é significativo, mas combinada ao número reduzido de camadas e a utilização de multiplas cabeças de atenção (\texttt{N\_Layers = 2}, \texttt{N\_Head = 2}), 
mantém o modelo ágil e compacto, suficiente para capturar regularidades sintáticas e estilísticas do texto. 
O dropout (\texttt{0.2}) fornece regularização. A taxa de aprendizado escolhida (\texttt{0.003}) foi ajustada empiricamente.

\subsubsection*{Arquitetura e fluxo do modelo}

A arquitetura utilizada neste trabalho segue o padrão dos modelos GPT do tipo \textit{decoder-only}, cujo fluxo computacional está ilustrado na Fig.~\ref{fig:arquitetura}. O processamento inicia-se pela soma entre o embedding dos tokens e o embedding posicional aprendido, resultando em uma representação contínua que preserva a ordem sequencial do texto. Em seguida, cada camada Transformer aplica uma normalização prévia (\textit{LayerNorm}) seguida de um bloco de autoatenção multi-cabeças mascarada, garantindo o comportamento autoregressivo: o modelo só pode utilizar informações de tokens passados para prever o próximo token.

\begin{figure}[H]
\centering
\includegraphics[width=0.5\linewidth]{imagens/model.png}
\caption{Arquitetura geral do modelo GPT utilizado no experimento.}
\label{fig:arquitetura}
\end{figure}
O resultado da atenção é então somado à entrada original por meio de uma conexão residual, estabilizando o fluxo de gradientes. 
Após isso, uma nova normalização é aplicada antes do bloco \textit{feed-forward} ponto-a-ponto, responsável por expandir e recomprimir a 
dimensionalidade interna, permitindo ao modelo capturar não-linearidades locais. 
Esse bloco também é seguido por uma conexão residual, completando a estrutura \textit{pré-norm} típica de arquiteturas modernas de Transformers. 
Ao final das camadas empilhadas (2) uma projeção linear — equivalente ao uso do embedding transposto — produz os logits sobre o vocabulário, 
posteriormente convertidos em probabilidades via \textit{softmax}, que indica o próximo token mais provável.

\subsubsection*{Tamanho do modelo}

O modelo GPT implementado apresenta um total de \textbf{418\,866 parâmetros} treináveis, distribuídos entre embeddings, projeções 
lineares das camadas de atenção, normalizações e redes feed-forward. A Tabela~\ref{tab:parametros} apresenta o detalhamento dos tensores de parâmetros que compõem o modelo.

\begin{table}[H]
\centering
\footnotesize
\begin{tabular}{lccc}
\toprule
\textbf{Parâmetro} & \textbf{Dimensão} & \textbf{Nº de parâmetros} \\
\midrule
wpe.weight                & 50 $\times$ 128   & 6\,400  \\
wte.weight                & 109 $\times$ 128  & 13\,952 \\
MM.0.in\_proj\_weight     & 384 $\times$ 128  & 49\,152 \\
MM.0.in\_proj\_bias       & 384               & 384 \\
MM.0.out\_proj\_weight    & 128 $\times$ 128  & 16\,384 \\
MM.0.out\_proj\_bias      & 128               & 128 \\
MM.1.in\_proj\_weight     & 384 $\times$ 128  & 49\,152 \\
MM.1.in\_proj\_bias       & 384               & 384 \\
MM.1.out\_proj\_weight    & 128 $\times$ 128  & 16\,384 \\
MM.1.out\_proj\_bias      & 128               & 128 \\
scale1.\{0,1\}.weight     & 128               & 128 + 128 \\
scale1.\{0,1\}.bias       & 128               & 128 + 128 \\
scale2.\{0,1\}.weight     & 128               & 128 + 128 \\
scale2.\{0,1\}.bias       & 128               & 128 + 128 \\
scale3.weight             & 128               & 128 \\
scale3.bias               & 128               & 128 \\
FFN.0.\{0,2\}.weight      & 512 $\times$ 128  & 65\,536 + 65\,536 \\
FFN.0.\{0,2\}.bias        & 512               & 512 + 128 \\
FFN.1.\{0,2\}.weight      & 512 $\times$ 128  & 65\,536 + 65\,536 \\
FFN.1.\{0,2\}.bias        & 512               & 512 + 128 \\
ln\_f.weight              & 109 $\times$ 128  & 13\,952 \\
\midrule
\textbf{Total}           & ---                & \textbf{418\,866} \\
\bottomrule
\end{tabular}
\caption{Parâmetros treináveis do modelo GPT utilizado no experimento.}
\label{tab:parametros}
\end{table}

\subsubsection*{Treinamento e métricas}

No treinamento, foi utilizada a função de perda \emph{cross-entropy},
computada entre os logits produzidos pelo modelo e os caracteres-alvo da sequência.
A cada época, registramos tanto a perda de treino quanto a de teste, permitindo
acompanhar a evolução do aprendizado e o possível surgimento de sobreajuste.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{imagens/loss.png}
    \caption{Evolução da função de perda durante o treinamento: comparação entre as curvas de treino e de teste ao longo das 1200 épocas.}
    \label{fig:loss}
\end{figure}


As curvas de perda apresentaram comportamento regular ao longo das épocas:
as perdas de treino e de teste permaneceram próximas, decrescendo de forma
suave sem indícios de divergência. Essa proximidade entre as curvas indica
ausência de sobreajuste, sugerindo que o modelo conseguiu aprender
padrões relevantes do corpus sem memorizar o conjunto de treinamento. Além disso,
a estabilidade observada nas últimas épocas reflete um processo de otimização
bem-sucedido.

O valor final da perda obtido após 1200 épocas foi de aproximadamente
\texttt{1.82} para o conjunto de treino e \texttt{1.78} para o conjunto de teste. 


\subsubsection*{Resultado}

Os resultados obtidos mostram que o modelo, apesar de compacto, aprendeu 
\emph{padrões estatísticos} presentes no inglês shakespeariano, como a distribuição típica de comprimentos 
de palavras, certa cadência rítmica, alternância entre diálogos e rubricas, além da reprodução aproximada 
de construções sintáticas características do período. 

Entretanto, o texto gerado permanece 
semanticamente incoerente, apresentando repetições excessivas e sentenças circulares. Esse comportamento 
é esperado para um modelo de pequena escala: ele captura a superfície do estilo, mas não reproduz 
a coerência e os encadeamentos semânticos ao longo do texto. O processo de tokenização mais robusto pode contribuir significativamente para melhorar a coerencia do texto.

\begin{figure}[H]
\centering
\begin{minipage}{0.45\linewidth}
    \centering
    \includegraphics[width=\linewidth]{imagens/resultado1.png}
\end{minipage}
\hfill
\begin{minipage}{0.45\linewidth}
    \centering
    \includegraphics[width=\linewidth]{imagens/resultado2.png}
\end{minipage}

\caption{Trechos do texto gerado pelo modelo treinado.}
\label{fig:texto-gerado}
\end{figure}


O experimento valida a implementação completa de um Transformer do tipo \emph{decoder-only}, 
confirmando que mesmo arquiteturas reduzidas conseguem capturar características linguísticas relevantes 
quando treinadas sobre um corpus particular. O modelo foi capaz de replicar com consistência o \emph{ritmo}, o \emph{fluxo} e a \emph{forma} geral 
do estilo shakespeariano.
    \end{answer}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\textbf{Questão 03:} Repita o item anterior sem o mecanismo de atenção (com múltiplas cabeças) e descreva 
o que ocorre no treinamento.
    
    \begin{answer}[]

    \end{answer}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}