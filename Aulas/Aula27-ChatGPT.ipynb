{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8186c3f0",
   "metadata": {},
   "source": [
    "## Direitos Autorais Modelo ChatGPT em R:\n",
    "Modelo apresentado na aula 27 do curso de estat√≠stica e Machine Learning do IME-USP.\n",
    "\n",
    "Autoria: Prof. Alexandre Galv√£o Patriota\n",
    "\n",
    "## üîπ Par√¢metros de Entrada do Modelo GPT\n",
    "\n",
    "A fun√ß√£o de inicializa√ß√£o do modelo recebe os seguintes argumentos:\n",
    "\n",
    "| **Par√¢metro** | **Significado** | **Interpreta√ß√£o** |\n",
    "|----------------|-----------------|-------------------------------|\n",
    "| **block_size** | Tamanho da janela de contexto (n√∫mero de tokens consecutivos considerados no bloco attention). | Define o comprimento da janela de condicionamento ‚Äî quantos tokens anteriores o modelo utiliza para prever o pr√≥ximo. |\n",
    "| **n_embd** | Dimens√£o do vetor de embedding. | Tamanho do espa√ßo latente cont√≠nuo onde cada token √© representado por um vetor real. |\n",
    "| **N_Layers** | N√∫mero de camadas do Transformer (profundidade da rede). | Quantas vezes o bloco ‚ÄúAten√ß√£o + Feed-Forward + Normaliza√ß√£o‚Äù √© repetido ao longo da rede. |\n",
    "| **nvoc** | Tamanho do vocabul√°rio. | N√∫mero de categorias poss√≠veis no modelo multinomial. Cada predi√ß√£o escolhe uma entre `nvoc` op√ß√µes (No m√©todo Greedy). |\n",
    "| **head** | N√∫mero de *heads* de aten√ß√£o. | Quantas proje√ß√µes paralelas de aten√ß√£o s√£o calculadas ‚Äî cada *head* modela um tipo distinto de depend√™ncia contextual. |\n",
    "| **p0** | Taxa de *dropout* (padr√£o 0.1). | Probabilidade de zerar aleatoriamente algumas ativa√ß√µes durante o treino, reduzindo o *overfitting*. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aefbfa7c",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(torch)\n",
    "\n",
    "GPT <- torch::nn_module(\n",
    "  initialize = function(block_size, n_embd, N_Layers, nvoc, Head, p0 = 0.1) {\n",
    "\n",
    "    self$N   <- N_Layers\n",
    "    self$wpe <- torch::nn_embedding(block_size, n_embd)\n",
    "    self$wte <- torch::nn_embedding(nvoc, n_embd, padding_idx = 1)\n",
    "\n",
    "    self$MM  <- torch::nn_module_list(lapply(\n",
    "      1:N_Layers,\n",
    "      function(x) torch::nn_multihead_attention(n_embd, Head, dropout = p0, batch_first = TRUE)\n",
    "    ))\n",
    "\n",
    "    self$scale1 <- torch::nn_module_list(lapply(\n",
    "      1:N_Layers,\n",
    "      function(x) torch::nn_layer_norm(n_embd)\n",
    "    ))\n",
    "\n",
    "    self$scale2 <- torch::nn_module_list(lapply(\n",
    "      1:N_Layers,\n",
    "      function(x) torch::nn_layer_norm(n_embd)\n",
    "    ))\n",
    "\n",
    "    self$scale3 <- torch::nn_layer_norm(n_embd, elementwise_affine = TRUE)\n",
    "\n",
    "    self$FFN <- torch::nn_module_list(lapply(\n",
    "      1:N_Layers,\n",
    "      function(x) {\n",
    "        torch::nn_sequential(\n",
    "          torch::nn_linear(n_embd, 4 * n_embd),\n",
    "          torch::nn_gelu(),\n",
    "          torch::nn_linear(4 * n_embd, n_embd),\n",
    "          torch::nn_dropout(p0)\n",
    "        )\n",
    "      }\n",
    "    ))\n",
    "\n",
    "    # cabe√ßa linear de sa√≠da (mantive seu nome ln_f)\n",
    "    self$ln_f  <- torch::nn_linear(n_embd, nvoc, bias = FALSE)\n",
    "    self$drop0 <- torch::nn_dropout(p = p0)\n",
    "  },\n",
    "\n",
    "  forward = function(x, return_intermediates = FALSE) {\n",
    "    # x: (B, T)\n",
    "    B <- x$size(1)\n",
    "    T <- x$size(2)\n",
    "\n",
    "    # posi√ß√µes 1..T (long)\n",
    "    x1 <- torch::torch_arange(1, T,\n",
    "      dtype = torch::torch_long(),\n",
    "      device = x$device\n",
    "    )\n",
    "\n",
    "    # m√°scara causal (pro√≠be olhar para o futuro)\n",
    "    wei <- torch::torch_triu(torch::torch_ones(T, T, device = x$device), diagonal = 1)$to(\n",
    "      dtype = torch::torch_bool()\n",
    "    )\n",
    "\n",
    "    # embeddings\n",
    "    output <- self$wte(x) + self$wpe(x1)$unsqueeze(1)  # (B, T, E)\n",
    "    output <- self$drop0(output)\n",
    "\n",
    "    # (opcional) inspe√ß√£o r√°pida\n",
    "    # cat(\"wei shape:\", as.character(wei$size()), \"\\n\"); print(wei$to(dtype = torch_int()))\n",
    "    # cat(\"x1 shape:\", as.character(x1$size()), \"\\n\"); print(x1)\n",
    "\n",
    "    for (j in 1:self$N) {\n",
    "      # pr√©-norm + aten√ß√£o multihead\n",
    "      QKV <- self$scale1[[j]](output)  # (B, T, E) pois batch_first = TRUE\n",
    "      attn_out <- self$MM[[j]](\n",
    "        query = QKV, key = QKV, value = QKV,\n",
    "        attn_mask = wei, need_weights = FALSE\n",
    "      )[[1]]\n",
    "      output <- output + attn_out\n",
    "\n",
    "      # feed-forward com pr√©-norm\n",
    "      output <- output + self$FFN[[j]](self$scale2[[j]](output))\n",
    "    }\n",
    "\n",
    "    # norm final + cabe√ßa linear ‚Üí logits (B, T, nvoc)\n",
    "    output <- self$scale3(output)\n",
    "    logits <- self$ln_f(output)\n",
    "\n",
    "    if (return_intermediates) {\n",
    "      return(list(\n",
    "        x1    = x1$to(\"cpu\"),\n",
    "        wei   = wei$to(dtype = torch_int())$to(\"cpu\"),\n",
    "        out   = output$to(\"cpu\"),\n",
    "        logits = logits$to(\"cpu\")\n",
    "      ))\n",
    "    }\n",
    "    logits\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "451728d4",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Criar uma inst√¢ncia do modelo\n",
    "model <- GPT(\n",
    "  block_size = 4, #8\n",
    "  n_embd = 8, #16\n",
    "  N_Layers = 2, #2\n",
    "  nvoc = 4, #20\n",
    "  Head = 2 #2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97b6e8ad",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in (function (weight, indices, padding_idx, scale_grad_by_freq, : Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got CPUFloatType instead (while checking arguments for embedding)\nException raised from checkScalarTypes at /Users/runner/work/libtorch-mac-m1/libtorch-mac-m1/pytorch/aten/src/ATen/TensorUtils.cpp:203 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>>) + 52 (0x10456c55c in libc10.dylib)\nframe #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const&) + 140 (0x1045691ac in libc10.dylib)\nframe #2: at::checkScalarTypes(char const*, at::TensorArg const&, c10::ArrayRef<c10::ScalarType>) + 480 (0x3000c36a0 in libtorch_cpu.dylib)\nframe #3: at::native::embedding_symint(at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool) + 120 (0x3004cdf7c in libtorch_cpu.dylib)\nframe #4: at::(anonymous namespace)::(anonymous namespace)::wrapper_CompositeExplicitAutograd__embedding(at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool) + 84 (0x3016905d8 in libtorch_cpu.dylib)\nframe #5: c10::impl::wrap_kernel_functor_unboxed_<c10::impl::detail::WrapFunctionIntoFunctor_<c10::CompileTimeFunctionPointer<at::Tensor (at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool), &at::(anonymous namespace)::(anonymous namespace)::wrapper_CompositeExplicitAutograd__embedding(at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool)>, at::Tensor, c10::guts::typelist::typelist<at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool>>, at::Tensor (at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool)>::call(c10::OperatorKernel*, c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool) + 52 (0x30173fccc in libtorch_cpu.dylib)\nframe #6: at::Tensor c10::Dispatcher::redispatch<at::Tensor, at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool>(c10::TypedOperatorHandle<at::Tensor (at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool)> const&, c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool) const + 132 (0x3012f16e4 in libtorch_cpu.dylib)\nframe #7: at::_ops::embedding::redispatch(c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool) + 156 (0x30123e268 in libtorch_cpu.dylib)\nframe #8: c10::impl::wrap_kernel_functor_unboxed_<c10::impl::detail::WrapFunctionIntoFunctor_<c10::CompileTimeFunctionPointer<at::Tensor (c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool), &torch::autograd::VariableType::(anonymous namespace)::embedding(c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool)>, at::Tensor, c10::guts::typelist::typelist<c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool>>, at::Tensor (c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool)>::call(c10::OperatorKernel*, c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool) + 1140 (0x303a22ef8 in libtorch_cpu.dylib)\nframe #9: at::_ops::embedding::call(at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool) + 364 (0x30123dc24 in libtorch_cpu.dylib)\nframe #10: at::embedding(at::Tensor const&, at::Tensor const&, long long, bool, bool) + 120 (0x10aaa652c in liblantern.dylib)\nframe #11: _lantern_embedding_tensor_tensor_intt_bool_bool + 180 (0x10aaa5ea8 in liblantern.dylib)\nframe #12: cpp_torch_namespace_embedding_weight_Tensor_indices_Tensor(XPtrTorchTensor, XPtrTorchIndexTensor, XPtrTorchint64_t, XPtrTorchbool, XPtrTorchbool) + 104 (0x109f99528 in torchpkg.so)\nframe #13: _torch_cpp_torch_namespace_embedding_weight_Tensor_indices_Tensor + 620 (0x109a76aac in torchpkg.so)\nframe #14: R_doDotCall + 1612 (0x1016eff4c in libR.dylib)\nframe #15: bcEval_loop + 128100 (0x10174c2a4 in libR.dylib)\nframe #16: bcEval + 684 (0x10171f46c in libR.dylib)\nframe #17: Rf_eval + 556 (0x10171eb6c in libR.dylib)\nframe #18: R_execClosure + 812 (0x10172172c in libR.dylib)\nframe #19: applyClosure_core + 164 (0x101720824 in libR.dylib)\nframe #20: Rf_eval + 1224 (0x10171ee08 in libR.dylib)\nframe #21: do_docall + 644 (0x1016bd484 in libR.dylib)\nframe #22: bcEval_loop + 40204 (0x101736b4c in libR.dylib)\nframe #23: bcEval + 684 (0x10171f46c in libR.dylib)\nframe #24: Rf_eval + 556 (0x10171eb6c in libR.dylib)\nframe #25: R_execClosure + 812 (0x10172172c in libR.dylib)\nframe #26: applyClosure_core + 164 (0x101720824 in libR.dylib)\nframe #27: Rf_eval + 1224 (0x10171ee08 in libR.dylib)\nframe #28: do_begin + 396 (0x101723fcc in libR.dylib)\nframe #29: Rf_eval + 1012 (0x10171ed34 in libR.dylib)\nframe #30: R_execClosure + 812 (0x10172172c in libR.dylib)\nframe #31: applyClosure_core + 164 (0x101720824 in libR.dylib)\nframe #32: Rf_eval + 1224 (0x10171ee08 in libR.dylib)\nframe #33: Rf_evalList + 204 (0x10171f80c in libR.dylib)\nframe #34: Rf_eval + 1312 (0x10171ee60 in libR.dylib)\nframe #35: do_set + 360 (0x101724fe8 in libR.dylib)\nframe #36: Rf_eval + 1012 (0x10171ed34 in libR.dylib)\nframe #37: do_begin + 396 (0x101723fcc in libR.dylib)\nframe #38: Rf_eval + 1012 (0x10171ed34 in libR.dylib)\nframe #39: R_execClosure + 812 (0x10172172c in libR.dylib)\nframe #40: applyClosure_core + 164 (0x101720824 in libR.dylib)\nframe #41: Rf_eval + 1224 (0x10171ee08 in libR.dylib)\nframe #42: do_set + 360 (0x101724fe8 in libR.dylib)\nframe #43: Rf_eval + 1012 (0x10171ed34 in libR.dylib)\nframe #44: do_eval + 1352 (0x101726048 in libR.dylib)\nframe #45: bcEval_loop + 40204 (0x101736b4c in libR.dylib)\nframe #46: bcEval + 684 (0x10171f46c in libR.dylib)\nframe #47: Rf_eval + 556 (0x10171eb6c in libR.dylib)\nframe #48: forcePromise + 232 (0x10171f6a8 in libR.dylib)\nframe #49: Rf_eval + 660 (0x10171ebd4 in libR.dylib)\nframe #50: do_withVisible + 64 (0x101726380 in libR.dylib)\nframe #51: do_internal + 400 (0x10178f190 in libR.dylib)\nframe #52: bcEval_loop + 40764 (0x101736d7c in libR.dylib)\nframe #53: bcEval + 684 (0x10171f46c in libR.dylib)\nframe #54: Rf_eval + 556 (0x10171eb6c in libR.dylib)\nframe #55: R_execClosure + 812 (0x10172172c in libR.dylib)\nframe #56: applyClosure_core + 164 (0x101720824 in libR.dylib)\nframe #57: Rf_eval + 1224 (0x10171ee08 in libR.dylib)\nframe #58: do_begin + 396 (0x101723fcc in libR.dylib)\nframe #59: Rf_eval + 1012 (0x10171ed34 in libR.dylib)\nframe #60: Rf_eval + 1012 (0x10171ed34 in libR.dylib)\nframe #61: Rf_eval + 1012 (0x10171ed34 in libR.dylib)\nframe #62: do_begin + 396 (0x101723fcc in libR.dylib)\n\n",
     "output_type": "error",
     "traceback": [
      "Error in (function (weight, indices, padding_idx, scale_grad_by_freq, : Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got CPUFloatType instead (while checking arguments for embedding)\nException raised from checkScalarTypes at /Users/runner/work/libtorch-mac-m1/libtorch-mac-m1/pytorch/aten/src/ATen/TensorUtils.cpp:203 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>>) + 52 (0x10456c55c in libc10.dylib)\nframe #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const&) + 140 (0x1045691ac in libc10.dylib)\nframe #2: at::checkScalarTypes(char const*, at::TensorArg const&, c10::ArrayRef<c10::ScalarType>) + 480 (0x3000c36a0 in libtorch_cpu.dylib)\nframe #3: at::native::embedding_symint(at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool) + 120 (0x3004cdf7c in libtorch_cpu.dylib)\nframe #4: at::(anonymous namespace)::(anonymous namespace)::wrapper_CompositeExplicitAutograd__embedding(at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool) + 84 (0x3016905d8 in libtorch_cpu.dylib)\nframe #5: c10::impl::wrap_kernel_functor_unboxed_<c10::impl::detail::WrapFunctionIntoFunctor_<c10::CompileTimeFunctionPointer<at::Tensor (at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool), &at::(anonymous namespace)::(anonymous namespace)::wrapper_CompositeExplicitAutograd__embedding(at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool)>, at::Tensor, c10::guts::typelist::typelist<at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool>>, at::Tensor (at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool)>::call(c10::OperatorKernel*, c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool) + 52 (0x30173fccc in libtorch_cpu.dylib)\nframe #6: at::Tensor c10::Dispatcher::redispatch<at::Tensor, at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool>(c10::TypedOperatorHandle<at::Tensor (at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool)> const&, c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool) const + 132 (0x3012f16e4 in libtorch_cpu.dylib)\nframe #7: at::_ops::embedding::redispatch(c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool) + 156 (0x30123e268 in libtorch_cpu.dylib)\nframe #8: c10::impl::wrap_kernel_functor_unboxed_<c10::impl::detail::WrapFunctionIntoFunctor_<c10::CompileTimeFunctionPointer<at::Tensor (c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool), &torch::autograd::VariableType::(anonymous namespace)::embedding(c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool)>, at::Tensor, c10::guts::typelist::typelist<c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool>>, at::Tensor (c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool)>::call(c10::OperatorKernel*, c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool) + 1140 (0x303a22ef8 in libtorch_cpu.dylib)\nframe #9: at::_ops::embedding::call(at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool) + 364 (0x30123dc24 in libtorch_cpu.dylib)\nframe #10: at::embedding(at::Tensor const&, at::Tensor const&, long long, bool, bool) + 120 (0x10aaa652c in liblantern.dylib)\nframe #11: _lantern_embedding_tensor_tensor_intt_bool_bool + 180 (0x10aaa5ea8 in liblantern.dylib)\nframe #12: cpp_torch_namespace_embedding_weight_Tensor_indices_Tensor(XPtrTorchTensor, XPtrTorchIndexTensor, XPtrTorchint64_t, XPtrTorchbool, XPtrTorchbool) + 104 (0x109f99528 in torchpkg.so)\nframe #13: _torch_cpp_torch_namespace_embedding_weight_Tensor_indices_Tensor + 620 (0x109a76aac in torchpkg.so)\nframe #14: R_doDotCall + 1612 (0x1016eff4c in libR.dylib)\nframe #15: bcEval_loop + 128100 (0x10174c2a4 in libR.dylib)\nframe #16: bcEval + 684 (0x10171f46c in libR.dylib)\nframe #17: Rf_eval + 556 (0x10171eb6c in libR.dylib)\nframe #18: R_execClosure + 812 (0x10172172c in libR.dylib)\nframe #19: applyClosure_core + 164 (0x101720824 in libR.dylib)\nframe #20: Rf_eval + 1224 (0x10171ee08 in libR.dylib)\nframe #21: do_docall + 644 (0x1016bd484 in libR.dylib)\nframe #22: bcEval_loop + 40204 (0x101736b4c in libR.dylib)\nframe #23: bcEval + 684 (0x10171f46c in libR.dylib)\nframe #24: Rf_eval + 556 (0x10171eb6c in libR.dylib)\nframe #25: R_execClosure + 812 (0x10172172c in libR.dylib)\nframe #26: applyClosure_core + 164 (0x101720824 in libR.dylib)\nframe #27: Rf_eval + 1224 (0x10171ee08 in libR.dylib)\nframe #28: do_begin + 396 (0x101723fcc in libR.dylib)\nframe #29: Rf_eval + 1012 (0x10171ed34 in libR.dylib)\nframe #30: R_execClosure + 812 (0x10172172c in libR.dylib)\nframe #31: applyClosure_core + 164 (0x101720824 in libR.dylib)\nframe #32: Rf_eval + 1224 (0x10171ee08 in libR.dylib)\nframe #33: Rf_evalList + 204 (0x10171f80c in libR.dylib)\nframe #34: Rf_eval + 1312 (0x10171ee60 in libR.dylib)\nframe #35: do_set + 360 (0x101724fe8 in libR.dylib)\nframe #36: Rf_eval + 1012 (0x10171ed34 in libR.dylib)\nframe #37: do_begin + 396 (0x101723fcc in libR.dylib)\nframe #38: Rf_eval + 1012 (0x10171ed34 in libR.dylib)\nframe #39: R_execClosure + 812 (0x10172172c in libR.dylib)\nframe #40: applyClosure_core + 164 (0x101720824 in libR.dylib)\nframe #41: Rf_eval + 1224 (0x10171ee08 in libR.dylib)\nframe #42: do_set + 360 (0x101724fe8 in libR.dylib)\nframe #43: Rf_eval + 1012 (0x10171ed34 in libR.dylib)\nframe #44: do_eval + 1352 (0x101726048 in libR.dylib)\nframe #45: bcEval_loop + 40204 (0x101736b4c in libR.dylib)\nframe #46: bcEval + 684 (0x10171f46c in libR.dylib)\nframe #47: Rf_eval + 556 (0x10171eb6c in libR.dylib)\nframe #48: forcePromise + 232 (0x10171f6a8 in libR.dylib)\nframe #49: Rf_eval + 660 (0x10171ebd4 in libR.dylib)\nframe #50: do_withVisible + 64 (0x101726380 in libR.dylib)\nframe #51: do_internal + 400 (0x10178f190 in libR.dylib)\nframe #52: bcEval_loop + 40764 (0x101736d7c in libR.dylib)\nframe #53: bcEval + 684 (0x10171f46c in libR.dylib)\nframe #54: Rf_eval + 556 (0x10171eb6c in libR.dylib)\nframe #55: R_execClosure + 812 (0x10172172c in libR.dylib)\nframe #56: applyClosure_core + 164 (0x101720824 in libR.dylib)\nframe #57: Rf_eval + 1224 (0x10171ee08 in libR.dylib)\nframe #58: do_begin + 396 (0x101723fcc in libR.dylib)\nframe #59: Rf_eval + 1012 (0x10171ed34 in libR.dylib)\nframe #60: Rf_eval + 1012 (0x10171ed34 in libR.dylib)\nframe #61: Rf_eval + 1012 (0x10171ed34 in libR.dylib)\nframe #62: do_begin + 396 (0x101723fcc in libR.dylib)\n\nTraceback:\n",
      "1. model(x, return_intermediates = TRUE)",
      "2. self$wte(x)   # at line 61 of file <text>",
      "3. nnf_embedding(input, self$weight, self$padding_idx, self$max_norm, \n .     self$norm_type, self$scale_grad_by_freq, self$sparse)",
      "4. torch_embedding(weight = weight, indices = input, padding_idx = padding_idx, \n .     scale_grad_by_freq = scale_grad_by_freq, sparse = sparse)",
      "5. call_c_function(fun_name = \"embedding\", args = args, expected_types = expected_types, \n .     nd_args = nd_args, return_types = return_types, fun_type = \"namespace\")",
      "6. do_call(f, args)",
      "7. do.call(fun, args)",
      "8. (function (weight, indices, padding_idx, scale_grad_by_freq, \n .     sparse) \n . {\n .     .Call(`_torch_cpp_torch_namespace_embedding_weight_Tensor_indices_Tensor`, \n .         weight, indices, padding_idx, scale_grad_by_freq, sparse)\n . })(weight = <pointer: 0x1528a6150>, indices = <pointer: 0x135a85090>, \n .     padding_idx = 1, scale_grad_by_freq = FALSE, sparse = FALSE)"
     ]
    }
   ],
   "source": [
    "# Criar uma entrada fict√≠cia (batch de tokens)\n",
    "x <- torch::torch_tensor(matrix(c(1,2,3,4,5,6,7,8), nrow = 1))\n",
    "\n",
    "# Executar o modelo\n",
    "res <- model(x, return_intermediates = TRUE)\n",
    "\n",
    "# Visualizar vari√°veis intermedi√°rias\n",
    "res$x1\n",
    "res$wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688ebde8",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
