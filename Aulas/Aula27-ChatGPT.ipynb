{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8186c3f0",
   "metadata": {},
   "source": [
    "## Direitos Autorais Modelo ChatGPT em R:\n",
    "Modelo apresentado na aula 27 do curso de estat√≠stica e Machine Learning do IME-USP.\n",
    "\n",
    "Autoria: Prof. Alexandre Galv√£o Patriota\n",
    "\n",
    "## üîπ Par√¢metros de Entrada do Modelo GPT\n",
    "\n",
    "A fun√ß√£o de inicializa√ß√£o do modelo recebe os seguintes argumentos:\n",
    "\n",
    "| **Par√¢metro** | **Significado** | **Interpreta√ß√£o** |\n",
    "|----------------|-----------------|-------------------------------|\n",
    "| **block_size** | Tamanho da janela de contexto (n√∫mero de tokens consecutivos considerados no bloco attention). | Define o comprimento da janela de condicionamento ‚Äî quantos tokens anteriores o modelo utiliza para prever o pr√≥ximo. |\n",
    "| **n_embd** | Dimens√£o do vetor de embedding. | Tamanho do espa√ßo latente cont√≠nuo onde cada token √© representado por um vetor real. |\n",
    "| **N_Layers** | N√∫mero de camadas do Transformer (profundidade da rede). | Quantas vezes o bloco ‚ÄúAten√ß√£o + Feed-Forward + Normaliza√ß√£o‚Äù √© repetido ao longo da rede. |\n",
    "| **nvoc** | Tamanho do vocabul√°rio. | N√∫mero de categorias poss√≠veis no modelo multinomial. Cada predi√ß√£o escolhe uma entre `nvoc` op√ß√µes (No m√©todo Greedy). |\n",
    "| **head** | N√∫mero de *heads* de aten√ß√£o. | Quantas proje√ß√µes paralelas de aten√ß√£o s√£o calculadas ‚Äî cada *head* modela um tipo distinto de depend√™ncia contextual. |\n",
    "| **p0** | Taxa de *dropout* (padr√£o 0.1). | Probabilidade de zerar aleatoriamente algumas ativa√ß√µes durante o treino, reduzindo o *overfitting*. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7737797e",
   "metadata": {},
   "source": [
    "## Par√¢metros de configura√ß√£o "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "24aa1f9d",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "config <- list(\n",
    "  #Corpus for training (global)\n",
    "  file_name = \"ABC.txt\",\n",
    "  train = !TRUE,\n",
    "  run = TRUE,\n",
    "  read_weights = !TRUE,\n",
    "\n",
    "  #gpt parameters (global)\n",
    "  block_size = 16,   #Maximum context\n",
    "  n_embd = 128,      #Embedding dimension\n",
    "  N_Layers = 2,      #Number of layers\n",
    "  Head = 2,          #Number of heads\n",
    "\n",
    "  #Training parameters (global)\n",
    "  lr = 0.003,        #Learning rate\n",
    "  batch_size = 64,   #Batch size\n",
    "  p0 = 0.2,          #Dropout proportion\n",
    "  epochs = 400,        #Number of epochs\n",
    "  num_workers = 6,  #Number of cpu workers\n",
    "\n",
    "  max_new_tokens = 700\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "aefbfa7c",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(torch)\n",
    "\n",
    "GPT <- torch::nn_module(\n",
    "  initialize = function(block_size, n_embd, N_Layers, nvoc, Head, p0 = 0.1) {\n",
    "\n",
    "    self$N   <- N_Layers\n",
    "    self$wpe <- torch::nn_embedding(block_size, n_embd)\n",
    "    self$wte <- torch::nn_embedding(nvoc, n_embd, padding_idx = 1)\n",
    "\n",
    "    self$MM  <- torch::nn_module_list(lapply(\n",
    "      1:N_Layers,\n",
    "      function(x) torch::nn_multihead_attention(n_embd, Head, dropout = p0, batch_first = TRUE)\n",
    "    ))\n",
    "\n",
    "    self$scale1 <- torch::nn_module_list(lapply(\n",
    "      1:N_Layers,\n",
    "      function(x) torch::nn_layer_norm(n_embd)\n",
    "    ))\n",
    "\n",
    "    self$scale2 <- torch::nn_module_list(lapply(\n",
    "      1:N_Layers,\n",
    "      function(x) torch::nn_layer_norm(n_embd)\n",
    "    ))\n",
    "\n",
    "    self$scale3 <- torch::nn_layer_norm(n_embd, elementwise_affine = TRUE)\n",
    "\n",
    "    self$FFN <- torch::nn_module_list(lapply(\n",
    "      1:N_Layers,\n",
    "      function(x) {\n",
    "        torch::nn_sequential(\n",
    "          torch::nn_linear(n_embd, 4 * n_embd),\n",
    "          torch::nn_gelu(),\n",
    "          torch::nn_linear(4 * n_embd, n_embd),\n",
    "          torch::nn_dropout(p0)\n",
    "        )\n",
    "      }\n",
    "    ))\n",
    "\n",
    "    # cabe√ßa linear de sa√≠da (mantive seu nome ln_f)\n",
    "    self$ln_f  <- torch::nn_linear(n_embd, nvoc, bias = FALSE)\n",
    "    self$drop0 <- torch::nn_dropout(p = p0)\n",
    "  },\n",
    "\n",
    "  forward = function(x, return_intermediates = FALSE) {\n",
    "    # x: (B, T)\n",
    "    B <- x$size(1)\n",
    "    T <- x$size(2)\n",
    "\n",
    "    # posi√ß√µes 1..T (long)\n",
    "    x1 <- torch::torch_arange(1, T,\n",
    "      dtype = torch::torch_long(),\n",
    "      device = x$device\n",
    "    )\n",
    "\n",
    "    # m√°scara causal (pro√≠be olhar para o futuro)\n",
    "    wei <- torch::torch_triu(torch::torch_ones(T, T, device = x$device), diagonal = 1)$to(\n",
    "      dtype = torch::torch_bool()\n",
    "    )\n",
    "\n",
    "    # embeddings\n",
    "    output <- self$wte(x) + self$wpe(x1)$unsqueeze(1)  # (B, T, E)\n",
    "    output <- self$drop0(output)\n",
    "\n",
    "    # (opcional) inspe√ß√£o r√°pida\n",
    "    # cat(\"wei shape:\", as.character(wei$size()), \"\\n\"); print(wei$to(dtype = torch_int()))\n",
    "    # cat(\"x1 shape:\", as.character(x1$size()), \"\\n\"); print(x1)\n",
    "\n",
    "    for (j in 1:self$N) {\n",
    "      # pr√©-norm + aten√ß√£o multihead\n",
    "      QKV <- self$scale1[[j]](output)  # (B, T, E) pois batch_first = TRUE\n",
    "      attn_out <- self$MM[[j]](\n",
    "        query = QKV, key = QKV, value = QKV,\n",
    "        attn_mask = wei, need_weights = FALSE\n",
    "      )[[1]]\n",
    "      output <- output + attn_out\n",
    "\n",
    "      # feed-forward com pr√©-norm\n",
    "      output <- output + self$FFN[[j]](self$scale2[[j]](output))\n",
    "    }\n",
    "\n",
    "    # norm final + cabe√ßa linear ‚Üí logits (B, T, nvoc)\n",
    "    output <- self$scale3(output)\n",
    "    logits <- self$ln_f(output)\n",
    "\n",
    "    if (return_intermediates) {\n",
    "      return(list(\n",
    "        x1     = x1$cpu(),\n",
    "        wei    = wei$to(dtype = torch_int())$cpu(),\n",
    "        out    = output$cpu(),\n",
    "        logits = logits$cpu()\n",
    "      ))\n",
    "    }\n",
    "    logits\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f1cf12",
   "metadata": {},
   "source": [
    "## Visualizando todas as estruturas que compoem o modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "eb02d40d",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch_tensor\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5\n",
       " 6\n",
       " 7\n",
       " 8\n",
       "[ CPULongType{8} ]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch_tensor\n",
       " 0  1  1  1  1  1  1  1\n",
       " 0  0  1  1  1  1  1  1\n",
       " 0  0  0  1  1  1  1  1\n",
       " 0  0  0  0  1  1  1  1\n",
       " 0  0  0  0  0  1  1  1\n",
       " 0  0  0  0  0  0  1  1\n",
       " 0  0  0  0  0  0  0  1\n",
       " 0  0  0  0  0  0  0  0\n",
       "[ CPUIntType{8,8} ]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(torch)\n",
    "\n",
    "# instanciar um modelo simples\n",
    "model <- GPT(\n",
    "  block_size = 8,\n",
    "  n_embd = 16,\n",
    "  N_Layers = 2,\n",
    "  nvoc = 32,\n",
    "  Head = 2\n",
    ")\n",
    "\n",
    "# entrada: 1 batch, T=8, tipo long (√≠ndices de tokens)\n",
    "x <- torch_tensor(matrix(1:8, nrow = 1))\n",
    "\n",
    "# executar e inspecionar intermedi√°rios\n",
    "res <- model(x, return_intermediates = TRUE)\n",
    "res$x1      # posi√ß√µes 1..T\n",
    "res$wei     # m√°scara causal (T x T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9c06f4",
   "metadata": {},
   "source": [
    "## üß† Fluxo Computacional do Modelo `GPT` (Fun√ß√£o `forward()`)\n",
    "\n",
    "```text\n",
    "Entrada x  (tokens inteiros)\n",
    "      ‚îÇ\n",
    "      ‚ñº\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë  üîπ Embeddings                               ‚ïë\n",
    "‚ïë  self$wte(x)  ‚Üí embedding sem√¢ntico          ‚ïë\n",
    "‚ïë  self$wpe(x1) ‚Üí embedding posicional         ‚ïë\n",
    "‚ïë  output = wte(x) + wpe(x1)                   ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "      ‚îÇ\n",
    "      ‚ñº\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë  üîπ M√°scara causal (wei)                     ‚ïë\n",
    "‚ïë  Matriz (T√óT) triangular superior = 1        ‚ïë\n",
    "‚ïë  ‚Üí impede que o token veja o \"futuro\"        ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "      ‚îÇ\n",
    "      ‚ñº\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë  üîÅ N vezes (para cada camada j)              ‚ïë\n",
    "‚ïë  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚ïë\n",
    "‚ïë  ‚îÇ 1. Normaliza√ß√£o:  self$scale1[[j]](x)   ‚îÇ ‚ïë\n",
    "‚ïë  ‚îÇ 2. Multi-Head Attention:                ‚îÇ ‚ïë\n",
    "‚ïë  ‚îÇ    Q,K,V = output                       ‚îÇ ‚ïë\n",
    "‚ïë  ‚îÇ    attn_out = self$MM[[j]](Q,K,V,mask)  ‚îÇ ‚ïë\n",
    "‚ïë  ‚îÇ 3. Res√≠duo:  output ‚Üê output + attn_out ‚îÇ ‚ïë\n",
    "‚ïë  ‚îÇ 4. Normaliza√ß√£o:  self$scale2[[j]](x)   ‚îÇ ‚ïë\n",
    "‚ïë  ‚îÇ 5. Feed-Forward: self$FFN[[j]](...)     ‚îÇ ‚ïë\n",
    "‚ïë  ‚îÇ 6. Res√≠duo:  output ‚Üê output + FFN_out  ‚îÇ ‚ïë\n",
    "‚ïë  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "      ‚îÇ\n",
    "      ‚ñº\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë  üîπ Normaliza√ß√£o final + Cabe√ßa Linear        ‚ïë\n",
    "‚ïë  output ‚Üê self$scale3(output)                 ‚ïë\n",
    "‚ïë  logits ‚Üê self$ln_f(output)                   ‚ïë\n",
    "‚ïë  (dimens√µes: [batch, seq_len, vocabul√°rio])   ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "      ‚îÇ\n",
    "      ‚ñº\n",
    "Sa√≠da: `logits`  ‚Üí pontua√ß√µes para cada token poss√≠vel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41a560c",
   "metadata": {},
   "source": [
    "## üß© Interpreta√ß√£o \n",
    "\n",
    "| **Etapa** | **Significado Matem√°tico** | **Interpreta√ß√£o Estat√≠stica** |\n",
    "|------------|----------------------------|--------------------------------|\n",
    "| **wte(x)** | embedding de palavras | converte cada token discreto em vetor cont√≠nuo ( x‚Çú ‚àà ‚Ñù·µê ) |\n",
    "| **wpe(x1)** | embedding posicional | injeta informa√ß√£o de ordem (posi√ß√£o temporal) |\n",
    "| **wei** | m√°scara causal | implementa  P(X‚Çú‚Çä‚ÇÅ ‚à£ X‚ÇÅ:‚Çú),  proibindo olhar para  X‚Çç‚Çä‚Çú‚Çé |\n",
    "| **MM (multihead)** | autoaten√ß√£o | estima depend√™ncias condicionais entre tokens |\n",
    "| **FFN** | feed-forward | mistura n√£o linear ‚Äî ajusta as representa√ß√µes locais |\n",
    "| **ln_f** | camada linear final | converte o espa√ßo latente em logits para o vocabul√°rio |\n",
    "| **logits** | sa√≠da final | aproxima  PÃÇ_Œ∏(X‚Çú‚Çä‚ÇÅ = v·µ¢ ‚à£ X‚ÇÅ:‚Çú) |\n",
    "\n",
    "---\n",
    "\n",
    "### üìò **Resumo**\n",
    "\n",
    "O `forward()` implementa o c√°lculo da **verossimilhan√ßa condicional**  \n",
    "f_Œ∏(X‚Çú‚Çä‚ÇÅ ‚à£ X‚ÇÅ:‚Çú)  \n",
    "atrav√©s de uma sequ√™ncia de transforma√ß√µes:\n",
    "\n",
    "‚û°Ô∏è **embeddings ‚Üí aten√ß√£o ‚Üí normaliza√ß√µes ‚Üí logits**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f68624",
   "metadata": {},
   "source": [
    "### Gerando o texto referencia de treino para o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "688ebde8",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Gera√ß√£o de dados ABC ‚Äî sequ√™ncias Markovianas\n",
    "# ============================================================\n",
    "\n",
    "file_name=\"ABC.txt\"\n",
    "# Defini√ß√£o do vocabul√°rio\n",
    "voc <- c(\"AABCBBC\", \"BCABCCA\", \"CAAACB\\n\\n\")\n",
    "#p <- rbind(c(0.4,0.4,0.2), c(0.6, 0.2, 0.2), c(0.7, 0.2, 0.1))\n",
    "p <- list()\n",
    "p[[1]] <- cbind(c(0.4, 0.4, 0.2))\n",
    "set.seed(1)\n",
    "M = 150000\n",
    "# for( i in 2:M)\n",
    "#    p[[t]] = p%*%p[[i - 1]]\n",
    "\n",
    "aux = function(s) voc[sample(1:3, 1, prob=p[[1]])]\n",
    "ABC <- paste(sapply(1:M, aux), collapse=\"\")\n",
    "write(ABC[1], file=file_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f8ec11",
   "metadata": {},
   "source": [
    "## Extra√ß√£o do vocabul√°rio do texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "1b77c2bc",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"<PAD>\" \"\\n\"    \"A\"     \"B\"     \"C\"    \n"
     ]
    }
   ],
   "source": [
    "#encoding into token ids\n",
    "\n",
    "file <- base::readChar(config$file_name, file.info(config$file_name)$size)\n",
    "voc <- c(\"<PAD>\", sort(unique(unlist(strsplit(file, \"\")))))\n",
    "print(voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "1461fbec",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "Encoder = function(file = file0, vocabulary = voc){\n",
    "  file = unlist(strsplit(file, \"\"))\n",
    "  filex = numeric(length(file))\n",
    "  for(i in 1:length(vocabulary)){\n",
    "    filex[file == vocabulary[i]] <- i\n",
    "  }\n",
    "  return(filex)\n",
    "}\n",
    "\n",
    "Decoder = function(file = file1, vocabulary = voc){\n",
    "  filex = file\n",
    "  for(i in 1:length(vocabulary)){\n",
    "    filex[file == i] <- vocabulary[i]\n",
    "  }\n",
    "  return(filex)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "352a85d7",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 5\n"
     ]
    }
   ],
   "source": [
    "encoded <- Encoder(file = file, vocabulary = voc)\n",
    "nvoc <- length(voc)\n",
    "print(nvoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "8c9ddc34",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>3</li><li>4</li><li>3</li><li>5</li><li>2</li><li>5</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 3\n",
       "\\item 4\n",
       "\\item 3\n",
       "\\item 5\n",
       "\\item 2\n",
       "\\item 5\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 3\n",
       "2. 4\n",
       "3. 3\n",
       "4. 5\n",
       "5. 2\n",
       "6. 5\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 3 4 3 5 2 5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>'A'</li><li>'B'</li><li>'A'</li><li>'C'</li><li>'\\n'</li><li>'C'</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'A'\n",
       "\\item 'B'\n",
       "\\item 'A'\n",
       "\\item 'C'\n",
       "\\item '\\textbackslash{}n'\n",
       "\\item 'C'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'A'\n",
       "2. 'B'\n",
       "3. 'A'\n",
       "4. 'C'\n",
       "5. '\\n'\n",
       "6. 'C'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"A\"  \"B\"  \"A\"  \"C\"  \"\\n\" \"C\" "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'ABAC\\nC'"
      ],
      "text/latex": [
       "'ABAC\\textbackslash{}nC'"
      ],
      "text/markdown": [
       "'ABAC\\nC'"
      ],
      "text/plain": [
       "[1] \"ABAC\\nC\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# texto de exemplo\n",
    "file0 <- \"ABA C\\nC\"\n",
    "file0 <- gsub(\" \", \"\", file0)   # (se quiser tirar espa√ßos)\n",
    "\n",
    "# encoder ‚Üí √≠ndices\n",
    "enc <- Encoder(file = file0, vocabulary = voc)\n",
    "enc\n",
    "# ex.: 3 4 3 5 2 5\n",
    "\n",
    "# decoder ‚Üí volta para s√≠mbolos\n",
    "dec <- Decoder(file = enc, vocabulary = voc)\n",
    "dec\n",
    "paste(dec, collapse = \"\")\n",
    "# deve reconstruir exatamente 'file0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d3e8b4",
   "metadata": {},
   "source": [
    "### Defini√ß√£o e Treino do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "3df20958",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "Model <- GPT(block_size = config$block_size,\n",
    "                n_embd = config$n_embd,\n",
    "                N_Layers = config$N_Layers,\n",
    "                nvoc = nvoc,\n",
    "                Head = config$Head)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c1d30265",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wpe.weight           16 x 128 ‚Üí 2048 par√¢metros\n",
      "wte.weight           5 x 128 ‚Üí 640 par√¢metros\n",
      "MM.0.out_proj.weight 128 x 128 ‚Üí 16384 par√¢metros\n",
      "MM.0.out_proj.bias   128 ‚Üí 128 par√¢metros\n",
      "MM.0.in_proj_weight  384 x 128 ‚Üí 49152 par√¢metros\n",
      "MM.0.in_proj_bias    384 ‚Üí 384 par√¢metros\n",
      "MM.1.out_proj.weight 128 x 128 ‚Üí 16384 par√¢metros\n",
      "MM.1.out_proj.bias   128 ‚Üí 128 par√¢metros\n",
      "MM.1.in_proj_weight  384 x 128 ‚Üí 49152 par√¢metros\n",
      "MM.1.in_proj_bias    384 ‚Üí 384 par√¢metros\n",
      "scale1.0.weight      128 ‚Üí 128 par√¢metros\n",
      "scale1.0.bias        128 ‚Üí 128 par√¢metros\n",
      "scale1.1.weight      128 ‚Üí 128 par√¢metros\n",
      "scale1.1.bias        128 ‚Üí 128 par√¢metros\n",
      "scale2.0.weight      128 ‚Üí 128 par√¢metros\n",
      "scale2.0.bias        128 ‚Üí 128 par√¢metros\n",
      "scale2.1.weight      128 ‚Üí 128 par√¢metros\n",
      "scale2.1.bias        128 ‚Üí 128 par√¢metros\n",
      "scale3.weight        128 ‚Üí 128 par√¢metros\n",
      "scale3.bias          128 ‚Üí 128 par√¢metros\n",
      "FFN.0.0.weight       512 x 128 ‚Üí 65536 par√¢metros\n",
      "FFN.0.0.bias         512 ‚Üí 512 par√¢metros\n",
      "FFN.0.2.weight       128 x 512 ‚Üí 65536 par√¢metros\n",
      "FFN.0.2.bias         128 ‚Üí 128 par√¢metros\n",
      "FFN.1.0.weight       512 x 128 ‚Üí 65536 par√¢metros\n",
      "FFN.1.0.bias         512 ‚Üí 512 par√¢metros\n",
      "FFN.1.2.weight       128 x 512 ‚Üí 65536 par√¢metros\n",
      "FFN.1.2.bias         128 ‚Üí 128 par√¢metros\n",
      "ln_f.weight          5 x 128 ‚Üí 640 par√¢metros\n",
      "Total de par√¢metros no modelo: 400128 \n"
     ]
    }
   ],
   "source": [
    "params <- Model$named_parameters()\n",
    "\n",
    "for (p in names(params)) {\n",
    "  cat(sprintf(\"%-20s %s ‚Üí %d par√¢metros\\n\",\n",
    "              p,\n",
    "              paste(dim(params[[p]]), collapse = \" x \"),\n",
    "              prod(dim(params[[p]]))))\n",
    "}\n",
    "\n",
    "total_params <- sum(sapply(params, function(x) prod(dim(x))))\n",
    "cat(\"Total de par√¢metros no modelo:\", total_params, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b57d7f1",
   "metadata": {},
   "source": [
    "## Sanity check\n",
    "\n",
    "Aplicacao da softmax no modelo n√£o treinado ainda.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c7f58727",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch_tensor\n",
       "1\n",
       "[ CPUFloatType{} ][ grad_fn = <SumBackward0> ]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch_tensor\n",
       " 1.0000  1.0000  1.0000  1.0000  1.0000  1.0000  1.0000  1.0000\n",
       "[ CPUFloatType{1,8} ][ grad_fn = <SumBackward1> ]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1) modelo\n",
    "Model <- GPT(\n",
    "  block_size = config$block_size,\n",
    "  n_embd     = config$n_embd,\n",
    "  N_Layers   = config$N_Layers,\n",
    "  nvoc       = nvoc,            # tamanho do vocabul√°rio que voc√™ calculou\n",
    "  Head       = config$Head,\n",
    "  p0         = config$p0\n",
    ")\n",
    "\n",
    "# 2) entrada v√°lida\n",
    "T <- 8\n",
    "x <- torch::torch_tensor(matrix(sample(1:nvoc, T, replace = TRUE), nrow = 1),\n",
    "                         dtype = torch::torch_long())\n",
    "\n",
    "# 3) forward -> logits (B, T, nvoc)\n",
    "logits <- Model(x)\n",
    "\n",
    "# 4) softmax no √∫ltimo eixo -> probas\n",
    "p <- torch::nnf_softmax(logits, dim = -1)\n",
    "\n",
    "# 5) verifique que cada distribui√ß√£o em (b, t, :) soma 1\n",
    "torch::torch_sum(p[1, 1, ])    # deve imprimir ~1\n",
    "torch::torch_sum(p, dim = -1)  # vetor (B, T) todo de 1's"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4460f534",
   "metadata": {},
   "source": [
    "### Defini√ß√£o do modelo de treino e do modelo de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "4e6efe35",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "file0 <- readChar(\"ABC.txt\", file.info(\"ABC.txt\")$size)\n",
    "voc <- c(\"<PAD>\", sort(unique(unlist(strsplit(file0, \"\")))))\n",
    "Encoded <- Encoder(file = file0, vocabulary = voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "4f6dab32",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "p_train = 0.8\n",
    "\n",
    "# Divis√£o dos dados\n",
    "n <- length(Encoded)\n",
    "\n",
    "BD.train <- torch_tensor(Encoded[1:round(p_train*n)],dtype=torch_int())\n",
    "BD.test <- torch_tensor(Encoded[round(p_train*n+1):n], dtype = torch_int())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5a6e07",
   "metadata": {},
   "source": [
    "### Treino do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "0742c13f",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in loss(FIT$view(c(-1, FIT$size(-1))), v$view(-1)): n√£o foi poss√≠vel encontrar a fun√ß√£o \"loss\"\n",
     "output_type": "error",
     "traceback": [
      "Error in loss(FIT$view(c(-1, FIT$size(-1))), v$view(-1)): n√£o foi poss√≠vel encontrar a fun√ß√£o \"loss\"\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "for(i in 1:config$epochs){\n",
    "\n",
    "    # 1) posi√ß√µes iniciais aleat√≥rias (garante que caiba um bloco + 1)\n",
    "    idx = sample(1:(round(p_treino*n) - config$batch_size), config$batch_size)\n",
    "\n",
    "    # 2) para cada idx, empilha a sequ√™ncia idx + 0..block_size\n",
    "    #    (forma: (batch_size) x (block_size+1)) e depois \"flatten\"\n",
    "    idx2 = as.integer(c(t(pmin(outer(as.integer(idx), 0:config$block_size, '+'), n))))\n",
    "\n",
    "    Z <- BD.train[idx2, drop = FALSE]$view(c(length(idx), config$block_size + 1))\n",
    "    X <- Z[,1:config$block_size]\n",
    "    Y <- Z[,2:(config$block_size+1)]\n",
    "    \n",
    "    Model$train()          # ativa modo de treino (dropout etc.)\n",
    "    FIT <- Model(X)        # forward pass\n",
    "    loss0 <- loss(FIT$view(c(-1, FIT$size(-1))), v$view(-1))\n",
    "\n",
    "    print(loss0$item())\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a7c6809b",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca:  1 Train loss:  1.75601923465729\n",
      "\n",
      "Epoca:  2 Train loss:  1.13897490501404\n",
      "\n",
      "Epoca:  3 Train loss:  1.05270624160767\n",
      "\n",
      "Epoca:  4 Train loss:  1.00460243225098\n",
      "\n",
      "Epoca:  5 Train loss:  0.943425834178925\n",
      "\n",
      "Epoca:  6 Train loss:  0.990464210510254\n",
      "\n",
      "Epoca:  7 Train loss:  0.965478360652924\n",
      "\n",
      "Epoca:  8 Train loss:  0.957518935203552\n",
      "\n",
      "Epoca:  9 Train loss:  0.955720484256744\n",
      "\n",
      "Epoca:  10 Train loss:  0.923735499382019\n",
      "\n",
      "Epoca:  11 Train loss:  0.990632891654968\n",
      "\n",
      "Epoca:  12 Train loss:  0.959781885147095\n",
      "\n",
      "Epoca:  13 Train loss:  0.935571908950806\n",
      "\n",
      "Epoca:  14 Train loss:  0.924338757991791\n",
      "\n",
      "Epoca:  15 Train loss:  0.937005639076233\n",
      "\n",
      "Epoca:  16 Train loss:  0.911421239376068\n",
      "\n",
      "Epoca:  17 Train loss:  0.84186840057373\n",
      "\n",
      "Epoca:  18 Train loss:  0.929946541786194\n",
      "\n",
      "Epoca:  19 Train loss:  0.895896375179291\n",
      "\n",
      "Epoca:  20 Train loss:  0.89698988199234\n",
      "\n",
      "Epoca:  21 Train loss:  0.84135639667511\n",
      "\n",
      "Epoca:  22 Train loss:  0.875084400177002\n",
      "\n",
      "Epoca:  23 Train loss:  0.848853588104248\n",
      "\n",
      "Epoca:  24 Train loss:  0.864954113960266\n",
      "\n",
      "Epoca:  25 Train loss:  0.863705337047577\n",
      "\n",
      "Epoca:  26 Train loss:  0.861273348331451\n",
      "\n",
      "Epoca:  27 Train loss:  0.835231781005859\n",
      "\n",
      "Epoca:  28 Train loss:  0.842216312885284\n",
      "\n",
      "Epoca:  29 Train loss:  0.754272401332855\n",
      "\n",
      "Epoca:  30 Train loss:  0.813172459602356\n",
      "\n",
      "Epoca:  31 Train loss:  0.783324897289276\n",
      "\n",
      "Epoca:  32 Train loss:  0.79122793674469\n",
      "\n",
      "Epoca:  33 Train loss:  0.751367032527924\n",
      "\n",
      "Epoca:  34 Train loss:  0.72253155708313\n",
      "\n",
      "Epoca:  35 Train loss:  0.694436848163605\n",
      "\n",
      "Epoca:  36 Train loss:  0.71682870388031\n",
      "\n",
      "Epoca:  37 Train loss:  0.659463226795197\n",
      "\n",
      "Epoca:  38 Train loss:  0.660697996616364\n",
      "\n",
      "Epoca:  39 Train loss:  0.671635270118713\n",
      "\n",
      "Epoca:  40 Train loss:  0.642580211162567\n",
      "\n",
      "Epoca:  41 Train loss:  0.640099465847015\n",
      "\n",
      "Epoca:  42 Train loss:  0.590740323066711\n",
      "\n",
      "Epoca:  43 Train loss:  0.632061064243317\n",
      "\n",
      "Epoca:  44 Train loss:  0.624534666538239\n",
      "\n",
      "Epoca:  45 Train loss:  0.634983062744141\n",
      "\n",
      "Epoca:  46 Train loss:  0.59262889623642\n",
      "\n",
      "Epoca:  47 Train loss:  0.616792559623718\n",
      "\n",
      "Epoca:  48 Train loss:  0.597490608692169\n",
      "\n",
      "Epoca:  49 Train loss:  0.621086776256561\n",
      "\n",
      "Epoca:  50 Train loss:  0.604881227016449\n",
      "\n",
      "Epoca:  51 Train loss:  0.53950572013855\n",
      "\n",
      "Epoca:  52 Train loss:  0.533551275730133\n",
      "\n",
      "Epoca:  53 Train loss:  0.561880350112915\n",
      "\n",
      "Epoca:  54 Train loss:  0.576523065567017\n",
      "\n",
      "Epoca:  55 Train loss:  0.56912350654602\n",
      "\n",
      "Epoca:  56 Train loss:  0.544935762882233\n",
      "\n",
      "Epoca:  57 Train loss:  0.541044116020203\n",
      "\n",
      "Epoca:  58 Train loss:  0.563920319080353\n",
      "\n",
      "Epoca:  59 Train loss:  0.553009569644928\n",
      "\n",
      "Epoca:  60 Train loss:  0.5399010181427\n",
      "\n",
      "Epoca:  61 Train loss:  0.525730967521667\n",
      "\n",
      "Epoca:  62 Train loss:  0.526814997196198\n",
      "\n",
      "Epoca:  63 Train loss:  0.518843472003937\n",
      "\n",
      "Epoca:  64 Train loss:  0.495281010866165\n",
      "\n",
      "Epoca:  65 Train loss:  0.531217277050018\n",
      "\n",
      "Epoca:  66 Train loss:  0.477956414222717\n",
      "\n",
      "Epoca:  67 Train loss:  0.517673552036285\n",
      "\n",
      "Epoca:  68 Train loss:  0.512510359287262\n",
      "\n",
      "Epoca:  69 Train loss:  0.476973086595535\n",
      "\n",
      "Epoca:  70 Train loss:  0.497386276721954\n",
      "\n",
      "Epoca:  71 Train loss:  0.521263539791107\n",
      "\n",
      "Epoca:  72 Train loss:  0.508018493652344\n",
      "\n",
      "Epoca:  73 Train loss:  0.485253900289536\n",
      "\n",
      "Epoca:  74 Train loss:  0.460225224494934\n",
      "\n",
      "Epoca:  75 Train loss:  0.506368041038513\n",
      "\n",
      "Epoca:  76 Train loss:  0.506300508975983\n",
      "\n",
      "Epoca:  77 Train loss:  0.455810606479645\n",
      "\n",
      "Epoca:  78 Train loss:  0.49233490228653\n",
      "\n",
      "Epoca:  79 Train loss:  0.466730535030365\n",
      "\n",
      "Epoca:  80 Train loss:  0.471251010894775\n",
      "\n",
      "Epoca:  81 Train loss:  0.46287602186203\n",
      "\n",
      "Epoca:  82 Train loss:  0.46523654460907\n",
      "\n",
      "Epoca:  83 Train loss:  0.456794857978821\n",
      "\n",
      "Epoca:  84 Train loss:  0.471310496330261\n",
      "\n",
      "Epoca:  85 Train loss:  0.525709211826324\n",
      "\n",
      "Epoca:  86 Train loss:  0.456950575113297\n",
      "\n",
      "Epoca:  87 Train loss:  0.472528219223022\n",
      "\n",
      "Epoca:  88 Train loss:  0.445233225822449\n",
      "\n",
      "Epoca:  89 Train loss:  0.439864188432693\n",
      "\n",
      "Epoca:  90 Train loss:  0.448958873748779\n",
      "\n",
      "Epoca:  91 Train loss:  0.473726570606232\n",
      "\n",
      "Epoca:  92 Train loss:  0.458167046308517\n",
      "\n",
      "Epoca:  93 Train loss:  0.454345047473907\n",
      "\n",
      "Epoca:  94 Train loss:  0.449942708015442\n",
      "\n",
      "Epoca:  95 Train loss:  0.452832788228989\n",
      "\n",
      "Epoca:  96 Train loss:  0.444949269294739\n",
      "\n",
      "Epoca:  97 Train loss:  0.466348677873611\n",
      "\n",
      "Epoca:  98 Train loss:  0.452715814113617\n",
      "\n",
      "Epoca:  99 Train loss:  0.433747887611389\n",
      "\n",
      "Epoca:  100 Train loss:  0.444264888763428\n",
      "\n",
      "Epoca:  101 Train loss:  0.420526295900345\n",
      "\n",
      "Epoca:  102 Train loss:  0.44675812125206\n",
      "\n",
      "Epoca:  103 Train loss:  0.417812526226044\n",
      "\n",
      "Epoca:  104 Train loss:  0.423294216394424\n",
      "\n",
      "Epoca:  105 Train loss:  0.415600210428238\n",
      "\n",
      "Epoca:  106 Train loss:  0.415230333805084\n",
      "\n",
      "Epoca:  107 Train loss:  0.419054478406906\n",
      "\n",
      "Epoca:  108 Train loss:  0.438443630933762\n",
      "\n",
      "Epoca:  109 Train loss:  0.435965955257416\n",
      "\n",
      "Epoca:  110 Train loss:  0.415096580982208\n",
      "\n",
      "Epoca:  111 Train loss:  0.420504748821259\n",
      "\n",
      "Epoca:  112 Train loss:  0.412221670150757\n",
      "\n",
      "Epoca:  113 Train loss:  0.41389799118042\n",
      "\n",
      "Epoca:  114 Train loss:  0.410435616970062\n",
      "\n",
      "Epoca:  115 Train loss:  0.411535263061523\n",
      "\n",
      "Epoca:  116 Train loss:  0.372157126665115\n",
      "\n",
      "Epoca:  117 Train loss:  0.399407118558884\n",
      "\n",
      "Epoca:  118 Train loss:  0.450490891933441\n",
      "\n",
      "Epoca:  119 Train loss:  0.404711276292801\n",
      "\n",
      "Epoca:  120 Train loss:  0.435905814170837\n",
      "\n",
      "Epoca:  121 Train loss:  0.404439866542816\n",
      "\n",
      "Epoca:  122 Train loss:  0.414030939340591\n",
      "\n",
      "Epoca:  123 Train loss:  0.420888274908066\n",
      "\n",
      "Epoca:  124 Train loss:  0.380000382661819\n",
      "\n",
      "Epoca:  125 Train loss:  0.40900307893753\n",
      "\n",
      "Epoca:  126 Train loss:  0.417952716350555\n",
      "\n",
      "Epoca:  127 Train loss:  0.367495507001877\n",
      "\n",
      "Epoca:  128 Train loss:  0.377870112657547\n",
      "\n",
      "Epoca:  129 Train loss:  0.402431160211563\n",
      "\n",
      "Epoca:  130 Train loss:  0.423342198133469\n",
      "\n",
      "Epoca:  131 Train loss:  0.402012825012207\n",
      "\n",
      "Epoca:  132 Train loss:  0.419027477502823\n",
      "\n",
      "Epoca:  133 Train loss:  0.397356986999512\n",
      "\n",
      "Epoca:  134 Train loss:  0.417548984289169\n",
      "\n",
      "Epoca:  135 Train loss:  0.386271059513092\n",
      "\n",
      "Epoca:  136 Train loss:  0.413530141115189\n",
      "\n",
      "Epoca:  137 Train loss:  0.420137584209442\n",
      "\n",
      "Epoca:  138 Train loss:  0.400627851486206\n",
      "\n",
      "Epoca:  139 Train loss:  0.390150815248489\n",
      "\n",
      "Epoca:  140 Train loss:  0.419396370649338\n",
      "\n",
      "Epoca:  141 Train loss:  0.399865984916687\n",
      "\n",
      "Epoca:  142 Train loss:  0.405980199575424\n",
      "\n",
      "Epoca:  143 Train loss:  0.400638937950134\n",
      "\n",
      "Epoca:  144 Train loss:  0.428580075502396\n",
      "\n",
      "Epoca:  145 Train loss:  0.41894805431366\n",
      "\n",
      "Epoca:  146 Train loss:  0.415449470281601\n",
      "\n",
      "Epoca:  147 Train loss:  0.436802953481674\n",
      "\n",
      "Epoca:  148 Train loss:  0.378604173660278\n",
      "\n",
      "Epoca:  149 Train loss:  0.399679332971573\n",
      "\n",
      "Epoca:  150 Train loss:  0.425458461046219\n",
      "\n",
      "Epoca:  151 Train loss:  0.385925233364105\n",
      "\n",
      "Epoca:  152 Train loss:  0.375865876674652\n",
      "\n",
      "Epoca:  153 Train loss:  0.421651273965836\n",
      "\n",
      "Epoca:  154 Train loss:  0.362531244754791\n",
      "\n",
      "Epoca:  155 Train loss:  0.413078457117081\n",
      "\n",
      "Epoca:  156 Train loss:  0.40222093462944\n",
      "\n",
      "Epoca:  157 Train loss:  0.374414801597595\n",
      "\n",
      "Epoca:  158 Train loss:  0.381319969892502\n",
      "\n",
      "Epoca:  159 Train loss:  0.400547593832016\n",
      "\n",
      "Epoca:  160 Train loss:  0.409481912851334\n",
      "\n",
      "Epoca:  161 Train loss:  0.413932859897614\n",
      "\n",
      "Epoca:  162 Train loss:  0.382659912109375\n",
      "\n",
      "Epoca:  163 Train loss:  0.388424009084702\n",
      "\n",
      "Epoca:  164 Train loss:  0.367132902145386\n",
      "\n",
      "Epoca:  165 Train loss:  0.396289348602295\n",
      "\n",
      "Epoca:  166 Train loss:  0.365304946899414\n",
      "\n",
      "Epoca:  167 Train loss:  0.406943917274475\n",
      "\n",
      "Epoca:  168 Train loss:  0.369742840528488\n",
      "\n",
      "Epoca:  169 Train loss:  0.389964640140533\n",
      "\n",
      "Epoca:  170 Train loss:  0.398050606250763\n",
      "\n",
      "Epoca:  171 Train loss:  0.368324011564255\n",
      "\n",
      "Epoca:  172 Train loss:  0.354559540748596\n",
      "\n",
      "Epoca:  173 Train loss:  0.372795343399048\n",
      "\n",
      "Epoca:  174 Train loss:  0.347192823886871\n",
      "\n",
      "Epoca:  175 Train loss:  0.382728666067123\n",
      "\n",
      "Epoca:  176 Train loss:  0.387674689292908\n",
      "\n",
      "Epoca:  177 Train loss:  0.359058916568756\n",
      "\n",
      "Epoca:  178 Train loss:  0.380126923322678\n",
      "\n",
      "Epoca:  179 Train loss:  0.369947731494904\n",
      "\n",
      "Epoca:  180 Train loss:  0.365938127040863\n",
      "\n",
      "Epoca:  181 Train loss:  0.367550492286682\n",
      "\n",
      "Epoca:  182 Train loss:  0.374796271324158\n",
      "\n",
      "Epoca:  183 Train loss:  0.369677037000656\n",
      "\n",
      "Epoca:  184 Train loss:  0.372683197259903\n",
      "\n",
      "Epoca:  185 Train loss:  0.353300422430038\n",
      "\n",
      "Epoca:  186 Train loss:  0.383351057767868\n",
      "\n",
      "Epoca:  187 Train loss:  0.374858349561691\n",
      "\n",
      "Epoca:  188 Train loss:  0.368562191724777\n",
      "\n",
      "Epoca:  189 Train loss:  0.373467743396759\n",
      "\n",
      "Epoca:  190 Train loss:  0.35198301076889\n",
      "\n",
      "Epoca:  191 Train loss:  0.365025192499161\n",
      "\n",
      "Epoca:  192 Train loss:  0.379774510860443\n",
      "\n",
      "Epoca:  193 Train loss:  0.383641093969345\n",
      "\n",
      "Epoca:  194 Train loss:  0.349701404571533\n",
      "\n",
      "Epoca:  195 Train loss:  0.349335581064224\n",
      "\n",
      "Epoca:  196 Train loss:  0.363563716411591\n",
      "\n",
      "Epoca:  197 Train loss:  0.368205517530441\n",
      "\n",
      "Epoca:  198 Train loss:  0.363098293542862\n",
      "\n",
      "Epoca:  199 Train loss:  0.374833941459656\n",
      "\n",
      "Epoca:  200 Train loss:  0.353153079748154\n",
      "\n",
      "Epoca:  201 Train loss:  0.355393797159195\n",
      "\n",
      "Epoca:  202 Train loss:  0.363220304250717\n",
      "\n",
      "Epoca:  203 Train loss:  0.373203277587891\n",
      "\n",
      "Epoca:  204 Train loss:  0.351295679807663\n",
      "\n",
      "Epoca:  205 Train loss:  0.363345921039581\n",
      "\n",
      "Epoca:  206 Train loss:  0.33591878414154\n",
      "\n",
      "Epoca:  207 Train loss:  0.338615357875824\n",
      "\n",
      "Epoca:  208 Train loss:  0.356175750494003\n",
      "\n",
      "Epoca:  209 Train loss:  0.3414346575737\n",
      "\n",
      "Epoca:  210 Train loss:  0.373482346534729\n",
      "\n",
      "Epoca:  211 Train loss:  0.348199725151062\n",
      "\n",
      "Epoca:  212 Train loss:  0.37289834022522\n",
      "\n",
      "Epoca:  213 Train loss:  0.345839828252792\n",
      "\n",
      "Epoca:  214 Train loss:  0.347237914800644\n",
      "\n",
      "Epoca:  215 Train loss:  0.395613223314285\n",
      "\n",
      "Epoca:  216 Train loss:  0.3687903881073\n",
      "\n",
      "Epoca:  217 Train loss:  0.338323801755905\n",
      "\n",
      "Epoca:  218 Train loss:  0.324870437383652\n",
      "\n",
      "Epoca:  219 Train loss:  0.347869545221329\n",
      "\n",
      "Epoca:  220 Train loss:  0.348468154668808\n",
      "\n",
      "Epoca:  221 Train loss:  0.349977433681488\n",
      "\n",
      "Epoca:  222 Train loss:  0.33104932308197\n",
      "\n",
      "Epoca:  223 Train loss:  0.356564253568649\n",
      "\n",
      "Epoca:  224 Train loss:  0.337739408016205\n",
      "\n",
      "Epoca:  225 Train loss:  0.341185063123703\n",
      "\n",
      "Epoca:  226 Train loss:  0.32926082611084\n",
      "\n",
      "Epoca:  227 Train loss:  0.346802294254303\n",
      "\n",
      "Epoca:  228 Train loss:  0.361213862895966\n",
      "\n",
      "Epoca:  229 Train loss:  0.359226137399673\n",
      "\n",
      "Epoca:  230 Train loss:  0.374958574771881\n",
      "\n",
      "Epoca:  231 Train loss:  0.325995802879333\n",
      "\n",
      "Epoca:  232 Train loss:  0.335649698972702\n",
      "\n",
      "Epoca:  233 Train loss:  0.356023252010345\n",
      "\n",
      "Epoca:  234 Train loss:  0.344483703374863\n",
      "\n",
      "Epoca:  235 Train loss:  0.351061940193176\n",
      "\n",
      "Epoca:  236 Train loss:  0.358863264322281\n",
      "\n",
      "Epoca:  237 Train loss:  0.361089676618576\n",
      "\n",
      "Epoca:  238 Train loss:  0.346363514661789\n",
      "\n",
      "Epoca:  239 Train loss:  0.344735413789749\n",
      "\n",
      "Epoca:  240 Train loss:  0.339703977108002\n",
      "\n",
      "Epoca:  241 Train loss:  0.372420102357864\n",
      "\n",
      "Epoca:  242 Train loss:  0.319923520088196\n",
      "\n",
      "Epoca:  243 Train loss:  0.346788913011551\n",
      "\n",
      "Epoca:  244 Train loss:  0.339623004198074\n",
      "\n",
      "Epoca:  245 Train loss:  0.381790161132812\n",
      "\n",
      "Epoca:  246 Train loss:  0.354514569044113\n",
      "\n",
      "Epoca:  247 Train loss:  0.353337973356247\n",
      "\n",
      "Epoca:  248 Train loss:  0.338051378726959\n",
      "\n",
      "Epoca:  249 Train loss:  0.341343998908997\n",
      "\n",
      "Epoca:  250 Train loss:  0.349976301193237\n",
      "\n",
      "Epoca:  251 Train loss:  0.34607407450676\n",
      "\n",
      "Epoca:  252 Train loss:  0.347647249698639\n",
      "\n",
      "Epoca:  253 Train loss:  0.324277967214584\n",
      "\n",
      "Epoca:  254 Train loss:  0.346957087516785\n",
      "\n",
      "Epoca:  255 Train loss:  0.33050861954689\n",
      "\n",
      "Epoca:  256 Train loss:  0.341950982809067\n",
      "\n",
      "Epoca:  257 Train loss:  0.319763571023941\n",
      "\n",
      "Epoca:  258 Train loss:  0.315853714942932\n",
      "\n",
      "Epoca:  259 Train loss:  0.337923616170883\n",
      "\n",
      "Epoca:  260 Train loss:  0.341256499290466\n",
      "\n",
      "Epoca:  261 Train loss:  0.336922198534012\n",
      "\n",
      "Epoca:  262 Train loss:  0.317852705717087\n",
      "\n",
      "Epoca:  263 Train loss:  0.34280726313591\n",
      "\n",
      "Epoca:  264 Train loss:  0.320031046867371\n",
      "\n",
      "Epoca:  265 Train loss:  0.333761841058731\n",
      "\n",
      "Epoca:  266 Train loss:  0.336011946201324\n",
      "\n",
      "Epoca:  267 Train loss:  0.337044179439545\n",
      "\n",
      "Epoca:  268 Train loss:  0.308651626110077\n",
      "\n",
      "Epoca:  269 Train loss:  0.331892728805542\n",
      "\n",
      "Epoca:  270 Train loss:  0.328435987234116\n",
      "\n",
      "Epoca:  271 Train loss:  0.324729710817337\n",
      "\n",
      "Epoca:  272 Train loss:  0.332769960165024\n",
      "\n",
      "Epoca:  273 Train loss:  0.354268848896027\n",
      "\n",
      "Epoca:  274 Train loss:  0.339876353740692\n",
      "\n",
      "Epoca:  275 Train loss:  0.341013044118881\n",
      "\n",
      "Epoca:  276 Train loss:  0.348126202821732\n",
      "\n",
      "Epoca:  277 Train loss:  0.336190700531006\n",
      "\n",
      "Epoca:  278 Train loss:  0.337571501731873\n",
      "\n",
      "Epoca:  279 Train loss:  0.34178814291954\n",
      "\n",
      "Epoca:  280 Train loss:  0.346626490354538\n",
      "\n",
      "Epoca:  281 Train loss:  0.341682881116867\n",
      "\n",
      "Epoca:  282 Train loss:  0.329385489225388\n",
      "\n",
      "Epoca:  283 Train loss:  0.315562427043915\n",
      "\n",
      "Epoca:  284 Train loss:  0.3252332508564\n",
      "\n",
      "Epoca:  285 Train loss:  0.319361686706543\n",
      "\n",
      "Epoca:  286 Train loss:  0.331965535879135\n",
      "\n",
      "Epoca:  287 Train loss:  0.335721284151077\n",
      "\n",
      "Epoca:  288 Train loss:  0.334084630012512\n",
      "\n",
      "Epoca:  289 Train loss:  0.344473630189896\n",
      "\n",
      "Epoca:  290 Train loss:  0.332302659749985\n",
      "\n",
      "Epoca:  291 Train loss:  0.327233493328094\n",
      "\n",
      "Epoca:  292 Train loss:  0.31773579120636\n",
      "\n",
      "Epoca:  293 Train loss:  0.319552838802338\n",
      "\n",
      "Epoca:  294 Train loss:  0.33087831735611\n",
      "\n",
      "Epoca:  295 Train loss:  0.337982177734375\n",
      "\n",
      "Epoca:  296 Train loss:  0.327375024557114\n",
      "\n",
      "Epoca:  297 Train loss:  0.314573258161545\n",
      "\n",
      "Epoca:  298 Train loss:  0.345324665307999\n",
      "\n",
      "Epoca:  299 Train loss:  0.333542466163635\n",
      "\n",
      "Epoca:  300 Train loss:  0.34693455696106\n",
      "\n",
      "Epoca:  301 Train loss:  0.328990697860718\n",
      "\n",
      "Epoca:  302 Train loss:  0.323930442333221\n",
      "\n",
      "Epoca:  303 Train loss:  0.327834188938141\n",
      "\n",
      "Epoca:  304 Train loss:  0.324014991521835\n",
      "\n",
      "Epoca:  305 Train loss:  0.323757857084274\n",
      "\n",
      "Epoca:  306 Train loss:  0.317250788211823\n",
      "\n",
      "Epoca:  307 Train loss:  0.350265741348267\n",
      "\n",
      "Epoca:  308 Train loss:  0.329882770776749\n",
      "\n",
      "Epoca:  309 Train loss:  0.348350405693054\n",
      "\n",
      "Epoca:  310 Train loss:  0.316954106092453\n",
      "\n",
      "Epoca:  311 Train loss:  0.345513373613358\n",
      "\n",
      "Epoca:  312 Train loss:  0.319360047578812\n",
      "\n",
      "Epoca:  313 Train loss:  0.331571608781815\n",
      "\n",
      "Epoca:  314 Train loss:  0.328323870897293\n",
      "\n",
      "Epoca:  315 Train loss:  0.333205133676529\n",
      "\n",
      "Epoca:  316 Train loss:  0.30581921339035\n",
      "\n",
      "Epoca:  317 Train loss:  0.328664243221283\n",
      "\n",
      "Epoca:  318 Train loss:  0.357829004526138\n",
      "\n",
      "Epoca:  319 Train loss:  0.339542329311371\n",
      "\n",
      "Epoca:  320 Train loss:  0.336038589477539\n",
      "\n",
      "Epoca:  321 Train loss:  0.310903012752533\n",
      "\n",
      "Epoca:  322 Train loss:  0.317424535751343\n",
      "\n",
      "Epoca:  323 Train loss:  0.34834548830986\n",
      "\n",
      "Epoca:  324 Train loss:  0.315460354089737\n",
      "\n",
      "Epoca:  325 Train loss:  0.319383174180984\n",
      "\n",
      "Epoca:  326 Train loss:  0.329752147197723\n",
      "\n",
      "Epoca:  327 Train loss:  0.3424391746521\n",
      "\n",
      "Epoca:  328 Train loss:  0.337474673986435\n",
      "\n",
      "Epoca:  329 Train loss:  0.305671602487564\n",
      "\n",
      "Epoca:  330 Train loss:  0.335165411233902\n",
      "\n",
      "Epoca:  331 Train loss:  0.334805876016617\n",
      "\n",
      "Epoca:  332 Train loss:  0.31762757897377\n",
      "\n",
      "Epoca:  333 Train loss:  0.338794887065887\n",
      "\n",
      "Epoca:  334 Train loss:  0.32246807217598\n",
      "\n",
      "Epoca:  335 Train loss:  0.340541690587997\n",
      "\n",
      "Epoca:  336 Train loss:  0.321712017059326\n",
      "\n",
      "Epoca:  337 Train loss:  0.327588260173798\n",
      "\n",
      "Epoca:  338 Train loss:  0.313551843166351\n",
      "\n",
      "Epoca:  339 Train loss:  0.327567875385284\n",
      "\n",
      "Epoca:  340 Train loss:  0.309306204319\n",
      "\n",
      "Epoca:  341 Train loss:  0.327450543642044\n",
      "\n",
      "Epoca:  342 Train loss:  0.314866662025452\n",
      "\n",
      "Epoca:  343 Train loss:  0.328332275152206\n",
      "\n",
      "Epoca:  344 Train loss:  0.306197613477707\n",
      "\n",
      "Epoca:  345 Train loss:  0.328742861747742\n",
      "\n",
      "Epoca:  346 Train loss:  0.331031382083893\n",
      "\n",
      "Epoca:  347 Train loss:  0.315851837396622\n",
      "\n",
      "Epoca:  348 Train loss:  0.346313774585724\n",
      "\n",
      "Epoca:  349 Train loss:  0.351530015468597\n",
      "\n",
      "Epoca:  350 Train loss:  0.327175736427307\n",
      "\n",
      "Epoca:  351 Train loss:  0.308773070573807\n",
      "\n",
      "Epoca:  352 Train loss:  0.314670205116272\n",
      "\n",
      "Epoca:  353 Train loss:  0.309602349996567\n",
      "\n",
      "Epoca:  354 Train loss:  0.314376324415207\n",
      "\n",
      "Epoca:  355 Train loss:  0.299741506576538\n",
      "\n",
      "Epoca:  356 Train loss:  0.324233680963516\n",
      "\n",
      "Epoca:  357 Train loss:  0.334819108247757\n",
      "\n",
      "Epoca:  358 Train loss:  0.324143201112747\n",
      "\n",
      "Epoca:  359 Train loss:  0.344594329595566\n",
      "\n",
      "Epoca:  360 Train loss:  0.310019910335541\n",
      "\n",
      "Epoca:  361 Train loss:  0.301071166992188\n",
      "\n",
      "Epoca:  362 Train loss:  0.316758692264557\n",
      "\n",
      "Epoca:  363 Train loss:  0.330282062292099\n",
      "\n",
      "Epoca:  364 Train loss:  0.323590636253357\n",
      "\n",
      "Epoca:  365 Train loss:  0.330362051725388\n",
      "\n",
      "Epoca:  366 Train loss:  0.323147088289261\n",
      "\n",
      "Epoca:  367 Train loss:  0.326429575681686\n",
      "\n",
      "Epoca:  368 Train loss:  0.32908570766449\n",
      "\n",
      "Epoca:  369 Train loss:  0.325228303670883\n",
      "\n",
      "Epoca:  370 Train loss:  0.32758554816246\n",
      "\n",
      "Epoca:  371 Train loss:  0.316557228565216\n",
      "\n",
      "Epoca:  372 Train loss:  0.321118801832199\n",
      "\n",
      "Epoca:  373 Train loss:  0.34020671248436\n",
      "\n",
      "Epoca:  374 Train loss:  0.328921645879745\n",
      "\n",
      "Epoca:  375 Train loss:  0.332449853420258\n",
      "\n",
      "Epoca:  376 Train loss:  0.317328155040741\n",
      "\n",
      "Epoca:  377 Train loss:  0.32299742102623\n",
      "\n",
      "Epoca:  378 Train loss:  0.326166093349457\n",
      "\n",
      "Epoca:  379 Train loss:  0.331281185150146\n",
      "\n",
      "Epoca:  380 Train loss:  0.3164481818676\n",
      "\n",
      "Epoca:  381 Train loss:  0.325862109661102\n",
      "\n",
      "Epoca:  382 Train loss:  0.316586434841156\n",
      "\n",
      "Epoca:  383 Train loss:  0.316126644611359\n",
      "\n",
      "Epoca:  384 Train loss:  0.337378889322281\n",
      "\n",
      "Epoca:  385 Train loss:  0.32315132021904\n",
      "\n",
      "Epoca:  386 Train loss:  0.332510441541672\n",
      "\n",
      "Epoca:  387 Train loss:  0.315681457519531\n",
      "\n",
      "Epoca:  388 Train loss:  0.311402350664139\n",
      "\n",
      "Epoca:  389 Train loss:  0.331371784210205\n",
      "\n",
      "Epoca:  390 Train loss:  0.31575146317482\n",
      "\n",
      "Epoca:  391 Train loss:  0.315823346376419\n",
      "\n",
      "Epoca:  392 Train loss:  0.334201633930206\n",
      "\n",
      "Epoca:  393 Train loss:  0.328131437301636\n",
      "\n",
      "Epoca:  394 Train loss:  0.314611166715622\n",
      "\n",
      "Epoca:  395 Train loss:  0.301663309335709\n",
      "\n",
      "Epoca:  396 Train loss:  0.331476479768753\n",
      "\n",
      "Epoca:  397 Train loss:  0.296776115894318\n",
      "\n",
      "Epoca:  398 Train loss:  0.320679366588593\n",
      "\n",
      "Epoca:  399 Train loss:  0.3291175365448\n",
      "\n",
      "Epoca:  400 Train loss:  0.307815611362457\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer <- torch::optim_adamw(Model$parameters, lr= config$lr)\n",
    "loss0 <- torch::nn_cross_entropy_loss()\n",
    "loss_store <- numeric(config$epochs)\n",
    "\n",
    "for (i in 1:config$epochs) {\n",
    "\n",
    "  # 1) posi√ß√µes iniciais aleat√≥rias (garante que caiba um bloco+1)\n",
    "  idx  <- sample(1:(round(p_treino * n) - config$batch_size), config$batch_size)\n",
    "\n",
    "  # 2) para cada idx, empilha a sequ√™ncia idx + 0..block_size  (flatten depois)\n",
    "  idx2 <- as.integer(c(t(pmin(outer(as.integer(idx), 0:config$block_size, `+`), n))))\n",
    "\n",
    "  # 3) monta os pares (X, Y) com janela deslizante de tamanho block_size\n",
    "  Z <- BD.train[idx2, drop = FALSE]$view(c(length(idx), config$block_size + 1))\n",
    "  X <- Z[, 1:config$block_size]\n",
    "  Y <- Z[, 2:(config$block_size + 1)]\n",
    "\n",
    "  # 4) forward + perda\n",
    "  FIT  <- Model$train()(X)   # chama o forward\n",
    "  loss = loss0(FIT$flatten(end_dim=2), Y$flatten())\n",
    "  optimizer$zero_grad()\n",
    "  loss$backward()\n",
    "  optimizer$step()\n",
    "  loss_store[i] = loss$item()\n",
    "  cli::cli_progress_message(paste(\"Epoca: \", i, \"Train loss: \", loss_store[i]))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cf4dcb",
   "metadata": {},
   "source": [
    "# Prevendo o pr√≥ximo token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "29166d41",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch_tensor\n",
       "(1,.,.) = \n",
       "  0.0000  0.0010  0.3372  0.5416  0.1202\n",
       "[ CPUFloatType{1,1,5} ][ grad_fn = <SoftmaxBackward0> ]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"A\"\n",
    "x = Encoder(prompt)\n",
    "x = torch_tensor(x, dtype=torch_int())$unsqueeze(1)\n",
    "nnf_softmax(Model$eval()(x), -1)\n",
    "\n",
    "#Proximo token mais prov√°vel = B P(B)=0.4691"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47dd031",
   "metadata": {},
   "source": [
    "### N√£o precisa nem aplicar a softmax, basta pegar a saida maxima da funcao logitus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "28ae8ebc",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch_tensor\n",
      " 4\n",
      "[ CPULongType{1,1} ]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"A\"\n",
    "x = Encoder(prompt)\n",
    "x = torch_tensor(x, dtype=torch_int())$unsqueeze(1)\n",
    "next_token = torch_argmax(Model$eval()(x), -1)\n",
    "print(next_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "aead83ac",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"token 2, dado token 1\"\n",
      "torch_tensor\n",
      " 4\n",
      "[ CPULongType{1,1} ]\n",
      "[1] \"token 3, dado token 2\"\n",
      "torch_tensor\n",
      " 5\n",
      "[ CPULongType{1} ]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"A\"\n",
    "x = Encoder(prompt)\n",
    "x = torch_tensor(x, dtype=torch_int())$unsqueeze(1)\n",
    "next_token = torch_argmax(Model$eval()(x), -1)\n",
    "print(\"token 2, dado token 1\")\n",
    "print(next_token)\n",
    "x <- torch_cat(list(x, next_token), -1)\n",
    "next_token = torch_argmax(Model$eval()(x)[,-1], -1)\n",
    "print(\"token 3, dado token 2\")\n",
    "print(next_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "e6e35bb7",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 5 √ó 2</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>indice</th><th scope=col>token</th></tr>\n",
       "\t<tr><th scope=col>&lt;int&gt;</th><th scope=col>&lt;chr&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>1</td><td>&lt;PAD&gt;</td></tr>\n",
       "\t<tr><td>2</td><td>\n",
       "   </td></tr>\n",
       "\t<tr><td>3</td><td>A    </td></tr>\n",
       "\t<tr><td>4</td><td>B    </td></tr>\n",
       "\t<tr><td>5</td><td>C    </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 5 √ó 2\n",
       "\\begin{tabular}{ll}\n",
       " indice & token\\\\\n",
       " <int> & <chr>\\\\\n",
       "\\hline\n",
       "\t 1 & <PAD>\\\\\n",
       "\t 2 & \n",
       "   \\\\\n",
       "\t 3 & A    \\\\\n",
       "\t 4 & B    \\\\\n",
       "\t 5 & C    \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 5 √ó 2\n",
       "\n",
       "| indice &lt;int&gt; | token &lt;chr&gt; |\n",
       "|---|---|\n",
       "| 1 | &lt;PAD&gt; |\n",
       "| 2 | <!----> |\n",
       "| 3 | A     |\n",
       "| 4 | B     |\n",
       "| 5 | C     |\n",
       "\n"
      ],
      "text/plain": [
       "  indice token\n",
       "1 1      <PAD>\n",
       "2 2      \\n   \n",
       "3 3      A    \n",
       "4 4      B    \n",
       "5 5      C    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.frame(\n",
    "  indice = seq_along(voc),\n",
    "  token = voc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "1fc4ab0f",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"token 2, dado token 1\"\n",
      "torch_tensor\n",
      " 4\n",
      "[ CPULongType{1,1} ]\n",
      "[1] \"token 3, dado token 2\"\n",
      "torch_tensor\n",
      " 5\n",
      "[ CPULongType{1} ]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"A\"\n",
    "x = Encoder(prompt)\n",
    "x = torch_tensor(x, dtype=torch_int())$unsqueeze(1)\n",
    "next_token = torch_argmax(Model$eval()(x), -1)\n",
    "print(\"token 2, dado token 1\")\n",
    "print(next_token)\n",
    "x <- torch_cat(list(x, next_token), -1)\n",
    "next_token = torch_argmax(Model$eval()(x)[,-1], -1)\n",
    "print(\"token 3, dado token 2\")\n",
    "print(next_token)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "b0e5b1a3",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch_tensor\n",
      " 3  4  5\n",
      "[ CPULongType{1,3} ]\n"
     ]
    }
   ],
   "source": [
    "prompt <- \"A\"\n",
    "x <- Encoder(prompt)\n",
    "x <- torch_tensor(x, dtype = torch_int())$unsqueeze(1)  # [1, T]\n",
    "\n",
    "# gerar mais 2 passos (exemplo)\n",
    "Model$eval()\n",
    "# passo 1\n",
    "logits <- Model(x)[, -1, ]                 # [1, V] (√∫ltima posi√ß√£o de tempo)\n",
    "next_token <- torch_argmax(logits, dim = -1)$unsqueeze(2)  # [1, 1]\n",
    "x <- torch_cat(list(x, next_token), dim = 2)               # [1, T+1]\n",
    "\n",
    "\n",
    "# passo 2\n",
    "logits <- Model(x)[, -1, ]                 # [1, V]\n",
    "next_token <- torch_argmax(logits, dim = -1)$unsqueeze(2)  # [1, 1]\n",
    "x <- torch_cat(list(x, next_token), dim = 2)               # [1, T+2]\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201104e0",
   "metadata": {},
   "source": [
    "## Gerando um texto, considerando a janela de contexto para previ√£o do pr√≥ximo token\n",
    "\n",
    "M√©tod Greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "b8d18483",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABCBBCBCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABCABCCABC \n"
     ]
    }
   ],
   "source": [
    "# gera texto a partir de um prompt\n",
    "generate <- function(prompt, max_new = config$max_new_tokens) {\n",
    "  Model$eval()  # modo avalia√ß√£o (dropout desligado)\n",
    "  with_no_grad({\n",
    "    # encode ‚Üí tensor [B=1, T]\n",
    "    x <- torch_tensor(Encoder(prompt), dtype = torch_int())$unsqueeze(1)\n",
    "\n",
    "    for (i in 1:max_new) {\n",
    "      # 1) recorte de contexto (janela causal)\n",
    "      if (x$size(2) <= config$block_size) {\n",
    "        ctx <- x\n",
    "      } else {\n",
    "        T <- x$size(2)\n",
    "        ctx <- x[, (T - config$block_size + 1):T]\n",
    "      }\n",
    "\n",
    "      # 2) forward e pega o √∫ltimo passo temporal\n",
    "      logits <- Model(ctx)            # [B, T_ctx, V]\n",
    "      last_logits <- logits[, -1, ]   # [B, V]\n",
    "\n",
    "      # 3) escolhe pr√≥ximo token e concatena no dim=2\n",
    "      next_token <- torch_argmax(last_logits, dim = -1)$unsqueeze(2)  # [B,1]\n",
    "      x <- torch_cat(list(x, next_token), dim = 2)                     # [B,T+1]\n",
    "    }\n",
    "\n",
    "    # decode\n",
    "    generated_idx <- as.integer(as_array(x$squeeze(1)))\n",
    "    paste(voc[generated_idx], collapse = \"\")\n",
    "  })\n",
    "}\n",
    "\n",
    "# exemplo\n",
    "cat(generate(\"A\"), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8596cb85",
   "metadata": {},
   "source": [
    "## Gerando texto com m√©todo topK(2)\n",
    "Corre√ß√£o: colocar uma entropia na escolha do proximo token, entre os dois mais prov√°veis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "333e8046",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BBCBCABCCABCABCCAAABCBBCBCABCCAAABCBBCBCABCCAAABCBBCBCABCCABCABCCAAABCBBCBCABCCAAABCBBCBCABCCAAABCBBCAABCBBCAABCBBCBCABCCAAABCBBCAABCBBCAABCBBCAABCBBCBCABCCAAABCBBCAABCBBCAABCBBCBCABCCABCABCCAAABCBBCBCABCCABCABCCABCABCCAAABCBBCBCABCCAAABCBBCBCABCCAAABCBBCBCABCCABCABCCABCABCCAAABCBBCBCABCCABCABCCAAABCBBCAABCBBCBCABCCAAABCBBCBCABCCABCABCCABCABCCAAABCBBCBCABCCABCABCCABCABCCABCABCCAAABCBBCBCABCCABCABCCAAABCBBCBCABCCABCABCCABCABCCAAABCBBCBCABCCAAABCBBCBCABCCAAABCBBCAABCBBCBCABCCAAABCBBCBCABCCAAABCBBCAABCBBCAABCBBCAABCBBCBCABCCABCABCCAAABCBBCAABCBBCAABCBBCBCABCCABCABCCAAABCBBCBCABCCAAABCBBCBCABCCAAABCBBCBCABCCAAABCBBCBCABCCABCABCCAAABCBBCBCABCCABCABCCAAABCBBCBCABCCABCABCCAAABCBBCBCABCCAAABCBBCBCAB"
     ]
    }
   ],
   "source": [
    "for (i in 1:config$max_new_tokens) {\n",
    "\n",
    "  if (x$size(2) <= config$block_size) {\n",
    "    logits   <- Model$eval()(x)[, -1, ]\n",
    "    logits   <- logits$topk(k_top)                       # [[1]] valores, [[2]] √≠ndices\n",
    "    vals     <- logits[[1]]$to(dtype = torch_float())    # <<< garante float\n",
    "    probs    <- torch::nnf_softmax(vals, dim = -1)       # <<< funcional (corre√ß√£o)\n",
    "    selected <- torch_multinomial(probs, num_samples = 1)\n",
    "    next_token <- logits[[2]][, selected$item()]$unsqueeze(1)\n",
    "    x <- torch_cat(list(x, next_token), -1)\n",
    "  } else {\n",
    "    xx       <- x[, (x$size(2) - config$block_size + 1):x$size(2)]\n",
    "    logits   <- Model$eval()(xx)[, -1, ]\n",
    "    logits   <- logits$topk(k_top)\n",
    "    vals     <- logits[[1]]$to(dtype = torch_float())    # <<< idem\n",
    "    probs    <- torch::nnf_softmax(vals, dim = -1)       # <<< funcional\n",
    "    selected <- torch_multinomial(probs, num_samples = 1)\n",
    "    next_token <- logits[[2]][, selected$item()]$unsqueeze(1)\n",
    "    x <- torch_cat(list(x, next_token), -1)\n",
    "  }\n",
    "cat(Decoder(as.numeric(next_token)))\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1775e0",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
