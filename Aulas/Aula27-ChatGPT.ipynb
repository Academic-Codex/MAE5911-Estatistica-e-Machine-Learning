{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8186c3f0",
   "metadata": {},
   "source": [
    "## Direitos Autorais Modelo ChatGPT em R:\n",
    "Modelo apresentado na aula 27 do curso de estat√≠stica e Machine Learning do IME-USP.\n",
    "\n",
    "Autoria: Prof. Alexandre Galv√£o Patriota\n",
    "\n",
    "## üîπ Par√¢metros de Entrada do Modelo GPT\n",
    "\n",
    "A fun√ß√£o de inicializa√ß√£o do modelo recebe os seguintes argumentos:\n",
    "\n",
    "| **Par√¢metro** | **Significado** | **Interpreta√ß√£o** |\n",
    "|----------------|-----------------|-------------------------------|\n",
    "| **block_size** | Tamanho da janela de contexto (n√∫mero de tokens consecutivos considerados no bloco attention). | Define o comprimento da janela de condicionamento ‚Äî quantos tokens anteriores o modelo utiliza para prever o pr√≥ximo. |\n",
    "| **n_embd** | Dimens√£o do vetor de embedding. | Tamanho do espa√ßo latente cont√≠nuo onde cada token √© representado por um vetor real. |\n",
    "| **N_Layers** | N√∫mero de camadas do Transformer (profundidade da rede). | Quantas vezes o bloco ‚ÄúAten√ß√£o + Feed-Forward + Normaliza√ß√£o‚Äù √© repetido ao longo da rede. |\n",
    "| **nvoc** | Tamanho do vocabul√°rio. | N√∫mero de categorias poss√≠veis no modelo multinomial. Cada predi√ß√£o escolhe uma entre `nvoc` op√ß√µes (No m√©todo Greedy). |\n",
    "| **head** | N√∫mero de *heads* de aten√ß√£o. | Quantas proje√ß√µes paralelas de aten√ß√£o s√£o calculadas ‚Äî cada *head* modela um tipo distinto de depend√™ncia contextual. |\n",
    "| **p0** | Taxa de *dropout* (padr√£o 0.1). | Probabilidade de zerar aleatoriamente algumas ativa√ß√µes durante o treino, reduzindo o *overfitting*. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7737797e",
   "metadata": {},
   "source": [
    "## Par√¢metros de configura√ß√£o "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24aa1f9d",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "config <- list(\n",
    "  #Corpus for training (global)\n",
    "  file_name = \"ABC.txt\",\n",
    "  train = !TRUE,\n",
    "  run = TRUE,\n",
    "  read_weights = !TRUE,\n",
    "\n",
    "  #gpt parameters (global)\n",
    "  block_size = 16,   #Maximum context\n",
    "  n_embd = 128,      #Embedding dimension\n",
    "  N_Layers = 2,      #Number of layers\n",
    "  Head = 2,          #Number of heads\n",
    "\n",
    "  #Training parameters (global)\n",
    "  lr = 0.003,        #Learning rate\n",
    "  batch_size = 64,   #Batch size\n",
    "  p0 = 0.2,          #Dropout proportion\n",
    "  epochs = 4,        #Number of epochs\n",
    "  num_workers = 6  #Number of cpu workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aefbfa7c",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(torch)\n",
    "\n",
    "GPT <- torch::nn_module(\n",
    "  initialize = function(block_size, n_embd, N_Layers, nvoc, Head, p0 = 0.1) {\n",
    "\n",
    "    self$N   <- N_Layers\n",
    "    self$wpe <- torch::nn_embedding(block_size, n_embd)\n",
    "    self$wte <- torch::nn_embedding(nvoc, n_embd, padding_idx = 1)\n",
    "\n",
    "    self$MM  <- torch::nn_module_list(lapply(\n",
    "      1:N_Layers,\n",
    "      function(x) torch::nn_multihead_attention(n_embd, Head, dropout = p0, batch_first = TRUE)\n",
    "    ))\n",
    "\n",
    "    self$scale1 <- torch::nn_module_list(lapply(\n",
    "      1:N_Layers,\n",
    "      function(x) torch::nn_layer_norm(n_embd)\n",
    "    ))\n",
    "\n",
    "    self$scale2 <- torch::nn_module_list(lapply(\n",
    "      1:N_Layers,\n",
    "      function(x) torch::nn_layer_norm(n_embd)\n",
    "    ))\n",
    "\n",
    "    self$scale3 <- torch::nn_layer_norm(n_embd, elementwise_affine = TRUE)\n",
    "\n",
    "    self$FFN <- torch::nn_module_list(lapply(\n",
    "      1:N_Layers,\n",
    "      function(x) {\n",
    "        torch::nn_sequential(\n",
    "          torch::nn_linear(n_embd, 4 * n_embd),\n",
    "          torch::nn_gelu(),\n",
    "          torch::nn_linear(4 * n_embd, n_embd),\n",
    "          torch::nn_dropout(p0)\n",
    "        )\n",
    "      }\n",
    "    ))\n",
    "\n",
    "    # cabe√ßa linear de sa√≠da (mantive seu nome ln_f)\n",
    "    self$ln_f  <- torch::nn_linear(n_embd, nvoc, bias = FALSE)\n",
    "    self$drop0 <- torch::nn_dropout(p = p0)\n",
    "  },\n",
    "\n",
    "  forward = function(x, return_intermediates = FALSE) {\n",
    "    # x: (B, T)\n",
    "    B <- x$size(1)\n",
    "    T <- x$size(2)\n",
    "\n",
    "    # posi√ß√µes 1..T (long)\n",
    "    x1 <- torch::torch_arange(1, T,\n",
    "      dtype = torch::torch_long(),\n",
    "      device = x$device\n",
    "    )\n",
    "\n",
    "    # m√°scara causal (pro√≠be olhar para o futuro)\n",
    "    wei <- torch::torch_triu(torch::torch_ones(T, T, device = x$device), diagonal = 1)$to(\n",
    "      dtype = torch::torch_bool()\n",
    "    )\n",
    "\n",
    "    # embeddings\n",
    "    output <- self$wte(x) + self$wpe(x1)$unsqueeze(1)  # (B, T, E)\n",
    "    output <- self$drop0(output)\n",
    "\n",
    "    # (opcional) inspe√ß√£o r√°pida\n",
    "    # cat(\"wei shape:\", as.character(wei$size()), \"\\n\"); print(wei$to(dtype = torch_int()))\n",
    "    # cat(\"x1 shape:\", as.character(x1$size()), \"\\n\"); print(x1)\n",
    "\n",
    "    for (j in 1:self$N) {\n",
    "      # pr√©-norm + aten√ß√£o multihead\n",
    "      QKV <- self$scale1[[j]](output)  # (B, T, E) pois batch_first = TRUE\n",
    "      attn_out <- self$MM[[j]](\n",
    "        query = QKV, key = QKV, value = QKV,\n",
    "        attn_mask = wei, need_weights = FALSE\n",
    "      )[[1]]\n",
    "      output <- output + attn_out\n",
    "\n",
    "      # feed-forward com pr√©-norm\n",
    "      output <- output + self$FFN[[j]](self$scale2[[j]](output))\n",
    "    }\n",
    "\n",
    "    # norm final + cabe√ßa linear ‚Üí logits (B, T, nvoc)\n",
    "    output <- self$scale3(output)\n",
    "    logits <- self$ln_f(output)\n",
    "\n",
    "    if (return_intermediates) {\n",
    "      return(list(\n",
    "        x1     = x1$cpu(),\n",
    "        wei    = wei$to(dtype = torch_int())$cpu(),\n",
    "        out    = output$cpu(),\n",
    "        logits = logits$cpu()\n",
    "      ))\n",
    "    }\n",
    "    logits\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f1cf12",
   "metadata": {},
   "source": [
    "## Visualizando todas as estruturas que compoem o modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb02d40d",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch_tensor\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5\n",
       " 6\n",
       " 7\n",
       " 8\n",
       "[ CPULongType{8} ]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch_tensor\n",
       " 0  1  1  1  1  1  1  1\n",
       " 0  0  1  1  1  1  1  1\n",
       " 0  0  0  1  1  1  1  1\n",
       " 0  0  0  0  1  1  1  1\n",
       " 0  0  0  0  0  1  1  1\n",
       " 0  0  0  0  0  0  1  1\n",
       " 0  0  0  0  0  0  0  1\n",
       " 0  0  0  0  0  0  0  0\n",
       "[ CPUIntType{8,8} ]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(torch)\n",
    "\n",
    "# instanciar um modelo simples\n",
    "model <- GPT(\n",
    "  block_size = 8,\n",
    "  n_embd = 16,\n",
    "  N_Layers = 2,\n",
    "  nvoc = 32,\n",
    "  Head = 2\n",
    ")\n",
    "\n",
    "# entrada: 1 batch, T=8, tipo long (√≠ndices de tokens)\n",
    "x <- torch_tensor(matrix(1:8, nrow = 1))\n",
    "\n",
    "# executar e inspecionar intermedi√°rios\n",
    "res <- model(x, return_intermediates = TRUE)\n",
    "res$x1      # posi√ß√µes 1..T\n",
    "res$wei     # m√°scara causal (T x T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9c06f4",
   "metadata": {},
   "source": [
    "## üß† Fluxo Computacional do Modelo `GPT` (Fun√ß√£o `forward()`)\n",
    "\n",
    "```text\n",
    "Entrada x  (tokens inteiros)\n",
    "      ‚îÇ\n",
    "      ‚ñº\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë  üîπ Embeddings                               ‚ïë\n",
    "‚ïë  self$wte(x)  ‚Üí embedding sem√¢ntico          ‚ïë\n",
    "‚ïë  self$wpe(x1) ‚Üí embedding posicional         ‚ïë\n",
    "‚ïë  output = wte(x) + wpe(x1)                   ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "      ‚îÇ\n",
    "      ‚ñº\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë  üîπ M√°scara causal (wei)                     ‚ïë\n",
    "‚ïë  Matriz (T√óT) triangular superior = 1        ‚ïë\n",
    "‚ïë  ‚Üí impede que o token veja o \"futuro\"        ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "      ‚îÇ\n",
    "      ‚ñº\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë  üîÅ N vezes (para cada camada j)              ‚ïë\n",
    "‚ïë  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚ïë\n",
    "‚ïë  ‚îÇ 1. Normaliza√ß√£o:  self$scale1[[j]](x)   ‚îÇ ‚ïë\n",
    "‚ïë  ‚îÇ 2. Multi-Head Attention:                ‚îÇ ‚ïë\n",
    "‚ïë  ‚îÇ    Q,K,V = output                       ‚îÇ ‚ïë\n",
    "‚ïë  ‚îÇ    attn_out = self$MM[[j]](Q,K,V,mask)  ‚îÇ ‚ïë\n",
    "‚ïë  ‚îÇ 3. Res√≠duo:  output ‚Üê output + attn_out ‚îÇ ‚ïë\n",
    "‚ïë  ‚îÇ 4. Normaliza√ß√£o:  self$scale2[[j]](x)   ‚îÇ ‚ïë\n",
    "‚ïë  ‚îÇ 5. Feed-Forward: self$FFN[[j]](...)     ‚îÇ ‚ïë\n",
    "‚ïë  ‚îÇ 6. Res√≠duo:  output ‚Üê output + FFN_out  ‚îÇ ‚ïë\n",
    "‚ïë  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "      ‚îÇ\n",
    "      ‚ñº\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë  üîπ Normaliza√ß√£o final + Cabe√ßa Linear        ‚ïë\n",
    "‚ïë  output ‚Üê self$scale3(output)                 ‚ïë\n",
    "‚ïë  logits ‚Üê self$ln_f(output)                   ‚ïë\n",
    "‚ïë  (dimens√µes: [batch, seq_len, vocabul√°rio])   ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "      ‚îÇ\n",
    "      ‚ñº\n",
    "Sa√≠da: `logits`  ‚Üí pontua√ß√µes para cada token poss√≠vel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41a560c",
   "metadata": {},
   "source": [
    "## üß© Interpreta√ß√£o \n",
    "\n",
    "| **Etapa** | **Significado Matem√°tico** | **Interpreta√ß√£o Estat√≠stica** |\n",
    "|------------|----------------------------|--------------------------------|\n",
    "| **wte(x)** | embedding de palavras | converte cada token discreto em vetor cont√≠nuo ( x‚Çú ‚àà ‚Ñù·µê ) |\n",
    "| **wpe(x1)** | embedding posicional | injeta informa√ß√£o de ordem (posi√ß√£o temporal) |\n",
    "| **wei** | m√°scara causal | implementa  P(X‚Çú‚Çä‚ÇÅ ‚à£ X‚ÇÅ:‚Çú),  proibindo olhar para  X‚Çç‚Çä‚Çú‚Çé |\n",
    "| **MM (multihead)** | autoaten√ß√£o | estima depend√™ncias condicionais entre tokens |\n",
    "| **FFN** | feed-forward | mistura n√£o linear ‚Äî ajusta as representa√ß√µes locais |\n",
    "| **ln_f** | camada linear final | converte o espa√ßo latente em logits para o vocabul√°rio |\n",
    "| **logits** | sa√≠da final | aproxima  PÃÇ_Œ∏(X‚Çú‚Çä‚ÇÅ = v·µ¢ ‚à£ X‚ÇÅ:‚Çú) |\n",
    "\n",
    "---\n",
    "\n",
    "### üìò **Resumo**\n",
    "\n",
    "O `forward()` implementa o c√°lculo da **verossimilhan√ßa condicional**  \n",
    "f_Œ∏(X‚Çú‚Çä‚ÇÅ ‚à£ X‚ÇÅ:‚Çú)  \n",
    "atrav√©s de uma sequ√™ncia de transforma√ß√µes:\n",
    "\n",
    "‚û°Ô∏è **embeddings ‚Üí aten√ß√£o ‚Üí normaliza√ß√µes ‚Üí logits**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f68624",
   "metadata": {},
   "source": [
    "### Gerando o texto referencia de treino para o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "688ebde8",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Gera√ß√£o de dados ABC ‚Äî sequ√™ncias Markovianas\n",
    "# ============================================================\n",
    "\n",
    "file_name=\"ABC.txt\"\n",
    "# Defini√ß√£o do vocabul√°rio\n",
    "voc <- c(\"AABCBBC\", \"BCABCCA\", \"CAAACB\\n\\n\")\n",
    "#p <- rbind(c(0.4,0.4,0.2), c(0.6, 0.2, 0.2), c(0.7, 0.2, 0.1))\n",
    "p <- list()\n",
    "p[[1]] <- cbind(c(0.4, 0.4, 0.2))\n",
    "set.seed(1)\n",
    "M = 150000\n",
    "# for( i in 2:M)\n",
    "#    p[[t]] = p%*%p[[i - 1]]\n",
    "\n",
    "aux = function(s) voc[sample(1:3, 1, prob=p[[1]])]\n",
    "ABC <- paste(sapply(1:M, aux), collapse=\"\")\n",
    "write(ABC[1], file=file_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f8ec11",
   "metadata": {},
   "source": [
    "## Extra√ß√£o do vocabul√°rio do texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b77c2bc",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"<PAD>\" \"\\n\"    \"A\"     \"B\"     \"C\"    \n"
     ]
    }
   ],
   "source": [
    "#encoding into token ids\n",
    "\n",
    "file <- base::readChar(config$file_name, file.info(config$file_name)$size)\n",
    "voc <- c(\"<PAD>\", sort(unique(unlist(strsplit(file, \"\")))))\n",
    "print(voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1461fbec",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "Encoder = function(file = file0, vocabulary = voc){\n",
    "  file = unlist(strsplit(file, \"\"))\n",
    "  filex = numeric(length(file))\n",
    "  for(i in 1:length(vocabulary)){\n",
    "    filex[file == vocabulary[i]] <- i\n",
    "  }\n",
    "  return(filex)\n",
    "}\n",
    "\n",
    "Decoder = function(file = file1, vocabulary = voc){\n",
    "  filex = file\n",
    "  for(i in 1:length(vocabulary)){\n",
    "    filex[file == i] <- vocabulary[i]\n",
    "  }\n",
    "  return(filex)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "352a85d7",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 5\n"
     ]
    }
   ],
   "source": [
    "encoded <- Encoder(file = file, vocabulary = voc)\n",
    "nvoc <- length(voc)\n",
    "print(nvoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8c9ddc34",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>3</li><li>4</li><li>3</li><li>5</li><li>2</li><li>5</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 3\n",
       "\\item 4\n",
       "\\item 3\n",
       "\\item 5\n",
       "\\item 2\n",
       "\\item 5\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 3\n",
       "2. 4\n",
       "3. 3\n",
       "4. 5\n",
       "5. 2\n",
       "6. 5\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 3 4 3 5 2 5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>'A'</li><li>'B'</li><li>'A'</li><li>'C'</li><li>'\\n'</li><li>'C'</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'A'\n",
       "\\item 'B'\n",
       "\\item 'A'\n",
       "\\item 'C'\n",
       "\\item '\\textbackslash{}n'\n",
       "\\item 'C'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'A'\n",
       "2. 'B'\n",
       "3. 'A'\n",
       "4. 'C'\n",
       "5. '\\n'\n",
       "6. 'C'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"A\"  \"B\"  \"A\"  \"C\"  \"\\n\" \"C\" "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'ABAC\\nC'"
      ],
      "text/latex": [
       "'ABAC\\textbackslash{}nC'"
      ],
      "text/markdown": [
       "'ABAC\\nC'"
      ],
      "text/plain": [
       "[1] \"ABAC\\nC\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# texto de exemplo\n",
    "file0 <- \"ABA C\\nC\"\n",
    "file0 <- gsub(\" \", \"\", file0)   # (se quiser tirar espa√ßos)\n",
    "\n",
    "# encoder ‚Üí √≠ndices\n",
    "enc <- Encoder(file = file0, vocabulary = voc)\n",
    "enc\n",
    "# ex.: 3 4 3 5 2 5\n",
    "\n",
    "# decoder ‚Üí volta para s√≠mbolos\n",
    "dec <- Decoder(file = enc, vocabulary = voc)\n",
    "dec\n",
    "paste(dec, collapse = \"\")\n",
    "# deve reconstruir exatamente 'file0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d3e8b4",
   "metadata": {},
   "source": [
    "### Defini√ß√£o e Treino do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3df20958",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "Model <- GPT(block_size = config$block_size,\n",
    "                n_embd = config$n_embd,\n",
    "                N_Layers = config$N_Layers,\n",
    "                nvoc = nvoc,\n",
    "                Head = config$Head)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c1d30265",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wpe.weight           16 x 128 ‚Üí 2048 par√¢metros\n",
      "wte.weight           5 x 128 ‚Üí 640 par√¢metros\n",
      "MM.0.out_proj.weight 128 x 128 ‚Üí 16384 par√¢metros\n",
      "MM.0.out_proj.bias   128 ‚Üí 128 par√¢metros\n",
      "MM.0.in_proj_weight  384 x 128 ‚Üí 49152 par√¢metros\n",
      "MM.0.in_proj_bias    384 ‚Üí 384 par√¢metros\n",
      "MM.1.out_proj.weight 128 x 128 ‚Üí 16384 par√¢metros\n",
      "MM.1.out_proj.bias   128 ‚Üí 128 par√¢metros\n",
      "MM.1.in_proj_weight  384 x 128 ‚Üí 49152 par√¢metros\n",
      "MM.1.in_proj_bias    384 ‚Üí 384 par√¢metros\n",
      "scale1.0.weight      128 ‚Üí 128 par√¢metros\n",
      "scale1.0.bias        128 ‚Üí 128 par√¢metros\n",
      "scale1.1.weight      128 ‚Üí 128 par√¢metros\n",
      "scale1.1.bias        128 ‚Üí 128 par√¢metros\n",
      "scale2.0.weight      128 ‚Üí 128 par√¢metros\n",
      "scale2.0.bias        128 ‚Üí 128 par√¢metros\n",
      "scale2.1.weight      128 ‚Üí 128 par√¢metros\n",
      "scale2.1.bias        128 ‚Üí 128 par√¢metros\n",
      "scale3.weight        128 ‚Üí 128 par√¢metros\n",
      "scale3.bias          128 ‚Üí 128 par√¢metros\n",
      "FFN.0.0.weight       512 x 128 ‚Üí 65536 par√¢metros\n",
      "FFN.0.0.bias         512 ‚Üí 512 par√¢metros\n",
      "FFN.0.2.weight       128 x 512 ‚Üí 65536 par√¢metros\n",
      "FFN.0.2.bias         128 ‚Üí 128 par√¢metros\n",
      "FFN.1.0.weight       512 x 128 ‚Üí 65536 par√¢metros\n",
      "FFN.1.0.bias         512 ‚Üí 512 par√¢metros\n",
      "FFN.1.2.weight       128 x 512 ‚Üí 65536 par√¢metros\n",
      "FFN.1.2.bias         128 ‚Üí 128 par√¢metros\n",
      "ln_f.weight          5 x 128 ‚Üí 640 par√¢metros\n",
      "Total de par√¢metros no modelo: 400128 \n"
     ]
    }
   ],
   "source": [
    "params <- Model$named_parameters()\n",
    "\n",
    "for (p in names(params)) {\n",
    "  cat(sprintf(\"%-20s %s ‚Üí %d par√¢metros\\n\",\n",
    "              p,\n",
    "              paste(dim(params[[p]]), collapse = \" x \"),\n",
    "              prod(dim(params[[p]]))))\n",
    "}\n",
    "\n",
    "total_params <- sum(sapply(params, function(x) prod(dim(x))))\n",
    "cat(\"Total de par√¢metros no modelo:\", total_params, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b57d7f1",
   "metadata": {},
   "source": [
    "## Sanity check\n",
    "\n",
    "Aplicacao da softmax no modelo n√£o treinado ainda.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c7f58727",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch_tensor\n",
       "1\n",
       "[ CPUFloatType{} ][ grad_fn = <SumBackward0> ]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch_tensor\n",
       " 1.0000  1.0000  1.0000  1.0000  1.0000  1.0000  1.0000  1.0000\n",
       "[ CPUFloatType{1,8} ][ grad_fn = <SumBackward1> ]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1) modelo\n",
    "Model <- GPT(\n",
    "  block_size = config$block_size,\n",
    "  n_embd     = config$n_embd,\n",
    "  N_Layers   = config$N_Layers,\n",
    "  nvoc       = nvoc,            # tamanho do vocabul√°rio que voc√™ calculou\n",
    "  Head       = config$Head,\n",
    "  p0         = config$p0\n",
    ")\n",
    "\n",
    "# 2) entrada v√°lida\n",
    "T <- 8\n",
    "x <- torch::torch_tensor(matrix(sample(1:nvoc, T, replace = TRUE), nrow = 1),\n",
    "                         dtype = torch::torch_long())\n",
    "\n",
    "# 3) forward -> logits (B, T, nvoc)\n",
    "logits <- Model(x)\n",
    "\n",
    "# 4) softmax no √∫ltimo eixo -> probas\n",
    "p <- torch::nnf_softmax(logits, dim = -1)\n",
    "\n",
    "# 5) verifique que cada distribui√ß√£o em (b, t, :) soma 1\n",
    "torch::torch_sum(p[1, 1, ])    # deve imprimir ~1\n",
    "torch::torch_sum(p, dim = -1)  # vetor (B, T) todo de 1's"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4460f534",
   "metadata": {},
   "source": [
    "### Defini√ß√£o do modelo de treino e do modelo de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4e6efe35",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "file0 <- readChar(\"ABC.txt\", file.info(\"ABC.txt\")$size)\n",
    "voc <- c(\"<PAD>\", sort(unique(unlist(strsplit(file0, \"\")))))\n",
    "Encoded <- Encoder(file = file0, vocabulary = voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4f6dab32",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "p_train = 0.8\n",
    "\n",
    "# Divis√£o dos dados\n",
    "n <- length(Encoded)\n",
    "\n",
    "BD.train <- torch_tensor(Encoded[1:round(p_train*n)],dtype=torch_int())\n",
    "BD.test <- torch_tensor(Encoded[round(p_train*n+1):n], dtype = torch_int())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5a6e07",
   "metadata": {},
   "source": [
    "### Treino do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0742c13f",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in loss(FIT$view(c(-1, FIT$size(-1))), v$view(-1)): n√£o foi poss√≠vel encontrar a fun√ß√£o \"loss\"\n",
     "output_type": "error",
     "traceback": [
      "Error in loss(FIT$view(c(-1, FIT$size(-1))), v$view(-1)): n√£o foi poss√≠vel encontrar a fun√ß√£o \"loss\"\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "for(i in 1:config$epochs){\n",
    "\n",
    "    # 1) posi√ß√µes iniciais aleat√≥rias (garante que caiba um bloco + 1)\n",
    "    idx = sample(1:(round(p_treino*n) - config$batch_size), config$batch_size)\n",
    "\n",
    "    # 2) para cada idx, empilha a sequ√™ncia idx + 0..block_size\n",
    "    #    (forma: (batch_size) x (block_size+1)) e depois \"flatten\"\n",
    "    idx2 = as.integer(c(t(pmin(outer(as.integer(idx), 0:config$block_size, '+'), n))))\n",
    "\n",
    "    Z <- BD.train[idx2, drop = FALSE]$view(c(length(idx), config$block_size + 1))\n",
    "    X <- Z[,1:config$block_size]\n",
    "    Y <- Z[,2:(config$block_size+1)]\n",
    "    \n",
    "    Model$train()          # ativa modo de treino (dropout etc.)\n",
    "    FIT <- Model(X)        # forward pass\n",
    "    loss0 <- loss(FIT$view(c(-1, FIT$size(-1))), v$view(-1))\n",
    "\n",
    "    print(loss0$item())\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a7c6809b",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error: objeto 'loss_store' n√£o encontrado\n",
     "output_type": "error",
     "traceback": [
      "Error: objeto 'loss_store' n√£o encontrado\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "optimizer <- torch::optim_adamw(Model$parameters, lr= config$lr)\n",
    "loss0 <- torch::nn_cross_entropy_loss()\n",
    "\n",
    "for (i in 1:config$epochs) {\n",
    "\n",
    "  # 1) posi√ß√µes iniciais aleat√≥rias (garante que caiba um bloco+1)\n",
    "  idx  <- sample(1:(round(p_treino * n) - config$batch_size), config$batch_size)\n",
    "\n",
    "  # 2) para cada idx, empilha a sequ√™ncia idx + 0..block_size  (flatten depois)\n",
    "  idx2 <- as.integer(c(t(pmin(outer(as.integer(idx), 0:config$block_size, `+`), n))))\n",
    "\n",
    "  # 3) monta os pares (X, Y) com janela deslizante de tamanho block_size\n",
    "  Z <- BD.train[idx2, drop = FALSE]$view(c(length(idx), config$block_size + 1))\n",
    "  X <- Z[, 1:config$block_size]\n",
    "  Y <- Z[, 2:(config$block_size + 1)]\n",
    "\n",
    "  # 4) forward + perda\n",
    "  FIT  <- Model$train()(X)   # chama o forward\n",
    "  loss = loss0(FIT$flatten(end_dim=2), Y$flatten())\n",
    "  optimizer$zero_grad()\n",
    "  loss$backward()\n",
    "  optimizer$step()\n",
    "  loss_store[j] = loss$item()\n",
    "  cli::cli_progress_message(paste(\"Epoca: \", j, \"Train loss: \", loss_store[j]))\n",
    "\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29166d41",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
