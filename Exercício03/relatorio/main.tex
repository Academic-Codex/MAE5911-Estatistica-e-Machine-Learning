\documentclass[a4paper]{article}
\usepackage{student}
\usepackage{graphicx}
\usepackage{caption}
\usepackage[version=4]{mhchem}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows.meta, positioning, decorations.pathreplacing}
\usepackage{enumitem}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{float}
\usepackage[portuguese]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[numbers,sort&compress]{natbib} % ou [authoryear]
\usepackage[alf]{abntex2cite} % citação ABNT autor-data
\usepackage{tcolorbox}
\usepackage{enumitem}
\usepackage{tikz}
\usetikzlibrary{positioning,shapes.geometric,arrows.meta,calc}
% --- TikZ básico ---
\usepackage{tikz}

% --- Bibliotecas necessárias ---
\usetikzlibrary{positioning}   % para node distance, below=..., right=...
\usetikzlibrary{shapes.geometric} % caixas arredondadas, shapes
\usetikzlibrary{arrows.meta}   % setas melhores
\usetikzlibrary{calc}          % coordenadas TikZ avançadas (útil p/ labels)

% --- Tabela bonita (para top/mid/bottom rule) ---
\usepackage{booktabs}

% --- Para usar [H] ---
\usepackage{float}

% --- Fonte menor no caption e figure ---
\usepackage{caption}
\tcbuselibrary{breakable}
\pagestyle{plain}

\tikzstyle{arrow} = [thick,->,>=stealth]

% Definindo o estilo de destaque com linhas pontilhadas
\tikzstyle{highlight} = [draw, dashed, thick, rectangle, rounded corners, inner sep=0.2cm, orange]


\tikzstyle{startstop} = [
    rectangle, rounded corners, minimum width=0.5cm,
    text centered, draw=black, fill=blue!10, font=\small
]
\tikzstyle{startstop_S} = [
    rectangle, rounded corners, minimum width=0.5cm, minimum height=0.8cm,
    text centered, draw=black, fill=green!30, font=\small
]
\tikzstyle{decision} = [
    diamond, aspect=2, draw=black, fill=orange!15, align=center,
    text centered, inner sep=0pt, font=\small
]
\tikzstyle{decision_S} = [
    diamond, aspect=2, draw=black, fill=orange!30, align=center,
    text centered, inner sep=0pt, font=\small
]
\tikzstyle{arrow} = [thick,->,>=stealth]



% Metadata
\date{\today}
\setmodule{MAE5911/IME: Fundamentos de Estatística e Machine Learning. \\ Prof.: Alexandre Galvão Patriota} 
\setterm{2o. semestre, 2025}

%-------------------------------%
% Other details
% TODO: Fill these
%-------------------------------%
\title{Lista 03 - 21/11}
\setmembername{Nara Avila Moraes}  % Fill group member names
\setmemberuid{5716734}  % Fill group member uids (same order)

%-------------------------------%
% Add / Delete commands and packages
% TODO: Add / Delete here as you need
%-------------------------------%
\usepackage{amsmath,amssymb,bm}

\newcommand{\KL}{\mathrm{KL}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\T}{\top}

\newcommand{\expdist}[2]{%
        \normalfont{\textsc{Exp}}(#1, #2)%
    }
\newcommand{\expparam}{\bm \lambda}
\newcommand{\Expparam}{\bm \Lambda}
\newcommand{\natparam}{\bm \eta}
\newcommand{\Natparam}{\bm H}
\newcommand{\sufstat}{\bm u}

% Main document
\begin{document}
% Add header
\header{}



\textbf{Questão 01:}
Considere uma amostra aleatória $(Y_1,X_1),\ldots,(Y_n,X_n)$ de $(Y,X)$ tal que a distribuição condicional
$Y \mid X = x \sim \mathcal{N}\big(\mu_{\theta}(x),\,\sigma^{2}_{\theta}(x)\big),$
e suponha que a distribuição de $X$ não contém informação sobre os parâmetros.


\begin{enumerate}[leftmargin=2.5cm, itemindent=0pt, label=\textbf{Ítem 1.\arabic*}]
    \item[\textbf{Ítem 1.1}] Apresente uma \textbf{rede neural} que modele o \textbf{quantil de ordem $75\%$} (terceiro quartil) da distribuição condicional $Y \mid X = x$.  O código deve ser generalizável para qualquer quantil.

    \item[\textbf{Ítem 1.2}] Mostre a aplicação do método nos seguintes dados simulados em \textsf{R}:
          \noindent
          \begin{tcolorbox}[
                  width=\linewidth,
                  colback=white,
                  colframe=black,
                  boxrule=0.2pt,   % <<< mais fino (padrão é 0.5pt)
                  left=0mm
              ]

              \[
                  \text{set.seed}(32)
              \]

              \[
                  n = 1000
              \]

              \[
                  x = \operatorname{sort}(\operatorname{runif}(n,-4,4))
              \]

              \[
                  y = \frac{3}{3 + 2|x|^{3}} + e^{-x^{2}} + \cos(x)\sin(x) + 0.3\,\varepsilon,
                  \qquad \varepsilon \sim \mathcal{N}(0,1)
              \]

          \end{tcolorbox}


\end{enumerate}
\noindent
Sugestão: reescreva a esperança e variância condicionais em termos do quantil
e construa a função de verossimilhança apropriada.

\begin{answer}[Ítem 1.1]
    \subsubsection*{Encontrando a função de verossimilhança em termos do quantil condicional}

    Seja
    \begin{equation}
        Y \mid X = x \sim \mathcal{N}\big(\mu(x), \sigma^2(x)\big).
    \end{equation}

    Para um nível de quantil $q \in (0,1)$, definimos o quantil condicional
    $Q_q(x)$ de $Y \mid X=x$ como o valor $t$ que satisfaz
    \begin{equation}
        \mathbb{P}\!\big( Y \le t \,\big|\, X = x \big) = q.
    \end{equation}

    No caso da Normal, o quantil condicional pode ser escrito em termos da média
    e do desvio-padrão:
    \begin{equation}
        Q_q(x) = \mu(x) + \sigma(x)\, z_q,
    \end{equation}
    em que $z_q = \Phi^{-1}(q)$ é o quantil de ordem $q$ da Normal padrão
    $\mathcal{N}(0,1)$ e $\Phi$ denota a sua função de distribuição acumulada.

    Podemos, então, reparametrizar o modelo em função de $Q_q(x)$, escrevendo
    \begin{equation}
        \mu(x) = Q_q(x) - \sigma(x)\,z_q.
    \end{equation}

    \subsection{Verossimilhança em termos do quantil}

    A densidade de $Y \mid X=x$ é
    \begin{equation}
        f(y \mid x) =
        \frac{1}{\sqrt{2\pi}\,\sigma(x)}
        \exp\!\left\{
        -\frac{\big(y-\mu(x)\big)^2}{2\sigma^2(x)}
        \right\}.
    \end{equation}

    Substituindo $\mu(x) = Q_q(x) - \sigma(x)z_q$, obtemos
    \begin{equation}
        f(y \mid x) =
        \frac{1}{\sqrt{2\pi}\,\sigma(x)}
        \exp\!\left\{
        -\frac{\big(y - Q_q(x) + \sigma(x) z_q\big)^2}
        {2\sigma^2(x)}
        \right\}.
    \end{equation}

    A log-verossimilhança para uma observação $(x_i,y_i)$, desprezando constantes
    que não dependem de $Q_q$, é dada por
    \begin{equation}
        \ell_i\big(Q_q,\sigma\big)
        = - \log \sigma(x_i)
        - \frac{\big(y_i - Q_q(x_i) + \sigma(x_i) z_q\big)^2}
        {2\sigma^2(x_i)}.
    \end{equation}

    O correspondente negativo da log-verossimilhança (função de perda) é
    \begin{equation}
        \mathcal{L}_{\text{NLL}}
        = \sum_{i=1}^n
        \left[
            \log \sigma(x_i)
            +
            \frac{\big(y_i - Q_q(x_i) + \sigma(x_i) z_q\big)^2}
            {2\sigma^2(x_i)}
            \right].
    \end{equation}


    \subsection*{Descrição do experimento e configuração do modelo}
    \subsubsection*{Hiperparâmetros do modelo}
    \begin{table}[H]
        \centering
        \caption{Hiperparâmetros utilizados no modelo de quantis.}
        \begin{tabular}{lll}
            \toprule
            \textbf{Hiperparâmetro} & \textbf{Valor} & \textbf{Descrição}                                         \\
            \midrule

            \texttt{q}              & 0.75           & quantil alvo modelado pela rede.                           \\

            \texttt{num\_epochs}    & 2500           & número total de épocas de treinamento.                     \\

            \texttt{lr}             & $10^{-3}$      & taxa de aprendizado (learning rate).                       \\

            \texttt{input\_dim}     & 1              & dimensão da entrada $x$.                                   \\

            \texttt{hidden\_mu}     & 10             & número de neurônios na camada oculta da rede do quantil.   \\

            \texttt{hidden\_sigma}  & 10             & número de neurônios na camada oculta da rede da variância. \\

            \texttt{seed}           & 2025           & semente aleatória utilizada nos experimentos.              \\

            \texttt{p\_train}       & 0.7            & proporção dos dados destinada ao treino.                   \\

            \bottomrule
        \end{tabular}
        \label{tab:hiperparametros_quantil}
    \end{table}
    \subsubsection*{Definição da Arquitetura}
    \begin{figure}[H]
        \centering
        \begin{tikzpicture}[
                x=1.8cm, y=1.2cm,
                >=Stealth,
                neuron/.style={circle,draw,minimum size=6mm},
                unicorn/.style={diamond, draw, fill=purple!10,
                        minimum width=18mm, minimum height=10mm},
                actblock/.style={rectangle,draw,rounded corners,
                        minimum width=9mm,minimum height=5mm},
                every node/.style={font=\footnotesize}
            ]

            % ======== INPUT NODE ========
            \node[neuron,label=left:{$x$}] (x) at (0,-1) {};

            % =====================================================
            % ===============  REDE DO QUANTIL  ===================
            % =====================================================


            % 10 neurônios ocultos (posicionados de y=3.5 até ~0.35)
            \foreach \i in {1,...,10}{
                    \node[neuron] (qh\i) at (2,{3.5 - 0.35*(\i-1)}) {};
                }

            % 10 blocos GELU (um por neurônio) – pequenos e colados na 1ª camada
            \foreach \i in {1,...,10}{
                    \node[actblock, minimum width=6mm, minimum height=4mm]
                    (geluq\i) at (2.5,{3.5 - 0.35*(\i-1)})
                    {\begin{tikzpicture}[scale=0.11]
                                \draw[line width=0.35pt]
                                (-1,-0.25)
                                .. controls (-0.6,-0.3) and (-0.1,-0.05) ..
                                (0.3,0.25)
                                .. controls (0.7,0.6) and (1,0.95) ..
                                (1.3,1.05);
                            \end{tikzpicture}};
                }



            % saída q(x)
            \node[neuron,fill=blue!15,
            label={[yshift=6pt]right:{$q(x)$}}]
            (qout) at (3.9,1.75) {};

            % input -> hidden (quantil)  [Linear(1,10): 10 pesos]
            \foreach \i in {1,...,10}{
                    \draw[->] (x) -- (qh\i);
                }

            % hidden -> GELU (cada neurônio tem sua própria ativação)
            \foreach \i in {1,...,10}{
                    \draw[->] (qh\i) -- (geluq\i);
                }

            % GELU -> saída (Linear(10,1): 10 pesos indo para q(x))
            \foreach \i in {1,...,10}{
                    \draw[->] (geluq\i) -- (qout);
                }

            % labels de parâmetros no ramo do quantil
            \node[align=center] at (2.2,4.1) {%
                \scriptsize Linear $(1,10)$:\\
                $10$ pesos + $10$ vieses
            };

            \node[align=center] at (4.2,2.4) {%
                \scriptsize Linear $(10,1)$:\\
                $10$ pesos + $1$ viés
            };

            % label da rede do quantil
            \node at (3.2,4.7) {\textbf{Sub-rede do Quantil } $Q_\tau(x)$};


            % =====================================================
            % ===============  REDE DA VARIÂNCIA  =================
            % =====================================================

            % 10 neurônios ocultos (y=-1.2 até ~-4.55)
            \foreach \i in {1,...,10}{
                    \node[neuron] (sh\i) at (2,{-1.2 - 0.35*(\i-1)}) {};
                }

            % 10 blocos GELU da variância (um por neurônio)
            \foreach \i in {1,...,10}{
                    \node[actblock, minimum width=6mm, minimum height=4mm]
                    (gelus\i) at (2.5,{-1.2 - 0.35*(\i-1)})                     {\begin{tikzpicture}[scale=0.11]
                                \draw[line width=0.35pt]
                                (-1,-0.25)
                                .. controls (-0.6,-0.3) and (-0.1,-0.05) ..
                                (0.3,0.25)
                                .. controls (0.7,0.6) and (1,0.95) ..
                                (1.3,1.05);
                            \end{tikzpicture}};;
                }

            % saída sigma_raw(x)
            \node[neuron,fill=green!15,label=right:{$\sigma_{\mathrm{raw}}(x)$}]
            (sraw) at (3.9,-3.0) {};

            % input -> hidden (variância) [Linear(1,10)]
            \foreach \i in {1,...,10}{
                    \draw[->] (x) -- (sh\i);
                }

            % hidden -> GELU (variância)
            \foreach \i in {1,...,10}{
                    \draw[->] (sh\i) -- (gelus\i);
                }

            % GELU -> saída (variância) [Linear(10,1): 10 pesos]
            \foreach \i in {1,...,10}{
                    \draw[->] (gelus\i) -- (sraw);
                }

            % labels de parâmetros no ramo da variância
            \node[align=center] at (2.2,-0.6) {%
                \scriptsize Linear $(1,10)$:\\
                $10$ pesos + $10$ vieses
            };

            \node[align=center] at (4.1,-2.3) {%
                \scriptsize Linear $(10,1)$:\\
                $10$ pesos + $1$ viés
            };

            % label da rede da variância
            \node at (3,-0.1) {\textbf{Sub-rede da Variância}};

            % ----------------  Softplus + eps  ----------------
            \node[draw,rounded corners,minimum width=2.4cm,minimum height=0.9cm,
                below=0.8cm of sraw] (softp)
            {Softplus $+\;\varepsilon$};

            \draw[->] (sraw) -- (softp);

            % --------------------  Quadrado  -------------------
            \node[draw,rounded corners,minimum width=2.0cm,minimum height=0.9cm,
                below=0.8cm of softp] (square)
            {$(\cdot)^2$};

            \draw[->] (softp) -- (square);

            % saída final sigma^2(x)
            \node[neuron,fill=green!25,
            label={[yshift=-11pt]right:{$\sigma^2(x)$}}]
            (sfinal) at (3.9,-7) {};

            \draw[->] (square) -- (sfinal);


            % =====================================================
            % ===============  NÓ FINAL forward()  ================
            % =====================================================

            \node[unicorn, right=3cm of $(qout)!0.5!(sfinal)$] (forward)
            {\texttt{forward()}};

            % setas q(x) -> forward
            \draw[->, thick] (qout.east) -- ++(0.8,0) |- (forward.west);

            % setas sigma^2(x) -> forward
            \draw[->, thick] (sfinal.east) -- ++(0.8,0) |- (forward.west);

            % nó de saída { q(x), sigma^2(x) }
            \node[neuron, fill=purple!20, right=1.4cm of forward,
            label=right:{$\{\,q(x),\;\sigma^2(x)\,\}$}] (final) {};

            \draw[->, thick] (forward) -- (final);

        \end{tikzpicture}
        \caption{Arquitetura da rede neural com 10 neurônios ocultos em cada subrede.}
    \end{figure}
    \begin{table}[H]
        \centering
        \footnotesize
        \begin{tabular}{lcc}
            \toprule
            \textbf{Parâmetro}  & \textbf{Dimensão} & \textbf{Nº de parâmetros} \\
            \midrule
            q\_net.0.weight     & $10 \times 1$     & 10                        \\
            q\_net.0.bias       & $10$              & 10                        \\
            q\_net.2.weight     & $1 \times 10$     & 10                        \\
            q\_net.2.bias       & $1$               & 1                         \\
            sigma\_net.0.weight & $10 \times 1$     & 10                        \\
            sigma\_net.0.bias   & $10$              & 10                        \\
            sigma\_net.2.weight & $1 \times 10$     & 10                        \\
            sigma\_net.2.bias   & $1$               & 1                         \\
            \midrule
            \textbf{Total}      & ---               & \textbf{62}               \\
            \bottomrule
        \end{tabular}
        \caption{Parâmetros treináveis da rede neural utilizada no experimento.}
        \label{tab:parametros_quantil}
    \end{table}
    \subsubsection*{Treinamento e métricas}
    Para o treinamento foram utilizados dados sintéticos gerados a partir de uma 
função $f(x)$ arbitrária com ruído que segue uma variância heteroscedástica 
$\sigma^2(x)$, da seguinte forma:

\[
Y = f(x) + \varepsilon, 
\qquad 
\varepsilon \sim \mathcal{N}\!\left(0,\, \sigma^2(x)\right),
\]

de modo que tanto a tendência central quanto a dispersão dos dados variam 
continuamente ao longo do domínio de $x$.

A média verdadeira utilizada foi definida por

\[
f(x)
= 
\frac{3}{\,3 + 2|x|^3\,}
+ e^{-x^2}
+ \cos(x)\,\sin(x).
\]

A variância condicional verdadeira foi especificada por:

\[
\sigma^2(x)
=
0.3
+ 0.8\!\left(1 + \sin(1 + 2x) + 0.2\cos(1 + 2x)\right)^2.
\]

A heteroscedasticidade dos dados é essencial para fornecer a capacidade do modelo em 
aprender padrões de ruído condicional.

Os dados finais utilizados no treinamento foram obtidos por:

\[
y_i = f(x_i) + \varepsilon_i,
\qquad
\varepsilon_i \sim \mathcal{N}\!\bigl(0,\,\sigma^2(x_i)\bigr),
\]

com $n = 1000$ amostras no intervalo $[-4, 4]$, ordenadas para facilitar a 
visualização gráfica, e divididas em proporção $70\%$ para treino e $30\%$ para 
teste.

\subsubsection*{Treino e Resultado}

\end{answer}


\end{document}
