\documentclass[a4paper]{article}
\usepackage{student}
\usepackage{graphicx}
\usepackage{caption}
\usepackage[version=4]{mhchem}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows.meta, positioning, decorations.pathreplacing}
\usepackage{enumitem}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{float}
\usepackage[portuguese]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[numbers,sort&compress]{natbib} % ou [authoryear]
\usepackage[alf]{abntex2cite} % citação ABNT autor-data
\usepackage{tcolorbox}
\usepackage{enumitem}
\tcbuselibrary{breakable}
\pagestyle{plain}

\tikzstyle{arrow} = [thick,->,>=stealth]

% Definindo o estilo de destaque com linhas pontilhadas
\tikzstyle{highlight} = [draw, dashed, thick, rectangle, rounded corners, inner sep=0.2cm, orange]


\tikzstyle{startstop} = [
    rectangle, rounded corners, minimum width=0.5cm,
    text centered, draw=black, fill=blue!10, font=\small
]
\tikzstyle{startstop_S} = [
    rectangle, rounded corners, minimum width=0.5cm, minimum height=0.8cm,
    text centered, draw=black, fill=green!30, font=\small
]
\tikzstyle{decision} = [
    diamond, aspect=2, draw=black, fill=orange!15, align=center,
    text centered, inner sep=0pt, font=\small
]
\tikzstyle{decision_S} = [
    diamond, aspect=2, draw=black, fill=orange!30, align=center,
    text centered, inner sep=0pt, font=\small
]
\tikzstyle{arrow} = [thick,->,>=stealth]



% Metadata
\date{\today}
\setmodule{MAE5911/IME: Fundamentos de Estatística e Machine Learning. \\ Prof.: Alexandre Galvão Patriota} 
\setterm{2o. semestre, 2025}

%-------------------------------%
% Other details
% TODO: Fill these
%-------------------------------%
\title{Lista 03 - 21/11}
\setmembername{Nara Avila Moraes}  % Fill group member names
\setmemberuid{5716734}  % Fill group member uids (same order)

%-------------------------------%
% Add / Delete commands and packages
% TODO: Add / Delete here as you need
%-------------------------------%
\usepackage{amsmath,amssymb,bm}

\newcommand{\KL}{\mathrm{KL}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\T}{\top}

\newcommand{\expdist}[2]{%
        \normalfont{\textsc{Exp}}(#1, #2)%
    }
\newcommand{\expparam}{\bm \lambda}
\newcommand{\Expparam}{\bm \Lambda}
\newcommand{\natparam}{\bm \eta}
\newcommand{\Natparam}{\bm H}
\newcommand{\sufstat}{\bm u}

% Main document
\begin{document}
% Add header
\header{}



\textbf{Questão 01:}
Considere uma amostra aleatória $(Y_1,X_1),\ldots,(Y_n,X_n)$ de $(Y,X)$ tal que a distribuição condicional
$Y \mid X = x \sim \mathcal{N}\big(\mu_{\theta}(x),\,\sigma^{2}_{\theta}(x)\big),$
e suponha que a distribuição de $X$ não contém informação sobre os parâmetros.


\begin{enumerate}[leftmargin=2.5cm, itemindent=0pt, label=\textbf{Ítem 1.\arabic*}]
    \item[\textbf{Ítem 1.1}] Apresente uma \textbf{rede neural} que modele o \textbf{quantil de ordem $75\%$} (terceiro quartil) da distribuição condicional $Y \mid X = x$.  O código deve ser generalizável para qualquer quantil.

    \item[\textbf{Ítem 1.2}] Mostre a aplicação do método nos seguintes dados simulados em \textsf{R}:
          \noindent
          \begin{tcolorbox}[
                  width=\linewidth,
                  colback=white,
                  colframe=black,
                  boxrule=0.2pt,   % <<< mais fino (padrão é 0.5pt)
                  left=0mm
              ]

              \[
                  \text{set.seed}(32)
              \]

              \[
                  n = 1000
              \]

              \[
                  x = \operatorname{sort}(\operatorname{runif}(n,-4,4))
              \]

              \[
                  y = \frac{3}{3 + 2|x|^{3}} + e^{-x^{2}} + \cos(x)\sin(x) + 0.3\,\varepsilon,
                  \qquad \varepsilon \sim \mathcal{N}(0,1)
              \]

          \end{tcolorbox}


\end{enumerate}
\noindent
Sugestão: reescreva a esperança e variância condicionais em termos do quantil
e construa a função de verossimilhança apropriada.

\begin{answer}[Ítem 1.1]
    \subsubsection*{Encontrando a função de verossimilhança em termos do quantil condicional}

    Seja
    \begin{equation}
        Y \mid X = x \sim \mathcal{N}\big(\mu(x), \sigma^2(x)\big).
    \end{equation}

    Para um nível de quantil $q \in (0,1)$, definimos o quantil condicional
    $Q_q(x)$ de $Y \mid X=x$ como o valor $t$ que satisfaz
    \begin{equation}
        \mathbb{P}\!\big( Y \le t \,\big|\, X = x \big) = q.
    \end{equation}

    No caso da Normal, o quantil condicional pode ser escrito em termos da média
    e do desvio-padrão:
    \begin{equation}
        Q_q(x) = \mu(x) + \sigma(x)\, z_q,
    \end{equation}
    em que $z_q = \Phi^{-1}(q)$ é o quantil de ordem $q$ da Normal padrão
    $\mathcal{N}(0,1)$ e $\Phi$ denota a sua função de distribuição acumulada.

    Podemos, então, reparametrizar o modelo em função de $Q_q(x)$, escrevendo
    \begin{equation}
        \mu(x) = Q_q(x) - \sigma(x)\,z_q.
    \end{equation}

    \subsection{Verossimilhança em termos do quantil}

    A densidade de $Y \mid X=x$ é
    \begin{equation}
        f(y \mid x) =
        \frac{1}{\sqrt{2\pi}\,\sigma(x)}
        \exp\!\left\{
        -\frac{\big(y-\mu(x)\big)^2}{2\sigma^2(x)}
        \right\}.
    \end{equation}

    Substituindo $\mu(x) = Q_q(x) - \sigma(x)z_q$, obtemos
    \begin{equation}
        f(y \mid x) =
        \frac{1}{\sqrt{2\pi}\,\sigma(x)}
        \exp\!\left\{
        -\frac{\big(y - Q_q(x) + \sigma(x) z_q\big)^2}
        {2\sigma^2(x)}
        \right\}.
    \end{equation}

    A log-verossimilhança para uma observação $(x_i,y_i)$, desprezando constantes
    que não dependem de $Q_q$, é dada por
    \begin{equation}
        \ell_i\big(Q_q,\sigma\big)
        = - \log \sigma(x_i)
        - \frac{\big(y_i - Q_q(x_i) + \sigma(x_i) z_q\big)^2}
        {2\sigma^2(x_i)}.
    \end{equation}

    O correspondente negativo da log-verossimilhança (função de perda) é
    \begin{equation}
        \mathcal{L}_{\text{NLL}}
        = \sum_{i=1}^n
        \left[
            \log \sigma(x_i)
            +
            \frac{\big(y_i - Q_q(x_i) + \sigma(x_i) z_q\big)^2}
            {2\sigma^2(x_i)}
            \right].
    \end{equation}

    \subsection{Caso com variância conhecida}

    Se assumirmos uma variância condicional conhecida e constante,
    $\sigma(x) \equiv \sigma_0$, o termo $\log \sigma(x_i)$ torna-se constante
    e pode ser ignorado na minimização. Nesse caso, a perda se reduz a
    \begin{equation}
        \mathcal{L}_{\text{NLL}}
        \propto
        \sum_{i=1}^n
        \big(
        y_i + \sigma_0 z_q - Q_q(x_i)
        \big)^2.
    \end{equation}

    Definindo a variável transformada
    \begin{equation}
        Z_i = y_i + \sigma_0 z_q,
    \end{equation}
    temos
    \begin{equation}
        Z_i \mid X_i = x
        \sim \mathcal{N}\big(Q_q(x),\, \sigma_0^2\big),
    \end{equation}
    ou seja, $Q_q(x)$ é exatamente a \emph{média condicional} de $Z_i$ dado $X_i=x$.

    Assim, sob o modelo Normal com variância constante, podemos estimar o quantil
    condicional $Q_q(x)$ minimizando um \emph{erro quadrático médio} sobre a
    variável transformada $Z_i$:
    \begin{equation}
        \mathcal{L}_{\text{MSE-quantil}}
        = \sum_{i=1}^n
        \big(
        Z_i - Q_q(x_i)
        \big)^2.
    \end{equation}

\subsection*{Descrição do experimento e configuração do modelo}
\subsubsection*{Hiperparâmetros do modelo}
\subsubsection*{Arquitetura}
\subsubsection*{Tamanho do modelo}
\subsubsection*{Treinamento e métricas}

\end{answer}


\end{document}
