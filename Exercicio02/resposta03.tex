%------------------------------------------------------------
% Questão 03 – Itens (a)–(d)
%------------------------------------------------------------

\paragraph{(a) $g(\theta)=E_\theta(Z)$}
\[\]
Pela definição de esperança para variáveis contínuas, sendo $h(z)$ a função de interesse:
\[
  E_\theta(h(Z)) = \int_{-\infty}^{\infty} h(z)\, f(z;\theta)\,dz.
\]

Em particular, a esperança do primeiro momento de $Z$, é:
\[
  E_\theta(Z)
  = \int_{-\infty}^{\infty} z\,\frac{1}{\sqrt{2\pi\sigma^2}}
  e^{-\frac{(z-\mu)^2}{2\sigma^2}}\,dz
  = \int_{-\infty}^{\infty} (\mu + y)\,
  \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{y^2}{2\sigma^2}}\,dy
  = \mu,
\]

\[
  E_\theta(Z)
  = \mu \int_{-\infty}^{\infty}
  \frac{1}{\sqrt{2\pi\sigma^{2}}}\,e^{-\frac{y^{2}}{2\sigma^{2}}} \, dy
  \;+\;
  \int_{-\infty}^{\infty}
  y\,\frac{1}{\sqrt{2\pi\sigma^{2}}}\,e^{-\frac{y^{2}}{2\sigma^{2}}}\,dy
\]

\[
  E_\theta(Z)
  = \mu \int_{-\infty}^{\infty} \phi_\sigma(y)\,dy
  \;+\;
  \int_{-\infty}^{\infty} y\,\phi_\sigma(y)\,dy
  = \mu \cdot 1 + 0
  = \mu.
\]

Então:
\[
  g(\theta) = g(\mu,\sigma^2) = E_\theta(Z) = \mu.
\]

Pela invariância do EMV,
\[
  \widehat g_{MV} = g(\widehat\mu_{MV},\widehat\sigma^2_{MV})
  = \widehat\mu_{MV}
  = \overline Z.
\]

Portanto,
\[
  \boxed{\widehat g_{MV} = \overline Z.}
\]

%--------------------------- (b)

\paragraph{(b) $g(\theta)=P_\theta(Z<2)$}
\[\]

Seja a função distribuição acumulada da Normal padrão \(N(0,1)\) definida por:
\[
  \Phi(x)=\frac{1}{\sqrt{2\pi}}
  \int_{-\infty}^{x} e^{-t^{2}/2}\,dt.
\]

Integrando a função densidade de probabilidade $f(z_i;\mu,\sigma^2)$ para encontrar $P_\theta(Z<2)$, obtemos:
\[
  P_\theta(Z<2)
  = \int_{-\infty}^{2} \frac{1}{\sqrt{2\pi\sigma^2}}
  e^{-(z-\mu)^2/(2\sigma^2)}dz
  = \int_{-\infty}^{\frac{2-\mu}{\sqrt{\sigma^2}}}
  \frac{1}{\sqrt{2\pi}}\,
  e^{-y^{2}/2}\,dy
  = \Phi\!\left(\frac{2-\mu}{\sqrt{\sigma^2}}\right),
\]


Assim, expressamos o resultado em termos de \(\Phi\):
\[
  g(\theta) =
  g(\mu,\sigma^2)
  = \Phi\!\left(
  \frac{2-\mu}{\sqrt{\sigma^2}}
  \right).
\]

Pela \textbf{invariância do EMV}, o estimador de máxima verossimilhança de $g(\theta)$ é:
\[
  \widehat g_{MV}
  = g(\widehat\mu_{MV},\widehat\sigma^2_{MV})
  = \Phi\!\left(
  \frac{2-\widehat\mu_{MV}}{\sqrt{\widehat\sigma^2_{MV}}}
  \right)
  = \Phi\!\left(
  \frac{2-\overline Z}{\sqrt{\widehat\sigma^2_{MV}}}
  \right)
  = \Phi\!\left(
  \frac{(2-\overline Z)\sqrt n}{
      \sqrt{\sum_{i=1}^n (z_i-\overline Z)^2}
    }
  \right)
\]

\[
  \boxed{
    \widehat g_{MV}
    = \Phi\!\left(
    \frac{(2-\overline Z)\sqrt n}{
        \sqrt{\sum_{i=1}^n (z_i-\overline Z)^2}
      }
    \right)}
\]

%--------------------------- (c)

\paragraph{(c) $g(\theta)=P_\theta(2.6<Z<4)$}

De forma análoga, escrevemos a probabilidade em termos da Normal padrão:
\[
  P_\theta(2.6<Z<4)
  = P_\theta(Z<4) - P_\theta(Z\le 2.6)
  = \Phi\!\left(\frac{4-\mu}{\sigma}\right)
  - \Phi\!\left(\frac{2.6-\mu}{\sigma}\right).
\]

Logo,
\[
  g(\mu,\sigma^2)
  = \Phi\!\left(\frac{4-\mu}{\sqrt{\sigma^2}}\right)
  - \Phi\!\left(\frac{2.6-\mu}{\sqrt{\sigma^2}}\right).
\]

Pela invariância do EMV,
\[
  \widehat g_{MV}
  = g(\widehat\mu_{MV},\widehat\sigma^2_{MV})
  = \Phi\!\left(
  \frac{4-\overline Z}{\sqrt{\widehat\sigma^2_{MV}}}
  \right)
  - \Phi\!\left(
  \frac{2.6-\overline Z}{\sqrt{\widehat\sigma^2_{MV}}}
  \right),
\]
onde novamente
\[
  \widehat\sigma^2_{MV}
  = \frac{1}{n}\sum_{i=1}^n (z_i-\overline Z)^2.
\]

Portanto,
\[
  \boxed{
    \widehat g_{MV}
    = \Phi\!\left(
    \frac{4-\overline Z}{\sqrt{\widehat\sigma^2_{MV}}}
    \right)
    - \Phi\!\left(
    \frac{2.6-\overline Z}{\sqrt{\widehat\sigma^2_{MV}}}
    \right).
  }
\]

%--------------------------- (d)

\paragraph{(d) $g(\theta)=\mathrm{Var}_\theta(Z)$}

Para a Normal $N(\mu,\sigma^2)$,
\[
  \mathrm{Var}_\theta(Z) = \sigma^2.
\]
Portanto,
\[
  g(\mu,\sigma^2) = \sigma^2.
\]

Pela invariância do EMV,
\[
  \widehat g_{MV}
  = g(\widehat\mu_{MV},\widehat\sigma^2_{MV})
  = \widehat\sigma^2_{MV}
  = \frac{1}{n}\sum_{i=1}^n (z_i-\overline Z)^2.
\]

Assim,
\[
  \boxed{
  \widehat g_{MV}
  = \widehat\sigma^2_{MV}
  = \frac{1}{n}\sum_{i=1}^n (z_i-\overline Z)^2.
  }
\]


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%---------------- (a) ----------------
\subsubsection*{(a) \; $g(\theta)=E_\theta(Z)$}
\paragraph{Esperança matemática.}
A esperança (ou valor esperado) de uma variável aleatória é o análogo contínuo da média ponderada.
Ela representa o valor médio que esperaríamos observar após infinitas repetições do experimento.

\begin{itemize}
  \item \textbf{Caso discreto:} se $Z$ assume valores $z_i$ com probabilidades $p_i$, então
        \[
          E(Z)=\sum_i z_i p_i.
        \]
  \item \textbf{Caso contínuo:} se $Z$ tem densidade $f(z)$, então
        \[
          E(Z)=\int_{-\infty}^{+\infty} z\,f(z)\,dz.
        \]
\end{itemize}

\paragraph{Exemplo: Normal $N(\mu,\sigma^2)$.}
A densidade é
\[
  f(z\mid\mu,\sigma^2)
  =\frac{1}{\sqrt{2\pi\sigma^2}}\exp\!\left[-\frac{(z-\mu)^2}{2\sigma^2}\right].
\]
Então:
\[
  E(Z)
  =\int_{-\infty}^{+\infty} z\,f(z\mid\mu,\sigma^2)\,dz.
\]

Fazendo a mudança de variável $x=\frac{z-\mu}{\sigma}$, obtemos
\[
  E(Z)
  =\int_{-\infty}^{+\infty}(\mu+\sigma x)
  \frac{1}{\sqrt{2\pi}}e^{-x^2/2}\,dx
  =\mu\underbrace{\int_{-\infty}^{+\infty}\frac{1}{\sqrt{2\pi}}e^{-x^2/2}\,dx}_{=1}
  +\sigma\underbrace{\int_{-\infty}^{+\infty}\frac{x}{\sqrt{2\pi}}e^{-x^2/2}\,dx}_{=0}.
\]
Logo,
\[
  \boxed{E(Z)=\mu.}
\]
\textbf{Fato conhecido da Normal:} se $Z \sim N(\mu,\sigma^2)$ então $E(Z)=\mu$.
\begin{itemize}
  \item[\(\triangleright\)] \textbf{Como justificar rapidamente:} pela \emph{linearidade da esperança} e pela
        \emph{padronização} $Z=\mu+\sigma T$ com $T\sim N(0,1)$,
        \[
          E(Z)=E(\mu+\sigma T)=\mu+\sigma E(T)=\mu+ \sigma\cdot 0=\mu .
        \]
\end{itemize}
Logo, $g(\theta)=\mu$ e, pela \textbf{invariância do EMV},
\[
  \boxed{\;\widehat g_{(a)}=\widehat\mu_{MV}=\overline Z\;}
\]

%---------------- (b) ----------------
\subsubsection*{(b) \; $g(\theta)=P_\theta(Z<2)$}

\textbf{Passo 1 — Padronização (transformação linear):}
Se $Z\sim N(\mu,\sigma^2)$ então
\[
  T=\frac{Z-\mu}{\sigma}\sim N(0,1).
\]

\textbf{Passo 2 — Troca de variável na probabilidade (monotonicidade):}
\[
  P_\theta(Z<2)=P\!\left(\frac{Z-\mu}{\sigma}<\frac{2-\mu}{\sigma}\right)
  =\Phi\!\left(\frac{2-\mu}{\sigma}\right).
\]
\emph{(Usamos que a função $z\mapsto (z-\mu)/\sigma$ é estritamente crescente quando $\sigma>0$.)}

\textbf{Passo 3 — Invariância do EMV (plug-in):}
\[
  \boxed{\;\widehat g_{(b)}=\Phi\!\left(\frac{2-\widehat\mu_{MV}}{\widehat\sigma_{MV}}\right)\;}
\]

%---------------- (c) ----------------
\subsubsection*{(c) \; $g(\theta)=P_\theta(2.6<Z<4)$}

\textbf{Passo 1 — Padronização:}
\[
  T=\frac{Z-\mu}{\sigma}\sim N(0,1).
\]

\textbf{Passo 2 — Regra da janela + CDF da Normal padrão:}
\[
  P_\theta(2.6<Z<4)=P\!\left(\frac{2.6-\mu}{\sigma}<T<\frac{4-\mu}{\sigma}\right)
  =\Phi\!\left(\frac{4-\mu}{\sigma}\right)-\Phi\!\left(\frac{2.6-\mu}{\sigma}\right).
\]

\textbf{Passo 3 — Invariância do EMV (plug-in):}
\[
  \boxed{\;\widehat g_{(c)}=\Phi\!\left(\frac{4-\widehat\mu_{MV}}{\widehat\sigma_{MV}}\right)
    -\Phi\!\left(\frac{2.6-\widehat\mu_{MV}}{\widehat\sigma_{MV}}\right)\;}
\]

%---------------- (d) ----------------
\subsubsection*{(d) \; $g(\theta)=\mathrm{Var}_\theta(Z)$}
\paragraph{Variância.}
A \textbf{variância} de uma variável aleatória $Z$ mede a dispersão dos valores em torno da média $E(Z)$.
Formalmente, ela é definida por
\[
  \mathrm{Var}(Z) = E\!\big[(Z - E(Z))^2\big].
\]

\paragraph{Definição expandida.}
Usando a propriedade de linearidade da esperança e a expansão do quadrado,
\[
  (Z - E(Z))^2 = Z^2 - 2Z\,E(Z) + E(Z)^2,
\]
temos
\[
  \mathrm{Var}(Z) = E(Z^2) - 2E(Z)E(Z) + [E(Z)]^2 = E(Z^2) - [E(Z)]^2.
\]
Portanto,
\[
  \boxed{\;\mathrm{Var}(Z) = E(Z^2) - [E(Z)]^2.\;}
\]

\paragraph{Cálculo para a Normal.}
Se $Z \sim \mathcal N(\mu, \sigma^2)$, já sabemos que $E(Z)=\mu$.
Logo precisamos calcular $E(Z^2)$.

Por definição:
\[
  E(Z^2) = \int_{-\infty}^{+\infty} z^2\,f(z\mid\mu,\sigma^2)\,dz,
  \quad\text{onde}\quad
  f(z\mid\mu,\sigma^2)=\frac{1}{\sqrt{2\pi\sigma^2}}\exp\!\left[-\frac{(z-\mu)^2}{2\sigma^2}\right].
\]

\paragraph{Mudança de variável (padronização).}
Definimos $x = \frac{z-\mu}{\sigma}$, de modo que $z = \mu + \sigma x$ e $dz = \sigma dx$.
Substituímos na integral:
\[
  E(Z^2)
  = \int_{-\infty}^{+\infty} (\mu+\sigma x)^2 \frac{1}{\sqrt{2\pi}}e^{-x^2/2}\,dx.
\]

Expandindo o quadrado:
\[
  (\mu+\sigma x)^2 = \mu^2 + 2\mu\sigma x + \sigma^2 x^2.
\]

Substituindo e separando termos:
\[
  E(Z^2)
  = \mu^2\underbrace{\int_{-\infty}^{+\infty}\frac{1}{\sqrt{2\pi}}e^{-x^2/2}\,dx}_{=1}
  + 2\mu\sigma\underbrace{\int_{-\infty}^{+\infty}\frac{x}{\sqrt{2\pi}}e^{-x^2/2}\,dx}_{=0}
  + \sigma^2\underbrace{\int_{-\infty}^{+\infty}\frac{x^2}{\sqrt{2\pi}}e^{-x^2/2}\,dx}_{=1}.
\]

\paragraph{Usamos propriedades da Normal padrão $X\sim N(0,1)$:}
\[
  E(X)=0, \quad E(X^2)=1.
\]

Logo:
\[
  E(Z^2)=\mu^2 + \sigma^2.
\]

\paragraph{Substituindo na definição da variância:}
\[
  \mathrm{Var}(Z) = E(Z^2) - [E(Z)]^2 = (\mu^2+\sigma^2) - \mu^2 = \sigma^2.
\]

\paragraph{Conclusão.}
\[
  \boxed{\;\mathrm{Var}(Z)=\sigma^2.\;}
\]
Assim, o parâmetro $\sigma^2$ da distribuição normal é, por definição, a variância populacional — ele controla a dispersão dos valores de $Z$ em torno da média $\mu$.
\textbf{Fato conhecido da Normal:} se $Z\sim N(\mu,\sigma^2)$, então $\mathrm{Var}(Z)=\sigma^2$.
\begin{itemize}
  \item[\(\triangleright\)] \textbf{Justificativa curta:} pela padronização $Z=\mu+\sigma T$ com $T\sim N(0,1)$,
        usando \emph{homogeneidade da variância} para fatores constantes,
        \[
          \mathrm{Var}(Z)=\mathrm{Var}(\mu+\sigma T)=\sigma^2\,\mathrm{Var}(T)=\sigma^2\cdot 1=\sigma^2 .
        \]
\end{itemize}
Logo, $g(\theta)=\sigma^2$ e, pela \textbf{invariância do EMV}, com o EMV já obtido para $\sigma^2$,
\[
  \boxed{\;\widehat g_{(d)}=\widehat\sigma^2_{MV}=\frac{1}{n}\sum_{i=1}^n (Z_i-\overline Z)^2\;}
\]
\emph{(Recordando: para a Normal com $\mu$ desconhecido, o EMV de $\sigma^2$ usa divisor $n$, não $n-1$.)}
