
%---------------- (a) ----------------
\subsubsection*{(a) \; $g(\theta)=E_\theta(Z)$}
\paragraph{Esperança matemática.}
A esperança (ou valor esperado) de uma variável aleatória é o análogo contínuo da média ponderada.  
Ela representa o valor médio que esperaríamos observar após infinitas repetições do experimento.

\begin{itemize}
  \item \textbf{Caso discreto:} se $Z$ assume valores $z_i$ com probabilidades $p_i$, então
  \[
  E(Z)=\sum_i z_i p_i.
  \]
  \item \textbf{Caso contínuo:} se $Z$ tem densidade $f(z)$, então
  \[
  E(Z)=\int_{-\infty}^{+\infty} z\,f(z)\,dz.
  \]
\end{itemize}

\paragraph{Exemplo: Normal $N(\mu,\sigma^2)$.}
A densidade é
\[
f(z\mid\mu,\sigma^2)
=\frac{1}{\sqrt{2\pi\sigma^2}}\exp\!\left[-\frac{(z-\mu)^2}{2\sigma^2}\right].
\]
Então:
\[
E(Z)
=\int_{-\infty}^{+\infty} z\,f(z\mid\mu,\sigma^2)\,dz.
\]

Fazendo a mudança de variável $x=\frac{z-\mu}{\sigma}$, obtemos
\[
E(Z)
=\int_{-\infty}^{+\infty}(\mu+\sigma x)
\frac{1}{\sqrt{2\pi}}e^{-x^2/2}\,dx
=\mu\underbrace{\int_{-\infty}^{+\infty}\frac{1}{\sqrt{2\pi}}e^{-x^2/2}\,dx}_{=1}
+\sigma\underbrace{\int_{-\infty}^{+\infty}\frac{x}{\sqrt{2\pi}}e^{-x^2/2}\,dx}_{=0}.
\]
Logo,
\[
\boxed{E(Z)=\mu.}
\]
\textbf{Fato conhecido da Normal:} se $Z \sim N(\mu,\sigma^2)$ então $E(Z)=\mu$.
\begin{itemize}
  \item[\(\triangleright\)] \textbf{Como justificar rapidamente:} pela \emph{linearidade da esperança} e pela
  \emph{padronização} $Z=\mu+\sigma T$ com $T\sim N(0,1)$,
  \[
  E(Z)=E(\mu+\sigma T)=\mu+\sigma E(T)=\mu+ \sigma\cdot 0=\mu .
  \]
\end{itemize}
Logo, $g(\theta)=\mu$ e, pela \textbf{invariância do EMV},
\[
\boxed{\;\widehat g_{(a)}=\widehat\mu_{MV}=\overline Z\;}
\]

%---------------- (b) ----------------
\subsubsection*{(b) \; $g(\theta)=P_\theta(Z<2)$}

\textbf{Passo 1 — Padronização (transformação linear):}
Se $Z\sim N(\mu,\sigma^2)$ então
\[
T=\frac{Z-\mu}{\sigma}\sim N(0,1).
\]

\textbf{Passo 2 — Troca de variável na probabilidade (monotonicidade):}
\[
P_\theta(Z<2)=P\!\left(\frac{Z-\mu}{\sigma}<\frac{2-\mu}{\sigma}\right)
=\Phi\!\left(\frac{2-\mu}{\sigma}\right).
\]
\emph{(Usamos que a função $z\mapsto (z-\mu)/\sigma$ é estritamente crescente quando $\sigma>0$.)}

\textbf{Passo 3 — Invariância do EMV (plug-in):}
\[
\boxed{\;\widehat g_{(b)}=\Phi\!\left(\frac{2-\widehat\mu_{MV}}{\widehat\sigma_{MV}}\right)\;}
\]

%---------------- (c) ----------------
\subsubsection*{(c) \; $g(\theta)=P_\theta(2.6<Z<4)$}

\textbf{Passo 1 — Padronização:}
\[
T=\frac{Z-\mu}{\sigma}\sim N(0,1).
\]

\textbf{Passo 2 — Regra da janela + CDF da Normal padrão:}
\[
P_\theta(2.6<Z<4)=P\!\left(\frac{2.6-\mu}{\sigma}<T<\frac{4-\mu}{\sigma}\right)
=\Phi\!\left(\frac{4-\mu}{\sigma}\right)-\Phi\!\left(\frac{2.6-\mu}{\sigma}\right).
\]

\textbf{Passo 3 — Invariância do EMV (plug-in):}
\[
\boxed{\;\widehat g_{(c)}=\Phi\!\left(\frac{4-\widehat\mu_{MV}}{\widehat\sigma_{MV}}\right)
-\Phi\!\left(\frac{2.6-\widehat\mu_{MV}}{\widehat\sigma_{MV}}\right)\;}
\]

%---------------- (d) ----------------
\subsubsection*{(d) \; $g(\theta)=\mathrm{Var}_\theta(Z)$}
\paragraph{Variância.}
A \textbf{variância} de uma variável aleatória $Z$ mede a dispersão dos valores em torno da média $E(Z)$.
Formalmente, ela é definida por
\[
\mathrm{Var}(Z) = E\!\big[(Z - E(Z))^2\big].
\]

\paragraph{Definição expandida.}
Usando a propriedade de linearidade da esperança e a expansão do quadrado,
\[
(Z - E(Z))^2 = Z^2 - 2Z\,E(Z) + E(Z)^2,
\]
temos
\[
\mathrm{Var}(Z) = E(Z^2) - 2E(Z)E(Z) + [E(Z)]^2 = E(Z^2) - [E(Z)]^2.
\]
Portanto,
\[
\boxed{\;\mathrm{Var}(Z) = E(Z^2) - [E(Z)]^2.\;}
\]

\paragraph{Cálculo para a Normal.}
Se $Z \sim \mathcal N(\mu, \sigma^2)$, já sabemos que $E(Z)=\mu$.
Logo precisamos calcular $E(Z^2)$.

Por definição:
\[
E(Z^2) = \int_{-\infty}^{+\infty} z^2\,f(z\mid\mu,\sigma^2)\,dz,
\quad\text{onde}\quad
f(z\mid\mu,\sigma^2)=\frac{1}{\sqrt{2\pi\sigma^2}}\exp\!\left[-\frac{(z-\mu)^2}{2\sigma^2}\right].
\]

\paragraph{Mudança de variável (padronização).}
Definimos $x = \frac{z-\mu}{\sigma}$, de modo que $z = \mu + \sigma x$ e $dz = \sigma dx$.
Substituímos na integral:
\[
E(Z^2)
= \int_{-\infty}^{+\infty} (\mu+\sigma x)^2 \frac{1}{\sqrt{2\pi}}e^{-x^2/2}\,dx.
\]

Expandindo o quadrado:
\[
(\mu+\sigma x)^2 = \mu^2 + 2\mu\sigma x + \sigma^2 x^2.
\]

Substituindo e separando termos:
\[
E(Z^2)
= \mu^2\underbrace{\int_{-\infty}^{+\infty}\frac{1}{\sqrt{2\pi}}e^{-x^2/2}\,dx}_{=1}
+ 2\mu\sigma\underbrace{\int_{-\infty}^{+\infty}\frac{x}{\sqrt{2\pi}}e^{-x^2/2}\,dx}_{=0}
+ \sigma^2\underbrace{\int_{-\infty}^{+\infty}\frac{x^2}{\sqrt{2\pi}}e^{-x^2/2}\,dx}_{=1}.
\]

\paragraph{Usamos propriedades da Normal padrão $X\sim N(0,1)$:}
\[
E(X)=0, \quad E(X^2)=1.
\]

Logo:
\[
E(Z^2)=\mu^2 + \sigma^2.
\]

\paragraph{Substituindo na definição da variância:}
\[
\mathrm{Var}(Z) = E(Z^2) - [E(Z)]^2 = (\mu^2+\sigma^2) - \mu^2 = \sigma^2.
\]

\paragraph{Conclusão.}
\[
\boxed{\;\mathrm{Var}(Z)=\sigma^2.\;}
\]
Assim, o parâmetro $\sigma^2$ da distribuição normal é, por definição, a variância populacional — ele controla a dispersão dos valores de $Z$ em torno da média $\mu$.
\textbf{Fato conhecido da Normal:} se $Z\sim N(\mu,\sigma^2)$, então $\mathrm{Var}(Z)=\sigma^2$.
\begin{itemize}
  \item[\(\triangleright\)] \textbf{Justificativa curta:} pela padronização $Z=\mu+\sigma T$ com $T\sim N(0,1)$,
  usando \emph{homogeneidade da variância} para fatores constantes,
  \[
  \mathrm{Var}(Z)=\mathrm{Var}(\mu+\sigma T)=\sigma^2\,\mathrm{Var}(T)=\sigma^2\cdot 1=\sigma^2 .
  \]
\end{itemize}
Logo, $g(\theta)=\sigma^2$ e, pela \textbf{invariância do EMV}, com o EMV já obtido para $\sigma^2$,
\[
\boxed{\;\widehat g_{(d)}=\widehat\sigma^2_{MV}=\frac{1}{n}\sum_{i=1}^n (Z_i-\overline Z)^2\;}
\]
\emph{(Recordando: para a Normal com $\mu$ desconhecido, o EMV de $\sigma^2$ usa divisor $n$, não $n-1$.)}
