Podemos calcular os ítens (a) e (b) utilizando a propriedade de invariância do estimador de máxima verossimilhança (EMV).

\textbf{Teorema (Invariância do EMV).}
Seja $\hat\theta_{MV}$ o estimador de máxima verossimilhança de um parâmetro
$\theta \in \Theta$ e seja $g:\Theta \to \mathcal{G}$ uma função.
Então o estimador de máxima verossimilhança de
\[
    \tau = g(\theta)
\]
é dado por
\[
    \hat\tau_{MV} = g(\hat\theta_{MV}).
\]
Isto é, para obter o EMV de qualquer função de $\theta$, basta aplicar essa
função ao EMV de $\theta$.

\medskip

\subsection*{Cálculo do EMV de $\theta$ ( $\hat\theta_{MV}$)}

\textbf{Função de verossimilhança}

Seja $Z_1,\dots,Z_n$ uma amostra aleatória de $Z \sim \mathrm{Bernoulli}(\theta), \quad \theta \in (0,1)$. Com:
$$
    P_\theta(Z_i = 1) = \theta,
    \qquad
    P_\theta(Z_i = 0) = 1 - \theta.
$$

A função de probabilidade para $Z_i$ é então:
\[
    f(z_i;\theta) = \theta^{z_i}(1-\theta)^{1-z_i}.
\]

Como $Z_1,\dots,Z_n$ são independentes, a probabilidade conjunta, definida como a função de verossimilhança é:
\[
    \ell(\theta)
    = \prod_{i=1}^n \theta^{z_i}(1-\theta)^{1-z_i}.
\]

\text{Juntando os expoentes, temos:}
\[
    \ell(\theta)
    = \theta^{\sum_{i=1}^n z_i}\,(1-\theta)^{\sum_{i=1}^n (1-z_i)}
    = \theta^{\sum_{i=1}^n z_i}\,(1-\theta)^{n-\sum_{i=1}^n z_i}.
\]

Seja
\[
    S = \sum_{i=1}^n z_i,
\]
Podemos escrever:
\[
    \ell(\theta) = \theta^{S}(1-\theta)^{n-S}.
\]

\medskip

\textbf{Função Log-verossimilhança}

Podemos simplificar os cálculos trabalhando com o logaritmo desta função, mantendo o estimador de máxima verossimilhança inalterado, uma vez que este trata-se do ponto crítico desta função.
Maximizar $\ell(\theta)$ ou
$\mathcal{L}(\theta) = \log \ell(\theta)$ é equivalente, devido ao fato do logaritmo ser estritamente crescente. Portanto, aplicando o logaritmo, simplificamos para:
\[
    \mathcal{L}(\theta)
    = \log \ell(\theta)
    = \log\big(\theta^{S}(1-\theta)^{n-S}\big)
    = S\log\theta + (n-S)\log(1-\theta).
\]

\medskip

\textbf{Ponto crítico da log-verossimilhança}

Calculando a derivada:
\[
    \frac{d\mathcal{L}}{d\theta}=
    S \cdot \frac{1}{\theta}
    + (n-S)\cdot\left(-\frac{1}{1-\theta}\right)
    = \frac{S}{\theta} - \frac{n-S}{1-\theta}.
\]

Seja a condição de máximo:
\[
    \frac{S}{\theta} - \frac{n-S}{1-\theta} = 0.
\]

\[
    \frac{S}{\theta} = \frac{n-S}{1-\theta}.
\]

\[
    S(1-\theta) = \theta(n-S).
\]

\[
    S - S\theta = n\theta - S\theta.
\]

Os termos $-S\theta$ cancelam em ambos os lados, restando:
\[
    S = n\theta.
\]

Isolando $\theta$:
\[
    \boxed{
        \hat\theta_{MV} = \frac{S}{n}.
    }
\]

Substituindo de volta em funcão da variável aleatória $Z_i$, obtemos:
\[
    \hat\theta_{MV}
    = \frac{1}{n}\sum_{i=1}^n Z_i
    = \bar Z.
\]