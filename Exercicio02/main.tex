\documentclass[a4paper]{article}
\usepackage{student}
\usepackage{graphicx}
\usepackage{caption}
\usepackage[version=4]{mhchem}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows.meta, positioning, decorations.pathreplacing}
\usepackage{enumitem}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{float}
\usepackage[portuguese]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[numbers,sort&compress]{natbib} % ou [authoryear]
\usepackage[alf]{abntex2cite} % citação ABNT autor-data
\pagestyle{plain}

\tikzstyle{arrow} = [thick,->,>=stealth]

% Definindo o estilo de destaque com linhas pontilhadas
\tikzstyle{highlight} = [draw, dashed, thick, rectangle, rounded corners, inner sep=0.2cm, orange]


\tikzstyle{startstop} = [
    rectangle, rounded corners, minimum width=0.5cm,
    text centered, draw=black, fill=blue!10, font=\small
]
\tikzstyle{startstop_S} = [
    rectangle, rounded corners, minimum width=0.5cm, minimum height=0.8cm,
    text centered, draw=black, fill=green!30, font=\small
]
\tikzstyle{decision} = [
    diamond, aspect=2, draw=black, fill=orange!15, align=center,
    text centered, inner sep=0pt, font=\small
]
\tikzstyle{decision_S} = [
    diamond, aspect=2, draw=black, fill=orange!30, align=center,
    text centered, inner sep=0pt, font=\small
]
\tikzstyle{arrow} = [thick,->,>=stealth]



% Metadata
\date{\today}
\setmodule{MAE5911/IME: Fundamentos de Estatística e Machine Learning. \\ Prof.: Alexandre Galvão Patriota} 
\setterm{2o. semestre, 2025}

%-------------------------------%
% Other details
% TODO: Fill these
%-------------------------------%
\title{Exercício 02 - 15/10}
\setmembername{Nara Avila Moraes}  % Fill group member names
\setmemberuid{5716734}  % Fill group member uids (same order)

%-------------------------------%
% Add / Delete commands and packages
% TODO: Add / Delete here as you need
%-------------------------------%
\usepackage{amsmath,amssymb,bm}

\newcommand{\KL}{\mathrm{KL}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\T}{\top}

\newcommand{\expdist}[2]{%
        \normalfont{\textsc{Exp}}(#1, #2)%
    }
\newcommand{\expparam}{\bm \lambda}
\newcommand{\Expparam}{\bm \Lambda}
\newcommand{\natparam}{\bm \eta}
\newcommand{\Natparam}{\bm H}
\newcommand{\sufstat}{\bm u}

% Main document
\begin{document}
    % Add header
    \header{}

\textbf{Questão 01:}  
Seja $(Z_1, \ldots, Z_n)$ uma amostra aleatória de $Z \sim \text{Ber}(\theta)$, $\theta \in (0,1)$.
\begin{itemize}
  \item[(a)] Encontre o EMV para $g(\theta) = P_\theta(Z = 0)$.
  \item[(b)] Encontre o EMV para $g(\theta) = \mathrm{Var}_\theta(Z)$.
  \item[(c)] Considere que os dados foram observados (0, 0, 1, 0, 0, 1). Encontre as estimativas de MV nos itens acima.
  \item[(d)] Construa o valor-p para a hipótese H: ``$\theta = 0.1$'' usando os dados do item anterior.
\end{itemize}

    \begin{answer}[]
O estimador via função de máxima verossimilhança foi proposto por Fisher e comprovadado ser superior a outros métodos estimadores, como o método de momentos. 
Este estimador é utilizado sobretudo quando a informação da distribuição de probabilidade do modelo é conhecida. 
Ela tem propriedades como eficiência assintótica, consistência, distribuição normal e invariancia [Notas de aula pg. 60]. 
O estimador de máxima verossimilhança representa o valor do parâmetro $\theta$ que torna os dados observados o mais plausível possível. 
\\[0.5em]
Devido a propriedade de invariancia do EMV, basta encontrarmos EMV para a letra (a) e depois calcular a letra (b) utilizando este estimador como parâmetro.


Seja \( (Z_1, \dots, Z_n) \) a amostra aleatória de 
\[
Z_i \sim \mathrm{Bernoulli}(\theta), \quad \theta \in (0,1).
\]

A função de verossimilhança, que é a função de probabilidade do modelo, reinter é dada por
\[
L(\theta; z_1, \dots, z_n)
= \prod_{i=1}^n \theta^{z_i}(1-\theta)^{1-z_i}.
\]

Para encontrar o estimador de máxima verossimilhança, precisamos maximizar \( L(\theta) \) em relação a \( \theta \). Mas como esta função se trata de um produtório,
é mais fácil maximizar a função log-verossimilhança, que por ser monotonicamente crescente, preserva o valor de theta do ponto máximo \( L(\theta) \).
A log-verossimilhança correspondente é
\[
\ell(\theta) 
= \sum_{i=1}^n \big[z_i \log\theta + (1-z_i)\log(1-\theta)\big].
\]
Derivando em relação a \(\theta\) e igualando a zero:
\[
\frac{\partial \ell(\theta)}{\partial \theta}
= \frac{\sum z_i}{\theta} - \frac{n-\sum z_i}{1-\theta} = 0,
\]
obtemos o estimador de máxima verossimilhança para a probabilidade de sucesso \(\theta\):
\[
\boxed{\hat{\theta}_{MV} = \bar{Z} = \frac{1}{n}\sum_{i=1}^n Z_i.}
\]

O teste de hipótese, mede o quão improvável seria observar $\hat{\theta}_{MV}$ se a hipótese nula fosse verdadeira. O valor-p é esta medida de improbabilidade.
\\[1em]
Em condições de regularidade, o valor-$p$ segue uma distribuição uniforme em $(0,1)$.
Isso significa que, se o seu valor é muito pequeno, por exemplo $p \leq 0{,}01$,
então o resultado observado é um evento raro sob a hipótese nula $H_0$,
ou, alternativamente, que a hipótese nula está incorreta.
Nessa situação, existem evidências para rejeitar $H_0$.


---

### (a) \( g(\theta) = P_\theta(Z=0) \)

Pela \textbf{propriedade de invariância do EMV}, temos:
\[
\boxed{\hat{g}_{MV} = g(\hat{\theta}_{MV}) = 1 - \bar{Z}.}
\]

---

### (b) \( g(\theta) = \mathrm{Var}_\theta(Z) \)

Sabemos que \( \mathrm{Var}_\theta(Z) = \theta(1 - \theta) \).  
Logo, aplicando novamente a invariância:
\[
\boxed{\hat{g}_{MV} = g(\hat{\theta}_{MV}) = \bar{Z}(1 - \bar{Z}).}
\]

---

### (c) Sejam os dados observados: \( (0, 0, 1, 0, 0, 1) \)

Temos \( n = 6 \) e \( \sum z_i = 2 \), portanto
\[
\hat{\theta}_{MV} = \bar{Z} = \frac{2}{6} = \frac{1}{3} \approx 0.3333.
\]
Assim:
\[
\widehat{P(Z=0)} = 1 - \hat{\theta}_{MV} = \frac{2}{3} \approx 0.6667,
\]
\[
\widehat{\mathrm{Var}}(Z) = \hat{\theta}_{MV}(1 - \hat{\theta}_{MV})
= \frac{1}{3} \cdot \frac{2}{3} = \frac{2}{9} \approx 0.2222.
\]

---

### (d) Teste de hipótese \( H_0: \theta = 0.1 \)

Queremos verificar se temos evidência a partir dos dados observados para rejeitar a hipótese nula 
de que a probabilidade de sucesso é $\theta = 0.1$.

Sabemos que, sob $H_0$, a estatística
\[
X = \sum_{i=1}^{n} Z_i
\]
segue uma distribuição $\text{Binomial}(n, \theta)$.

Com $n = 6$ e $x_{\text{obs}} = 2$ sucessos observados, temos:
\[
X \sim \text{Binomial}(6, 0.1).
\]

---
 
O valor-p é a probabilidade de observar um valor da estatística $T(Z_n)$ 
\textit{tão ou mais extremo quanto o observado}, assumindo que $H_0$ é verdadeira:
\[
p = P_{H_0}\big(T(Z_n) \ge T(z_n)\big).
\]

No contexto binomial, a estatística de teste natural é o número de sucessos $X$,
e portanto:
\[
p = P_{H_0}(X \ge 2).
\]

---
\\
Calculando o valor-p:
\[
P(X=0) = (0.9)^6 = 0.531441, \qquad 
P(X=1) = \binom{6}{1}(0.1)(0.9)^5 = 0.354294.
\]

Logo:
\[
valor-p = 1 - [P(X=0) + P(X=1)] = 1 - (0.531441 + 0.354294) = 0.114265.
\]

\[
\boxed{valor-p = 0.114265
\]

---

Este resultado \textbf{não é improvável} sob $H_0$ (<0.05).  
Portanto, não há evidência para rejeitar  $H_0$, isto é, não evidências de que $\theta = 0.1$ seja incompatível com os dados.

\[
\boxed{\text{Conclusão: Não há evidências suficientes para rejeitar } H_0 : \theta = 0.1.}
\]

\end{mdframed}

    \end{answer}

\textbf{Questão 02:}  
Seja $(Z_1, \ldots, Z_n)$ uma amostra aleatória de $Z \sim \text{Exp}(\theta)$, $\theta \in (0,\infty)$.
\begin{itemize}
  \item[(a)] Encontre o EMV para $g(\theta) = P_\theta(Z > 1)$.
  \item[(b)] Encontre o EMV para $g(\theta) = P_\theta(0.1 < Z < 1)$.
  \item[(c)] Encontre o EMV para $g(\theta) = \mathrm{Var}_\theta(Z)$.
  \item[(d)] Considere que os dados foram observados (0.2, 0.6, 0.3, 0.2, 0.8, 0.12). Encontre um IC aproximado de 95\% de confiança para $g(\theta)$ nos itens acima.
  \item[(e)] Faça uma simulação de Monte Carlo para verificar se os IC's aproximados obtidos no passo anterior têm cobertura próxima do nível de confiança estabelecido. Caso não tenham, proponha um tamanho amostral que produza IC's mais confiáveis para cada caso.
\end{itemize}

    \begin{answer}[]

    \end{answer}

\textbf{Questão 03:}  
Seja $(Z_1, \ldots, Z_n)$ uma amostra aleatória de $Z \sim N(\mu, \sigma^2)$, $\theta = (\mu, \sigma^2) \in (-\infty,\infty) \times (0,\infty)$.
\begin{itemize}
  \item[(a)] Encontre o EMV para $g(\theta) = E_\theta(Z)$.
  \item[(b)] Encontre o EMV para $g(\theta) = P_\theta(Z < 2)$.
  \item[(c)] Encontre o EMV para $g(\theta) = P_\theta(2.6 < Z < 4)$.
  \item[(d)] Encontre o EMV para $g(\theta) = \mathrm{Var}_\theta(Z)$.
  \item[(e)] Considere que os dados foram observados (2.4, 2.7, 2.3, 2, 2.5, 2.6). Encontre as estimativas de MV nos itens acima.
\end{itemize}

    \begin{answer}[]

    \end{answer}

\textbf{Questão 04:}  
Seja $(Z_1, \ldots, Z_n)$ uma amostra aleatória de $Z \sim f_\theta$, $\theta \in (0,\infty)$, tal que a função densidade de probabilidade é dada por  
\[
f_\theta(x) = \theta \, x^{\theta-1}, \quad x \in (0,1),
\]
e $f_\theta(x) = 0$, caso contrário.
\begin{itemize}
  \item[(a)] Encontre o EMV para $g(\theta) = E_\theta(Z)$.
  \item[(b)] Encontre o EMV para $g(\theta) = P_\theta(Z > 0.3)$.
  \item[(c)] Encontre o EMV para $g(\theta) = P_\theta(0 < Z < 0.1)$.
  \item[(d)] Encontre o EMV para $g(\theta) = \mathrm{Var}_\theta(Z)$.
  \item[(e)] Considere que os dados foram observados (0.12, 0.50, 0.20, 0.23, 0.30, 0.11). Encontre as estimativas de MV para os itens acima.
\end{itemize}

    \begin{answer}[]

    \end{answer}

\end{document}
